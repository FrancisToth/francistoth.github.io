<!doctype html>
<html lang="en">

<head>
    <meta charset="utf-8">

    <title>Introduction to Spark - by Francis Toth</title>

    <meta name="description" content="A framework for easily creating beautiful presentations using HTML">
    <meta name="author" content="Hakim El Hattab">

    <meta name="apple-mobile-web-app-capable" content="yes" />
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />

    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">

    <link rel="stylesheet" href="css/reveal.css">
    <link rel="stylesheet" href="css/custom.css">
    <link rel="stylesheet" href="css/theme/francis.css" id="theme">

    <!-- Code syntax highlighting -->
    <link rel="stylesheet" href="lib/css/zenburn.css">

    <!-- Printing and PDF exports -->
    <script>
        var link = document.createElement( 'link' );
        link.rel = 'stylesheet';
        link.type = 'text/css';
        link.href = window.location.search.match( /print-pdf/gi ) ? 'css/print/pdf.css' : 'css/print/paper.css';
        document.getElementsByTagName( 'head' )[0].appendChild( link );
    </script>

    <!--[if lt IE 9]>
    <script src="lib/js/html5shiv.js"></script>
    <![endif]-->
</head>

<body>


<aside style="display: block; position: fixed; bottom: 10px; right: 10px; z-index: 30;">
    <a href="http://www.yoppworks.com"><img src="images/spark_scala_meetup/spark_scala_meetup/yoppworks-logo-landscape-large-text-dark-bg.png"
                                            height="30"></a>
</aside>

<div class="reveal">
    <div class="slides">
        <section>
            <aside class="notes">
                Hi everyone, my name is Francis, and today I'd like to give you a brief introduction to Spark.<br/><br/>

                Big Data is overall about two main challenges. First it's about storing data that cannot fit on a
                single node, and secondly it's about figuring out how to compute it.<br/><br/>

                In order to overcome those challenges, Internet giants, such as Google or Amazon, decided to create
                new tools specially designed to deal with large amount of data. And this how technologies such as
                MapReduce and Hadoop were created.<br/><br/>

                However, most of those tools were designed for a specific problem. And because you cannot use the same
                tool for all kind of jobs, some projects were started in order to provide a better answer to the
                Big Data problem.

                This is where Sparks come from.
            </aside>
            <h2 style="display: inline-block;"><img
                    style="vertical-align: text-bottom; margin: 18px 0; border: 0; background-color: #243044"
                    src="images/spark_scala_meetup/spark_logo.transparent.png"/></h2>
        </section>
        <section>
            <h2>Quick overview</h2>
            <aside class="notes">
                So Spark was <b>founded in 2009</b> at the AmpLab in California.<br/><br/>

                ***It is a <b>cluster computing platform</b> which extends the traditional <b>MapReduce model</b>,
                and which is <b>designed to support</b> various types of computations on large <b>dataset</b>.
                Originally, Spark intended to address <b>common problems</b> people were facing while doing Machine
                learning. Therefore, it is highly <b>optimized for iterative and cyclic</b> operations on the same
                set of data.<br/><br/>

                A data pipeline is nothing more than a <b>sequence of steps</b>, each being responsible for <b>processing</b>
                the data, and for <b>passing</b> the result over to the next step. With <b>Hadoop</b>, this is usually
                done by <b>chaining</b> different frameworks with each others and by storing the data resulting from
                each step on the disk.<br/><br/>

                This approach has actually <b>two main issues</b>, first, in terms of <b>performances</b>, as each
                step requires a certain amount of IO to store/read the intermediary data, and secondly in terms of
                <b>maintenance</b>, as it requires to juggle with sometime different languages, paradigms or tools.

                <b>Spark</b> addresses this, by providing a unified API. The main idea is that every Spark's
                library is based on the same core concepts which makes the switch between one library to another
                almost without any problem.<br/><br/>

                ***Compared to <b>traditional MapReduce engines</b>, benchmarks have shown a <b>significant increase</b>
                in performances which have to be qualified however, as a benchmark may show different results
                depending on the context. Overall, you can expect Spark to be twice to three times faster than Hadoop.
                These performances can be actually explained by the fact that Spark makes a heavy use of caching in the
                contrary of Hadoop which is mostly disk dependent.<br/><br/>

                Finally, Spark is today <b>one of the most popular</b> and <b>active</b> project regarding<b>large-scale
                data processing</b>. It is used by <b>Amazon</b>, <b>Ebay</b>, and <b>Netflix</b> among others, and
                has seen its number of <b>contributors</b> growing tremendously since it's hosted by <b>Apache</b>.
            </aside>
            <ul>
                <li>Created in 2009</li>
                <li class="fragment">Large-scale data processing engine</li>
                <li class="fragment">Unified framework</li>
                <li class="fragment">Extremely effective</li>
                <li class="fragment">Popular</li>
            </ul>
        </section>

        <section>
            <h2>Agenda</h2>
            <aside class="notes">
                So today, we'll mainly focus on <b>Spark's core aspects</b>, present its <b>execution model</b> and the
                ways it deals with <b>large-scale data computation</b>. If we have some time, we'll then talk about its
                higher level API's.

                <!--<b>features</b>-->
                <!--it supports. Then we'll briefly cover some of its higher level API's with Spark-SQL, a lib responsible for-->
                <!--doing interactive queries and Spark-Streaming which deals with continuous and close-to-real-time-->
                <!--processing. Unfortunately, we won't have time to cover MLib which is a lib dedicated to machine-->
                <!--learning, and GraphX, which is Spark's graph processing lib.-->
            </aside>
            <div style="display: block; text-align: center">
                <img style="border: 0; background-color: #FFFFFF" src="images/spark_scala_meetup/spark-stack.png"/>
            </div>
        </section>

        <section>
            <h2>Spark Architecture</h2>
            <aside class="notes">
                In terms of architecture, a Spark application relies on three components. First, the driver. The driver is
                the central coordinator of a Spark program. It is responsible for defining distributed dataset along with
                the computations which have to run on top of it<br/><br/>

                In order to perform those <b>tasks</b>, the driver relies on a certain amount of <b>executors</b>. An executor is
                responsible for running the tasks requested by the <b>driver</b>. It provides in-memory storage used by Spark
                for caching purpose along with <b>cores</b> used to run the computation requested by the driver.<br/><br/>

                <!--The Driver and its executors have all their own Java processes allowing a Spark application to be completely-->
                <!--isolated from one another. On the other hand, this also means that data cannot be shared between each-->
                <!--application except by writing it to an external storage.<br/><br/>-->

                What about <b>physical resources</b> now? Those have to come from somewhere right? Well, this is the
                the cluster manager's job. Basically, the cluster manager is responsible for providing the physical
                resources required by a <b>Spark application</b> in order to run its computations.<br/><br/>

                <!--You can think about the cluster manager as the one defining how a job is executed while the driver takes-->
                <!--only care about the what. -->

                So depending on the cluster manager, you may end up with running the Driver
                and the executors on the same laptop or on a distributed cluster composed of several worker nodes like
                it is usually the case.<br/><br/>

                In Spark, the cluster manager is actually a completely <b>independent plugin</b> which does not rely on a
                specific implementation. This allows Spark to run transparently on top of Yarn, Mesos, or its built-in
                standalone cluster manager.<br/><br/>
            </aside>
            <div class="fragment" data-fragment-index="1" style="border: 1px solid white; width: 20%; height: 100px;margin-left: 40%; margin-right: 40%;line-height: 100px;">
                Driver
            </div>
            <div class="fragment" data-fragment-index="5">
                <div class="down" style="margin: 20px 0 10px 0;"><div class="arrow"></div></div>
                <div style="border: 1px solid white; width: 20%; height: 100px;margin-left: 40%; margin-right: 40%">Cluster<br/>Manager</div>
                <div style="margin: 20px 0 10px 0;">
                    <div class="down-l"><div class="arrow"></div></div>
                    <div class="down" style="margin: 0 30px 0 30px;"><div class="arrow"></div></div>
                    <div class="down-r"><div class="arrow"></div></div>
                </div>
            </div>
            <div class="fragment" data-fragment-index="2" style="display: inline-block;">
                <div>
                    <div><span style="background-color: #243044">&nbsp;Executor&nbsp;</span></div>
                    <div style="border: 3px solid #5C97D6;padding: 10px 10px 0 10px;position: relative; z-index: -1;top:-20px;">
                        <div class="fragment" data-fragment-index="4" style="display:inline-block;margin-top:10px;">
                            <div class="partition-block blue" style="width:30px; height:30px;">&nbsp;</div>
                            <div class="partition-block blue" style="width:30px; height:30px;">&nbsp;</div><br/>
                            <div class="partition-block blue" style="width:30px; height:30px;">&nbsp;</div>
                            <div class="partition-block blue" style="width:30px; height:30px;">&nbsp;</div>
                        </div>
                        <div class="fragment" data-fragment-index="3" style="display:inline-block;margin-top:10px;vertical-align: top;border: 1px solid white;padding: 5px;line-height: 65px;">Cache</div>
                    </div>
                </div>
                <div class="fragment" data-fragment-index="6">
                    <div class="line horizontal"></div>
                    <div><span style="background-color: #243044">&nbsp;Node 1&nbsp;</span></div>
                </div>
            </div>
            <div class="fragment" data-fragment-index="2" style="display: inline-block;">
                <div style="display: inline-block;">
                    <div><span style="background-color: #243044">&nbsp;Executor&nbsp;</span></div>
                    <div style="border: 3px solid #5C97D6;padding: 10px 10px 0 10px;position: relative; z-index: -1;top:-20px;">
                        <div class="fragment" data-fragment-index="4" style="display:inline-block;margin-top:10px;">
                            <div class="partition-block blue" style="width:30px; height:30px;">&nbsp;</div>
                            <div class="partition-block blue" style="width:30px; height:30px;">&nbsp;</div><br/>
                            <div class="partition-block blue" style="width:30px; height:30px;">&nbsp;</div>
                            <div class="partition-block blue" style="width:30px; height:30px;">&nbsp;</div>
                        </div>
                        <div class="fragment" data-fragment-index="3" style="display:inline-block;margin-top:10px;vertical-align: top;border: 1px solid white;padding: 5px;line-height: 65px;">Cache</div>
                    </div>
                </div>
                <div style="display: inline-block;">
                    <div><span style="background-color: #243044">&nbsp;Executor&nbsp;</span></div>
                    <div style="border: 3px solid #5C97D6;padding: 10px 10px 0 10px;position: relative; z-index: -1;top:-20px;">
                        <div class="fragment" data-fragment-index="4" style="display:inline-block;margin-top:10px;">
                            <div class="partition-block blue" style="width:30px; height:30px;">&nbsp;</div>
                            <div class="partition-block blue" style="width:30px; height:30px;">&nbsp;</div><br/>
                            <div class="partition-block blue" style="width:30px; height:30px;">&nbsp;</div>
                            <div class="partition-block blue" style="width:30px; height:30px;">&nbsp;</div>
                        </div>
                        <div class="fragment" data-fragment-index="3" style="display:inline-block;margin-top:10px;vertical-align: top;border: 1px solid white;padding: 5px;line-height: 65px;">Cache</div>
                    </div>
                </div>
                <div class="fragment" data-fragment-index="6">
                    <div class="line horizontal"></div>
                    <div><span style="background-color: #243044">&nbsp;Node 2&nbsp;</span></div>
                </div>
            </div>
        </section>

        <section>
            <h2>Resilient Distributed Dataset</h2>
            <section data-transition="none none">
                <aside class="notes">
                    The core fundamental of Spark is the <b>RDD</b>. RDD stands for Resilient Distributed Dataset and is the
                    <b>main entry point</b> of a Spark job. The name may be a bit <b>misleading</b>, as an RDD is not a dataset on its
                    own. It's actually an <b>interface</b> to your data allowing you to define <b>computations</b> on top of it in a
                    transparent way.<br/><br/>

                    Concretely, an RDD represents the data as a <b>collection of partitions</b> distributed among the Spark cluster. Every time a
                    computation is requested, it is sent by the <b>Driver</b> to each executor, in order to process each
                    partition with a certain level of parallelism.<br/><br/>

                    Now why <b>resilient</b>? Well, Spark provides some mechanism allowing to replay a <b>computation</b> if a
                    partition has been lost. But we'll cover this a bit later.
                </aside>

                <div style="display:table;margin: auto">
                    <div style="display:table-cell;vertical-align: middle;width:100px;">
                         <div class="rectangle blue-border">
                            <div class="child blue-border"><div class="child blue-border" style="line-height:70px;"><span style="font-size: 30px;">Data</span></div></div>
                        </div>
                    </div>
                    <div style="display:table-cell;vertical-align: middle;width:150px;">
                        <div style="margin-top:50px;" class="up-r"><div class="arrow"></div></div><br/>
                        <div class="right"><div class="arrow"></div></div><br/>
                        <div class="down-r"><div class="arrow"></div></div>
                    </div>
                    <div style="display:table-cell;vertical-align: middle;">
                        <div class="executor">
                            <div class="title"><span>&nbsp;Executor&nbsp;</span></div>
                            <div class="body">
                                <div class="partition-list">
                                    <div class="block blue">&nbsp;</div>
                                    <div class="block blue" style="opacity: 0.7;">&nbsp;</div>
                                    <div class="block blue" style="opacity: 0.2;">&nbsp;</div>
                                    <div class="block blue" style="background-color: #243044">&nbsp;</div>
                                </div>
                            </div>
                        </div>
                        <div class="executor">
                            <div class="title"><span>&nbsp;Executor&nbsp;</span></div>
                            <div class="body">
                                <div class="partition-list">
                                    <div class="block blue">&nbsp;</div>
                                    <div class="block blue" style="opacity: 0.7;">&nbsp;</div>
                                    <div class="block blue" style="opacity: 0.2;">&nbsp;</div>
                                    <div class="block hidden">&nbsp;</div>
                                </div>
                                <div class="partition-list">
                                    <div class="block blue">&nbsp;</div>
                                    <div class="block blue" style="opacity: 0.7;">&nbsp;</div>
                                    <div class="block hidden">&nbsp;</div>
                                    <div class="block hidden">&nbsp;</div>
                                </div>
                            </div>
                        </div>
                    </div>
                    <div style="display:table-cell;vertical-align: middle;">
                        <div class="line vertical" style="height: 300px;margin-left: 50px;"></div>
                    </div>
                    <div style="display:table-cell;vertical-align: middle;">
                        <span style="margin-left: 30px">RDD</span>
                    </div>
                </div>
            </section>
            <section data-transition="none none">
                <aside class="notes">
                    An RDD can be created using several methods. Overall, Spark can <b>interface</b> with a
                    wide variety of <b>distributed storage</b> including <b>HDFS</b>, <b>OpenStack Swift</b>, <b>Cassandra</b>,
                    <b>S3</b>, or even <b>custom solutions</b>. You can also load simple scala collections which is
                    great for testing purpose.<br/><br/>

                    As you can notice, each data loading method takes an <b>additional parameter</b> defining in how many
                    <b>partitions</b> the data should be split into <b>once loaded</b>. If it is not provided, Spark will
                    fall back on some <b>default values</b> either defined in its configuration or by the <b>datasource</b> itself. For example,
                    in the case of HDFS, Spark will create a single partition for each input split. A simple Scala
                    collection on the other hand would be split by default into as many partitions as there are cores
                    available in the cluster.
                </aside>
                    <pre><code class="scala" data-trim data-noescape style="font-size: 1.3em; line-height: 1.2em">
/* sc stands for Spark's context */

// numSlices is optional
val ints: RDD[String] =
  sc.parallelize(Seq(1, 2, 3,4), numSlices)

// minPartitions is optional
val seqFiles: RDD[(Text, Text)] =
  sc.sequenceFile[Text, text](input, minPartitions)

// minPartitions is optional
val rdd3: RDD[String] =
  sc.textFile("hdfs://...", minPartitions)
                    </code></pre>
            </section>
        </section>
        <section>
            <h2>Transformation & Actions</h2>
            <section data-transition="none none">
                <aside class="notes">
                    Overall, an <b>RDD</b> supports <b>two types</b> of operation. <b>Transformations</b>, which create a new RDD
                    from an <b>existing</b> one, and <b>actions</b> which perform a computation on an RDD's dataset.
                    Most common <b>transformations</b> are <b>map</b>, <b>filter</b> or <b>flatMap</b>, while <b>actions</b>
                    are more about operations consisting in evaluating, <b>collecting</b>, <b>counting</b>, or <b>sampling</b> the data.
                    Actions will be typically responsible for <b>persisting</b> the result of a computation, or sending it back
                    to the <b>driver</b>.
                </aside>
                <div style="margin-top: 80px">
                    Transformations
                    <pre><code class="scala" style="font-size: 25px;">val wc: RDD[(String, Int)] = rdd
    .flatMap(_.split(" "))
    .map((_, 1))
    .reduceByKey(_ + _)</code></pre>

                    Actions
                    <pre><code class="scala" style="font-size: 25px;">
// Retrieves the data of each partition
val result: Seq[(String, Int)] = wc.collect()

// Writes the data in text files
val result: Unit = wc.saveAsTextFile("...")

// Counts the number of element in an RDD
val result: Long = wc.count()

// Retrieves the first four elements of an RDD
val result: Seq[(String, Int)] = wc.take(4)
                    </code></pre>
                </div>
            </section>
            <section data-transition="none none">
                <aside class="notes">
                    One very interesting aspect of <b>data loading</b> and <b>transformations</b>, is that they are <b>lazily
                    evaluated</b>. Actually, as long as <b>no action has been performed</b>, Spark may not have brought
                    data or computed <b>anything</b> at all. Each transformation actually creates a new RDD which
                    <b>maintains a pointer to its ancestors</b> along with the metadata regarding its relationship with
                    them. In <b>Spark</b> terminology, this is what we call the <b>lineage</b> of an RDD. Therefore
                    defining an RDD goes back to define a <b>specification</b> of what an executor should do in order to
                    compute a result.<br/><br/>

                    Now, concretely, an RDD's <b>lineage</b> is stored in an <b>directed acyclic graph</b> which has two
                    main purposes. First, it allows the <b>recomputing</b> of an RDD which data is lost or not available,
                    and <b>Secondly</b>, it is used by Spark to figure out an <b>optimal execution plan</b> we'll talk
                    about in a bit.
                </aside>
                <div style="display: block; text-align: center; margin-top:80px">
                    Lineage<br/>
                    <div style="display: inline-block; width:290px;padding:10px;margin: 10px;font-size: 35px;">
                        <div style="margin:10px;padding:5px;border: 1px solid white">textFile(...)</div>
                        <div class="up"><div class="arrow"></div></div>
                        <div style="margin:10px;padding:5px;border: 1px solid white">.filter(...)</div>
                        <div class="up"><div class="arrow"></div></div>
                        <div style="margin:10px;padding:5px;border: 1px solid white">.map(...)</div>
                        <div class="up"><div class="arrow"></div></div>
                        <div style="margin:10px;padding:5px;border: 1px solid white">.reduceByKey(...)</div>
                    </div>
                    <div style="display: inline-block; width:290px;font-size: 35px;">
                        <div style="margin:10px;padding:5px;border: 1px solid white">HadoopRDD</div>
                        <div class="up"><div class="arrow"></div></div>
                        <div style="margin:10px;padding:5px;border: 1px solid white">FilteredRDD</div>
                        <div class="up"><div class="arrow"></div></div>
                        <div style="margin:10px;padding:5px;border: 1px solid white">MappedRDD</div>
                        <div class="up"><div class="arrow"></div></div>
                        <div style="margin:10px;padding:5px;border: 1px solid white">ShuffledRDD</div>
                    </div>
                </div>
            </section>
            <section data-transition="none none">
                <aside class="notes">
                    Alright, let's quickly look at how a <b>transformation is applied</b>. So as I told you, whenever a computation
                    is needed, the <b>driver</b> sends it to each executor which starts computing <b>as many partition</b> as they have
                    cores available.<br/><br/>

                    Take the <b>map function</b> for example. It takes a partition, process each of its elements, and then
                    store the result in a new partition. Another example is <b>union</b>. Union takes two partition, and merges
                    them into one single partition. The <b>operations</b> involved by these transformations, are all
                    performed locally on <b>each executor</b> without involving any data movement.<br/><br/>

                    Now some transformations may have to <b>access to partitions stored on different machines</b> For
                    example, <b>groupByKey</b> takes an RDD of key value pairs and <b>regroup</b> all the pairs having the same key in
                    the same partition. As those pairs may be stored on <b>different executors</b>, the data has to be
                    re-distributed all over the cluster, in order to get <b>all the pairs</b> with the same key at the same
                    location.<br/><br/>

                    Long story short, some transformations imply <b>data redistribution</b> and some others not. In Spark terminology,
                    the operation consisting in <b>redistributing</b> the data is referred to as a <b>shuffle</b>. Transformations which
                    do not require a shuffle are called <b>narrow transformations</b> while those requiring one are referred
                    to as <b>wide transformations</b>.
                </aside>
                <div style="display:table;margin: 100px auto 50px auto;">
                    <div style="display:table-cell;vertical-align: middle; ">
                        <div class="executor fragment" data-fragment-index="1" style="margin-left:90px;margin-right: 22px;">
                            <div class="body" style="border: 0; display: inline-block; vertical-align: middle">
                                <div class="partition-list">
                                    <div class="block blue">&nbsp;</div>
                                    <div class="block blue" style="opacity: 0.7;">&nbsp;</div>
                                    <div class="block blue" style="opacity: 0.2;">&nbsp;</div>
                                    <div class="block blue">&nbsp;</div>
                                </div>
                            </div>
                            <div style="display: inline-block; margin: 0 5px 22px 5px;vertical-align: middle">
                                <div class="right" ><div class="arrow"></div></div><br/>
                            </div>
                            <div class="body" style="border: 0; display: inline-block;vertical-align: middle">
                                <div class="partition-list" style="vertical-align: middle">
                                    <div class="block red">&nbsp;</div>
                                    <div class="block red" style="opacity: 0.7;">&nbsp;</div>
                                    <div class="block red" style="opacity: 0.2;">&nbsp;</div>
                                    <div class="block red">&nbsp;</div>
                                </div>
                            </div>
                        </div>
                        <div class="executor fragment" data-fragment-index="2" style="margin-left:90px;margin-right: 22px;">
                            <div class="body" style="border: 0; display: inline-block; vertical-align: middle">
                                <div class="partition-list">
                                    <div class="block blue">&nbsp;</div>
                                    <div class="block blue" style="opacity: 0.7;">&nbsp;</div>
                                    <div class="block blue" style="opacity: 0.2;">&nbsp;</div>
                                    <div class="block blue">&nbsp;</div>
                                </div>
                                <div class="partition-list">
                                    <div class="block red">&nbsp;</div>
                                    <div class="block red" style="opacity: 0.7;">&nbsp;</div>
                                    <div class="block red" style="opacity: 0.2;">&nbsp;</div>
                                    <div class="block red">&nbsp;</div>
                                </div>
                            </div>
                            <div style="display: inline-block; margin: 0px 5px 22px 5px;vertical-align: middle">
                                <div class="down-r" ><div class="arrow"></div></div><br/>
                                <div class="up-r" ><div class="arrow"></div></div>
                            </div>
                            <div class="body" style="border: 0; display: inline-block;vertical-align: middle">
                                <div class="partition-list" style="vertical-align: middle">
                                    <div class="block blue">&nbsp;</div>
                                    <div class="block red" style="opacity: 0.7;">&nbsp;</div>
                                    <div class="block blue" style="opacity: 0.7;">&nbsp;</div>
                                    <div class="block">...</div>
                                </div>
                            </div>
                        </div>
                    </div>
                    <div class="fragment" data-fragment-index="1" style="display:table-cell;vertical-align: middle;padding: 20px;border-left: 3px solid white">
                        <ul style="margin-left:50px;">
                            <li>map</li>
                            <li>flatMap</li>
                            <li class="fragment" data-fragment-index="2">union</li>
                            <li class="fragment" data-fragment-index="2">...</li>
                        </ul>
                    </div>
                </div>
                <div style="height:30px;"></div>
                <div style="display:table;margin: auto 0 auto 100px;">
                    <div style="display:table-cell;vertical-align: middle">
                        <div class="executor fragment" data-fragment-index="3" style="margin-left:90px;margin-right:30px;">
                            <div class="body" style="border: 0; display: inline-block; vertical-align: middle">
                                <div class="partition-list">
                                    <div class="block blue">&nbsp;</div>
                                    <div class="block blue" style="opacity: 0.7;">&nbsp;</div>
                                    <div class="block blue" style="opacity: 0.2;">&nbsp;</div>
                                    <div class="block blue">&nbsp;</div>
                                </div>
                                <div class="partition-list">
                                    <div class="block red">&nbsp;</div>
                                    <div class="block red" style="opacity: 0.7;">&nbsp;</div>
                                    <div class="block red" style="opacity: 0.2;">&nbsp;</div>
                                    <div class="block red">&nbsp;</div>
                                </div>
                            </div>
                            <div style="display: inline-block; margin: 0 5px 30px 5px;vertical-align: middle">
                                <div class="up-r" ><div class="arrow"></div></div><br/>
                                <div class="right" ><div class="arrow"></div></div><br/>
                                <div class="down-r" ><div class="arrow"></div></div><br/>
                            </div>
                            <div class="body" style="border: 0; display: inline-block;vertical-align: middle">
                                <div class="partition-list" style="vertical-align: middle">
                                    <div class="block blue">&nbsp;</div>
                                    <div class="block blue" style="opacity: 0.7;">&nbsp;</div>
                                    <div class="block red" style="opacity: 0.2;">&nbsp;</div>
                                    <div class="block red">&nbsp;</div>
                                </div>
                                <div class="partition-list" style="vertical-align: middle">
                                    <div class="block blue" style="opacity: 0.7;">&nbsp;</div>
                                    <div class="block red" style="opacity: 0.2;">&nbsp;</div>
                                    <div class="block red">&nbsp;</div>
                                    <div class="block blue" style="opacity: 0.7;">&nbsp;</div>
                                </div>
                                <div class="partition-list" style="vertical-align: middle">
                                    <div class="block" style="width:120px;">...</div>
                                </div>
                            </div>
                        </div>
                    </div>
                    <div class="fragment" data-fragment-index="3" style="display:table-cell;vertical-align: middle;padding: 20px;border-left: 3px solid white">
                        <ul style="margin-left:50px;">
                            <li>reduceByKey</li>
                            <li>groupByKey</li>
                            <li>join*</li>
                            <li>...</li>
                        </ul>
                    </div>
                </div>
            </section>

            <section data-transition="none none">
                <aside class="notes">
                    What does that imply <b>concretely</b>? Well a <b>narrow transformation</b> can be applied on each partition
                    without involving any <b>data movement</b>. However, when you get into the second case, nothing is that
                    sure anymore. <b>Data needs to be moved</b>, and therefore has to be <b>serialized first</b>, sent over the network,
                    and <b>deserialized</b> once received.<br/><br/>

                    As you can guess, this may be a pretty expensive operation especially if the data to shuffle reaches
                    a certain size. Actually we'll see that this is just the tip of the iceberg.<br/><br/>

                    So this was an overview of what transformations and actions. Let's see how they are executed now.
                </aside>
                <div style="display:table;margin: auto">
                    <div style="display:table-cell;vertical-align: middle;width:100px;">
                        <div class="rectangle blue-border">
                            <div class="child blue-border"><div class="child blue-border" style="line-height:70px;"><span style="font-size: 30px;">Data</span></div></div>
                        </div>
                    </div>
                    <div style="display:table-cell;vertical-align: middle;width:150px;">
                        <div style="margin-top:50px;" class="up-r"><div class="arrow"></div></div><br/>
                        <div class="right"><div class="arrow"></div></div><br/>
                        <div class="down-r"><div class="arrow"></div></div>
                    </div>
                    <div style="display:table-cell;vertical-align: middle;">
                        <div class="executor" style="margin-top:30px;">
                            <div class="title"><span>&nbsp;Executor&nbsp;</span></div>
                            <div class="body">
                                <div class="partition-list" style="display: inline-block">
                                    <div class="block blue">&nbsp;</div>
                                    <div class="block blue" style="opacity: 0.7;">&nbsp;</div>
                                    <div class="block blue" style="opacity: 0.2;">&nbsp;</div>
                                </div>
                                <div class="right" style="display: inline-block"><div class="arrow"></div></div>
                                <div class="partition-list" style="display: inline-block">
                                    <div class="block red">&nbsp;</div>
                                    <div class="block red" style="opacity: 0.7;">&nbsp;</div>
                                    <div class="block red" style="opacity: 0.2;">&nbsp;</div>
                                </div>
                            </div>
                        </div>
                        <div class="executor">
                            <div class="title"><span>&nbsp;Executor&nbsp;</span></div>
                            <div class="body">
                                <div class="partition-list" style="display: inline-block">
                                    <div class="block blue">&nbsp;</div>
                                    <div class="block blue" style="opacity: 0.7;">&nbsp;</div>
                                    <div class="block blue" style="opacity: 0.2;">&nbsp;</div>
                                </div>
                                <div class="right" style="display: inline-block"><div class="arrow"></div></div>
                                <div class="partition-list" style="display: inline-block">
                                    <div class="block green">&nbsp;</div>
                                    <div class="block green" style="opacity: 0.7;">&nbsp;</div>
                                    <div class="block green" style="opacity: 0.2;">&nbsp;</div>
                                </div>
                            </div>
                        </div>
                    </div>
                    <div class="fragment" data-fragment-index="2" style="display:table-cell;width:70px;vertical-align: middle;">
                        <div class="up-r" style="margin-top:50px;"  ><div class="arrow"></div></div><br/>
                        <div class="right" ><div class="arrow"></div></div><br/>
                        <div class="down-r"><div class="arrow"></div></div>
                    </div>
                    <div class="fragment" data-fragment-index="2" style="display:table-cell;vertical-align: middle;">
                        <div class="executor" style="margin-top:30px;">
                            <div class="title"><span>&nbsp;Executor&nbsp;</span></div>
                            <div class="body">
                                <div class="partition-list" style="display: inline-block">
                                    <div class="block green">&nbsp;</div>
                                    <div class="block red" style="opacity: 0.7;">&nbsp;</div>
                                </div>
                            </div>
                        </div>
                        <div class="executor">
                            <div class="title"><span>&nbsp;Executor&nbsp;</span></div>
                            <div class="body">
                                <div class="partition-list" style="display: inline-block">
                                    <div class="block green">&nbsp;</div>
                                    <div class="block green" style="opacity: 0.2;">&nbsp;</div>
                                </div>
                                <div class="partition-list" style="display: inline-block">
                                    <div class="block red" style="opacity: 0.7;">&nbsp;</div>
                                    <div class="block red" style="opacity: 0.2;">&nbsp;</div>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </section>
            <section data-transition="none none">
                <aside class="notes">
                    Let's look at some code now and see what part of Spark program runs on the Driver and on its
                    executors.<br/><br/>

                    Remember that every transformation is executed on the executors. Therefore, when you read a Spark
                    program, keep in mind that some part of the code is run on the driver, while some others are
                    performed on the executors. And some others may actually involve both.<br/><br/>

                    As you can see, Spark's API abstracts how the code is concretely run. This is actually great but
                    sometime a bit confusing.<br/><br/>
                </aside>
                <div style="text-align: left;margin-top:50px;display:inline-block;">
                    <ul style="margin-left: 0;margin-right:80px; list-style-type:none;white-space: nowrap;vertical-align: top;">
                        <li class="fragment highlight-light-blue"  data-fragment-index="1">val output = sparkContext</li>
                        <li class="fragment highlight-red"   data-fragment-index="2" style="padding-left: 50px">.textFile("hdfs://...")</li>
                        <li class="fragment highlight-red"   data-fragment-index="2" style="padding-left: 50px">.flatMap(_.split(" "))</li>
                        <li class="fragment highlight-red"   data-fragment-index="2" style="padding-left: 50px">.map((_, 1))</li>
                        <li class="fragment highlight-red"   data-fragment-index="2" style="padding-left: 50px">.reduceByKey(_ + _)</li>
                        <li class="fragment highlight-green" data-fragment-index="3" style="padding-left: 50px">.collect()</li>
                        <li class="fragment highlight-light-blue"  data-fragment-index="1">print output</li>
                    </ul>
                </div>
                <div style="text-align: left; margin-top:60px;display:inline-block; font-size: 30px;width:460px;">
                    <ul style="list-style: none;vertical-align: top;">
                        <li class="fragment" data-fragment-index="1"><div class="box blue"  style="display:inline-block;vertical-align: middle"></div>&nbsp;Executed on the driver</li>
                        <li class="fragment" data-fragment-index="2"><div class="box red"   style="display:inline-block;vertical-align: middle"></div>&nbsp;Executed on the executors</li>
                        <li class="fragment" data-fragment-index="3"><div class="box light-green" style="display:inline-block;vertical-align: middle"></div>&nbsp;Both may be involved</li>
                    </ul>
                </div>
            </section>
            <section data-transition="none none">
                <aside class="notes">
                    Having this in mind, is there anyone who could tell me what is the value printed out once this program
                    is run?<br/><br/>

                    In this example, the <b>closure</b> inside the foreach function refers to the counter variable. In
                    order to <b>distribute</b> this operation, Spark has to <b>serialize it</b> along with any variable
                    it refers to (so, in our case, the <b>counter</b> variable).<br/><br/>

                    During this <b>serialization process</b>, the counter variable will be <b>copied</b> and sent along
                    with the <b>closure</b>. So at some point, the <b>counter</b> variable used by the driver and its
                    executors <b>are no longer the same</b>.<br/><br/>

                    Let's get further.
                </aside>
                <ul style="list-style-type:none;padding-right: 30px">
                    <li class="fragment highlight-green" data-fragment-index="1">var counter = 0</li>
                    <li class="fragment highlight-green" data-fragment-index="1">val rdd = sc.parallelize(Seq(1, 2, 3, ...))</li>
                    <li>&nbsp;</li>
                    <li>rdd.map { rddItem ⇒
                    <li class="fragment highlight-red" data-fragment-index="2">&nbsp;&nbsp;&nbsp;counter += 1</li>
                    <li class="fragment highlight-red" data-fragment-index="2">&nbsp;&nbsp;&nbsp;rddItem</li>
                    <li>}.collect()<br/>&nbsp;</li>
                    <li class="fragment highlight-green" data-fragment-index="1">print("Counter value: " + counter)</li>
                </ul>
            </section>
            <section data-transition="none none">
                <aside class="notes">
                    What about this code now? Anybody can tell me what is going on here?<br/><br/>

                    Well this program <b>cannot run</b>. Remember, a transformation has to be <b>serialized</b> in order to be
                    executed, and a JDBC connection cannot be serialized.<br/><br/>

                    Alright, we can fix the problem like this. Any concern regarding the solution?<br/><br/>

                    Well whenever we run this program, a <b>JDBC connection</b> will be requested for each record of the dataset.
                    Which is pretty bad. Actually, do you have an idea about what would happen if initializing this
                    connection would require a <b>lot of memory</b>? Well it would imply many <b>garbage collections</b>
                    cycles. Whenever you do this, look at the number of records first, just in case of.<br/><br/>

                    This brings us to ask <b>two questions</b>. How can we define variables which are shared by the driver and
                    its executors? And secondly what can we do about variables which cannot be <b>serialized</b>b>?<br/><br/>

                    Alright, let's start with the second question as it's easier to answer.
                </aside>
                <pre><code class="scala" style="font-size: 25px;">val rdd = sc.parallelize(Seq(1, 2, 3, 4))

val con = ... // get JDBC connection
rdd.map { record ⇒
    findOrderById(con, record).length
}</code></pre>

                <pre class="fragment"><code class="scala" style="font-size: 25px;">val rdd = sc.parallelize(Seq(1, 2, 3, 4))

rdd.map { record ⇒
    val con = ... // get JDBC connection
    findOrderById(con, record).length
}</code></pre>
            </section>
            <section data-transition="none none">
                <aside class="notes">
                    In order to solve this problem, we'll use a <b>special transformation</b> called mapPartition. Basically
                    mapPartition allows you to execute a function for <b>each partition</b> of the dataset. This function
                    takes an <b>iterator</b> which iterates over the records contained in a <b>given partition</b>, and returns
                    another iterator. This code will now request a <b>JDBC connection</b> for each partition instead of
                    requesting one for each record.<br/><br/>

                    We won't get into details here, but almost all transformations provided by Spark are actually
                    a <b>specialization</b> of the mapPartition function. One thing to watch for when you use <b>mapPartition</b> is
                    to resist the temptation of <b>evaluating</b> the iterator by getting its size or its content. Doing so
                    would force the loading of all the records of the partition being <b>processed</b>b, which may throw an
                    OME.<br/><br/>

                    Now what about the other question, regarding how a variable can be shared between the driver and
                    its executors. Well, in order to answer that one, let's move on to the next topic.
                </aside>
                <pre><code class="scala" style="font-size: 25px;">val rdd = sc.parallelize(Seq(1, 2, 3, 4))

rdd.map { record ⇒
    val con = ... // get JDBC connection
    findOrderById(con, record).length
}</code></pre>

                <pre><code class="scala" style="font-size: 25px;">rdd.mapPartitions { recordIterator ⇒
    val con = ... // get JDBC connection
    recordIterator.map { record ⇒
        findOrderById(con, record).length
    }
}</code></pre>
            </section>
        </section>

        <section>
            <h2>Shared variables</h2>
            <section data-transition="none none">
                <aside class="notes">
                    Now let's go back to the <b>previous example</b>. The whole point of this code is to <b>share</b> a
                    variable between the <b>Driver</b> and its <b>executor</b>. However, and as we've seen it, this
                    cannot work for the reasons we've mentioned earlier. But actually, it is even <b>worse</b> than that. <br/>

                    Actually, the <b>serialization process</b> we've talked about earlier is done <b>as many time as</b>
                    there are partitions. So if your <b>cluster</b> contains <b>10 nodes</b> with a <b>hundred partitions</b>
                    each, this closure will be <b>serialized</b> and sent through the network a <b>thousand</b> times.<br/><br/>

                    Now if the <b>counter</b> variable is actually a <b>huge object</b>, this may get very <b>inefficient</b>
                    at some point. So the whole problem here is not only about <b>sharing a variable</b>, but also to do
                    it while <b>minimizing IOs</b> and in a <b>safe way</b> in terms of <b>concurrency</b>.

                    In order to resolve this problem, Sparks supports <b>2 types of shared variables</b> : <b>Accumulators</b>
                    and <b>Broadcast variables</b>. The first is designed to send a value from the <b>executors</b> to their
                    <b>driver</b>, while the <b>second one</b> is for the <b>other way around</b>.
                </aside>
                <div style="text-align: left">
                    <pre style="width:100%;"><code class="scala" style="font-size: 25px;">var counter = 0
val rdd = sc.parallelize(data)

rdd.map { rddItem ⇒ counter += 1 }.collect()
print("Counter value: " + counter)</code></pre>

                    <pre class="fragment" style="width:100%;"><code class="scala" style="font-size: 25px;">val hugeArray = ...
val bigRddWithIndex = ...

bigRddWithIndex.map { index ⇒ hugeArray[rddItem.key] }
...</code></pre>
                </div>
                <p class="fragment">Accumulators and Broadcast variables !</p>
            </section>

            <section data-transition="none none">
                <aside class="notes">
                    Accumulators are overall designed to <b>aggregate</b> values coming from the executors to the
                    driver. In order to make this safely, an accumulator's value is <b>only visible by the driver</b>,
                    and can be <b>updated only by its executors</b>.<br/><br/>

                    Now as you remember, an RDD may be <b>recomputed</b> for different reasons, which means that, if
                    updated inside a <b>transformation</b>, an <b>accumulator</b> may be modified more than required.
                    Actually, Spark <b>guarantees</b> that for a <b>given accumulator</b>, updates are applied <b>only
                    once</b> if they are done inside an <b>action</b>, but this is <b>not true</b> if they are applied inside
                    a <b>transformation</b>. Actually, this feature has been a lot <b>criticized</b>, as it has been found out that
                    in some edge cases, <b>accumulators</b> may be updated more than required even if those <b>updates</b> are done
                    inside an action. Therefore, I strongly suggest you to use them only for <b>debugging purpose</b>, or if
                    they are idempotent.
                </aside>
                <table>
                    <tr>
                        <td style="vertical-align: middle">
                            Accumulators
                        <pre style="width:100%;"><code class="scala" style="font-size: 25px;">val counter = sc.accumulator(0)
var rdd = sc.parallelize(data)

rdd.foreach {
    rddItem ⇒ counter += 1
}
print(counter.value)</code></pre>
                        </td>
                        <td><img style="margin-top:60px; box-shadow: 0; background-color: #FFFFFF; width:100%"
                                 src="images/spark_scala_meetup/accumulators.png"/></td>
                    </tr>
                    <tr>
                        <td colspan="2">Accumulators aggregate values coming from the executors to the driver</td>
                    </tr>
                </table>
            </section>

            <section data-transition="none none">
                <aside class="notes">
                    In a previous example, we were wondering about how it would be possible to share a <b>heavy
                    object</b> without <b>impacting network throughput</b>, or dealing with <b>concurrency issues</b>.
                    Well <b>broadcast variables</b> are exactly designed for this purpose. Basically, a broadcast variable
                    is able to wrap a value and is guaranteed to be <b>sent only once</b> per worker node. This is
                    especially useful if you need to share a <b>lookup table</b> across a cluster's nodes. <br/><br/>

                    In this example, we <b>wrap a big array</b> inside a <b>broadcast variable</b>, and are able to access it by
                    calling the <b>value property</b>. <!--Now no matter if this value is <b>mutable or not</b>, any of its
                    <b>updates</b> won't ever be <b>propagated</b> to the other nodes and will remain <b>local only</b>.<br/><br/>-->

                    Now <b>broadcast variables</b> may be a <b>bottleneck</b> at some point, as serializing and deserializing
                    a data structure can be <b>sometime expensive</b>. So it is important to choose a proper <b>data structure</b>
                    along with a good <b>serialization library</b>. Spark comes by default with <b>Kryo</b> but you can
                    also use other libraries or your own <b>serialization routines</b>.
                </aside>
                <table>
                    <tr>
                        <td style="vertical-align: middle">
                            Broadcast variables
                        <pre style="width:100%;"><code class="scala" style="font-size: 25px;">val bigArray = sc.broadcast(...)
var bigRddWithIndex = ...

bigRddWithIndex.map { e ⇒
  broadcastedArray.value[e.key]
}</code></pre>
                        </td>
                        <td><img style="margin-top:60px; box-shadow: 0; background-color: #FFFFFF; width:100%"
                                 src="images/spark_scala_meetup/broadcast_var.png"/></td>
                    </tr>
                    <tr>
                        <td colspan="2">Broadcast variables propagate a read-only value to all executors</td>
                    </tr>
                </table>
            </section>
        </section>

        <section data-transition="none none">
            <h2>RDD : Persistence and Caching</h2>
            <section data-transition="none none">
                <aside class="notes">
                    As we've mentioned it, Spark will <b>recompute</b> an RDD <b>any time</b> an action is being
                    performed on it. As you may guess, recomputing an RDD can quickly become a burden depending on the
                    size of its dataset.<br/><br/>

                    So in order to prevent that, Spark provides caching features allowing an RDDs data to be stored
                    either in <b>memory, on disk, or both of them</b>. This is especially useful, if you need to iterate
                    over an RDD, like when you're training a machine learning model for example.<br/><br/>

                    Persisting an RDD allows <b>future actions</b> to be much faster, actually often by more than 10
                    times. In order to do it, all you need is to marked the RDD as persisted, using the persist method
                    and to specify a storage level.<br/><br/>

                    ***You can also use the <b>cache method</b>, which basically is like persist but using the default
                    storage that is in memory.<br/><br/>

                    ***Finally, RDDs come with a method <b>unpersist()</b> that lets you manually remove them from the
                    cache.
                </aside>
                <pre><code class="scala" style="font-size: 25px;">val rdd = input.map(x ⇒ x * x)
rdd.count()      // RDD is computed first here
result.collect() // RDD is recomputed here</code></pre>

                <pre class="fragment" style="font-size: 25px;"><code class="scala">val rdd = input.map(x ⇒ x * x)
rdd.persist(StorageLevel.DISK_ONLY)

rdd.count()      // RDD is computed first here
result.collect() // RDD won' be recomputed here</code></pre>

                <pre class="fragment" ><code class="scala"  style="font-size: 25px;">result.cache()     // Use memory storage level
result.unpersist() // Suggest a manual eviction
</code></pre>
            </section>

            <section data-transition="none none">
                <aside class="notes">
                    Now no matter if you decide to cache an RDD's data in the memory or in the disk, keep in mind that
                    Spark will always favor memory over disk in order to allows operations to run as fast as possible.
                    So, data will be <b>spilled</b> on the disk only if you attempt to cache <b>too much data to fit in
                    memory</b>. Secondly, Spark will evict old cached partitions using an LRU cache policy.<br/><br/>

                    Here are the <b>standard ways</b> of caching an RDD's data. As you can see each has its <b>tradeoffs</b>
                    and <b>advantages</b>. You can also <b>replicate</b> an RDD's data over <b>several nodes</b> or, if
                    the RDD contains <b>too much</b> data to fit on a <b>single node</b>, caching it on multiple nodes.

                    Overall, this means that you should not worry too much about your <b>job breaking</b> if you ask
                    Spark to cache <b>too much data</b>. <b>However</b>, keep in mind that caching <b>unnecessary</b>
                    data can lead to more <b>data evictions</b> and <b>re-computation time</b>.
                </aside>
                <table>
                    <tr>
                        <td>Level</td>
                        <td>Space</td>
                        <td>CPU</td>
                        <td>RAM</td>
                        <td>Disk</td>
                    </tr>
                    <tr>
                        <td>MEMORY_ONLY</td>
                        <td>High</td>
                        <td>Low</td>
                        <td>Y</td>
                        <td>N</td>
                    </tr>
                    <tr>
                        <td>DISK_ONLY</td>
                        <td>Low</td>
                        <td>High</td>
                        <td>N</td>
                        <td>Y</td>
                    </tr>
                    <tr>
                        <td>MEMORY_AND_DISK</td>
                        <td>High</td>
                        <td>Medium</td>
                        <td>Some</td>
                        <td>Some</td>
                    </tr>
                    <tr>
                        <td>MEMORY_ONLY_SER*</td>
                        <td>Low</td>
                        <td>High</td>
                        <td>Y</td>
                        <td>N</td>
                    </tr>
                    <tr>
                        <td>MEMORY_AND_DISK_SER*</td>
                        <td>Low</td>
                        <td>High</td>
                        <td>Some</td>
                        <td>Some</td>
                    </tr>
                    <tr>
                        <td>&nbsp;</td>
                    </tr>
                    <tr>
                        <td colspan="5">(*) : Data is serialized before being persisted (takes less space)</td>
                    </tr>
                </table>
            </section>
            <section data-transition="none none">
                <aside class="notes">
                    Now another caching feature we did not talk about is <b>checkpointing</b>. In contrast with persisting,which
                    stores an RDD's partitions in the <b>caching layer</b> of each executor, checkpointing writes them to an
                    <b>external storage</b> such as HDFS.<br/><br/>

                    Persisting is performed in the <b>executor's JVM</b>, and therefore it may take space that could be used for
                    other <b>computations</b> or increase the risk of memory failures. As <b>checkpointing</b> allow an RDD to be stored
                    outside the executor, it allows to leave space for further <b>computations</b>. Also Checkpointing allows an
                    RDD's data to survive beyond duration of a Spark application<br/><br/>

                    However, it has one <b>drawback</b>. Checkpointing may be slower than persisting. No magic here.<br/><br/>

                    <!--One last thing, checkpoint is an <b>action</b>. Therefore, if you perform any other computations with the-->
                    <!--checkpointed RDD, think about caching it before checkpointing it.<br/><br/>-->
                </aside>
                Checkpointing: Write an RDD on an external storage
                <pre style="font-size: 25px;"><code class="scala">sc.setCheckpointDir("...")
someRDD.checkpoint // Materialize the RDD</code></pre>
                <div style="text-align: left">
                    <ul>
                        <li>Release pressure on executor's memory/disk</li>
                        <li>RDD's data survives beyond a Spark application</li>
                        <li>Checkpointing may be slower</li>
                    </ul>
                </div>
            </section>
            <section data-transition="none none">
                <aside class="notes">
                    Now you may wonder, when should I <b>persist, cache or checkpoint</b> an RDD. Well it's very hard to answer
                    and is a case by case problem. Actually there is a very good book which addresses this question, and
                    I will provide you with its reference at the end of this talk.<br/><br/>

                    Keep in mind, that <b>persisting or checkpointing</b> an RDD has some cost, and it can be so high
                    that recomputing can be a better option sometime.<br/><br/>

                    So here are some best practices. First of all, you should rely on persisting whenever <b>multiple</b>
                    actions have to be performed on the same RDD.<br/><br/>

                    Secondly, whenever a <b>long chain of transformation</b> gets too expansive to compute or <b>prone to fail</b>.
                    Breaking the <b>whole chain</b> may allow you to release some <b>pressure on the memory</b> and the CPU as well.<br/>

                    In general, it is worth reusing an <b>RDD</b> rather than recomputing it, if the <b>computation is large</b>, relative
                    to your cluster and the rest of your job.
                </aside>
                <div style="text-align: left">
                    Best Practices:<br/>
                    <ul>
                        <li>Iterative computations / Multiple actions</li>
                        <li>Breaking Long chain of (expansive) transformations</li>
                        <li>If the cost to compute each partition is too high</li>
                    </ul>
                </div>
            </section>
        </section>

        <section data-transition="none none">
            <h2>Spark Execution Plan</h2>
            <section data-transition="none none">
                <aside class="notes">
                    Now that you have a <b>basic understanding</b> of how an RDD works, we can get a little bit more
                    into details and see how it is <b>computed</b> by Spark<br/><br/>

                    Whenever an <b>action</b> is called on an RDD, Spark <b>translates</b> the lineage of the RDD to a <b>execution plan</b>.
                    First it traverses the <b>whole graph</b> starting by the <b>last transformation</b> requested and goes <b>backward</b>
                    until the creation of the RDD.<br/><br/>

                    Once this done, Spark <b>divides</b> the work to be done into one to <b>multiples stages</b> depending on the
                    nature of the transformations involved.<br/><br/>

                    A stage is basically a set of <b>one to many</b> transformations which do not require any <b>data
                    movement</b> or <b>shuffling</b>, and which can be therefore <b>pipelined</b>. It's some kind of a
                    <b>super-operation</b> in which <b>transformations</b> have been <b>fused</b> together in order to
                    not going over the data <b>multiple time</b> and to avoid the <b>overhead</b> of each operation if
                    they were <b>executed</b> one after the other.<br/><br/>

                    Now, each stage is composed of as <b>many task</b> as there are partitions available <b>when its
                    executed</b>. So if you have <b>10 partitions</b> for a given stage, the scheduler will create
                    <b>10 tasks</b> for it. Now as you may guess, <b>the more task</b> your whole Spark process needs
                    to execute, <b>the longer</b> it will take to compute the final result. So one way to optimize
                    this, is to <b>prevent shuffling</b> to happen, or at least limit its <b>impact on performances</b>,
                    which is exactly what we're about to discuss.
                </aside>
                <div style="display:table;margin: 120px auto auto auto">
                    <div style="display: table-caption; text-align: center;margin-bottom: 80px;">
                        <div class="fragment border" data-fragment-index="1" style="display: inline-block">
                            <div style="display: inline-block">&nbsp;textFile&nbsp;</div>
                            <div class="left" style="vertical-align: middle"><div class="arrow"></div></div>
                            <div style="display: inline-block">&nbsp;flatMap&nbsp;</div>
                            <div class="left" style="vertical-align: middle"><div class="arrow"></div></div>
                            <div style="display: inline-block">&nbsp;map&nbsp;</div>
                        </div>
                        <div class="left" style="vertical-align: middle"><div class="arrow"></div></div>
                        <div class="fragment border" data-fragment-index="2" style="display: inline-block">
                            <div style="display: inline-block">&nbsp;reduceByKey&nbsp;</div>
                        </div>
                    </div>
                    <div style="display:table-row;">
                        <div style="display:table-cell;vertical-align: middle;width:100px;">
                            <div class="rectangle blue-border">
                                <div class="child blue-border"><div class="child blue-border" style="line-height:70px;"><span style="font-size: 30px;">Data</span></div></div>
                            </div>
                        </div>
                        <div style="display:table-cell;vertical-align: middle;width:150px;">
                            <div style="margin-top:50px;" class="up-r"><div class="arrow"></div></div><br/>
                            <div class="right"><div class="arrow"></div></div><br/>
                            <div class="down-r"><div class="arrow"></div></div>
                        </div>
                        <div class="fragment border" data-fragment-index="1" style="display:table-cell;vertical-align: middle;margin-top:25px;padding:0 5px 0 5px;">
                            <div style="display:table-cell;vertical-align: middle;">
                                <div class="executor">
                                    <div class="title"><span>&nbsp;Executor&nbsp;</span></div>
                                    <div class="body">
                                        <div class="partition-list" style="display: inline-block">
                                            <div class="block blue">&nbsp;</div>
                                            <div class="block blue" style="opacity: 0.7;">&nbsp;</div>
                                            <div class="block blue" style="opacity: 0.2;">&nbsp;</div>
                                        </div>
                                        <div class="right" style="display: inline-block"><div class="arrow"></div></div>
                                        <div class="partition-list" style="display: inline-block">
                                            <div class="block red">&nbsp;</div>
                                            <div class="block red" style="opacity: 0.7;">&nbsp;</div>
                                            <div class="block red" style="opacity: 0.2;">&nbsp;</div>
                                        </div>
                                    </div>
                                </div>
                                <div class="executor">
                                    <div class="title"><span>&nbsp;Executor&nbsp;</span></div>
                                    <div class="body">
                                        <div class="partition-list" style="display: inline-block">
                                            <div class="block blue">&nbsp;</div>
                                            <div class="block blue" style="opacity: 0.7;">&nbsp;</div>
                                            <div class="block blue" style="opacity: 0.2;">&nbsp;</div>
                                        </div>
                                        <div class="right" style="display: inline-block"><div class="arrow"></div></div>
                                        <div class="partition-list" style="display: inline-block">
                                            <div class="block green">&nbsp;</div>
                                            <div class="block green" style="opacity: 0.7;">&nbsp;</div>
                                            <div class="block green" style="opacity: 0.2;">&nbsp;</div>
                                        </div>
                                    </div>
                                </div>
                            </div>
                        </div>
                        <div style="display:table-cell;width:70px;vertical-align: middle;">
                            <div class="up-r"  ><div class="arrow"></div></div><br/>
                            <div class="right" ><div class="arrow"></div></div><br/>
                            <div class="down-r"><div class="arrow"></div></div>
                        </div>
                        <div class="fragment border" data-fragment-index="2" style="display:table-cell;margin-top:25px;padding:0 5px 0 5px;vertical-align: middle;">
                            <div class="executor">
                                <div class="title"><span>&nbsp;Executor&nbsp;</span></div>
                                <div class="body">
                                    <div class="partition-list" style="display: inline-block">
                                        <div class="block green">&nbsp;</div>
                                        <div class="block red" style="opacity: 0.7;">&nbsp;</div>
                                    </div>
                                </div>
                            </div>
                            <div class="executor">
                                <div class="title"><span>&nbsp;Executor&nbsp;</span></div>
                                <div class="body">
                                    <div class="partition-list" style="display: inline-block">
                                        <div class="block green">&nbsp;</div>
                                        <div class="block green" style="opacity: 0.2;">&nbsp;</div>
                                    </div>
                                    <div class="partition-list" style="display: inline-block">
                                        <div class="block red" style="opacity: 0.7;">&nbsp;</div>
                                        <div class="block red" style="opacity: 0.2;">&nbsp;</div>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </div>
                    <div style="display:table-row;vertical-align: middle;width:100px;">
                        <div style="display:table-cell;"></div>
                        <div style="display:table-cell;"></div>
                        <div class="fragment" data-fragment-index="1" style="display:table-cell;text-align: center">
                            <span class="fragment flip" data-fragment-index="3">Stage 1</span>
                            <div class="fragment" data-fragment-index="3">
                                <div class="down"><div class="arrow orange"></div></div>
                                <div class="rectangle small orange-border" style="margin: 0 auto 0 auto">
                                    <div class="child small orange-border"><div class="child small orange-border">&nbsp;</div></div>
                                </div>
                            </div>
                        </div>
                        <div style="display:table-cell;"></div>
                        <div class="fragment" data-fragment-index="2" style="display:table-cell;text-align: center">
                            <span class="fragment flip" data-fragment-index="3">Stage 2</span>
                            <div class="fragment" data-fragment-index="3">
                                <div class="down"><div class="arrow orange"></div></div>
                                <div class="rectangle small orange-border" style="margin: 0 auto 0 auto">
                                    <div class="child small orange-border"><div class="child small orange-border">&nbsp;</div></div>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </section>
        </section>
        <section data-transition="none none">
            <h2>Efficient Shuffles</h2>
            <section data-transition="none none">
                <aside class="notes">
                    Alright, what's the problem with shuffles.<br/><br/>

                    We saw that first it implies <b>data movement</b> which may be expensive.<br/><br/>

                    Also, and as we've just seen it, the more shuffles you do, the more <b>stages</b> are created.
                    And Stages are <b>sequential</b>, so Spark cannot start s stage before making sure that the <b>dataset</b> has
                    been <b>completely</b> processed by the previous stage.<br/><br/>


                    And finally, a shuffle has an impact on <b>fault tolerance</b>.
                </aside>
                <div style="display:inline-block;text-align: left; margin: auto auto auto auto">
                    What's wrong with shuffles?<br/>
                    <ul style="margin-left: 50px;">
                        <li>Data movement is expansive</li>
                        <li>Shuffles generate Stages</li>
                        <li>Fault tolerance</li>
                    </ul>
                </div>
            </section>
            <section data-transition="none none">
                <aside class="notes">
                    Fault Tolerance with Shuffling

                    So for all of these reasons, shuffles should be limited. How can we do this, well, let's take
                    a couple of use cases.
                </aside>
                <div style="display:inline-block;vertical-align: top">
                    <div class="vertical-box-set" style="display: inline-block;vertical-align: top">
                        <div class="box red fragment background green" data-fragment-index="5"></div>
                        <div class="box red"></div>
                        <div class="box red fragment background green" data-fragment-index="5"></div>
                    </div>
                    <div style="margin:25px 25px 0 25px;display:inline-block;vertical-align: top">
                        <div class="right"><div class="arrow"></div></div><br/>
                        <div class="right"><div class="arrow"></div></div><br/>
                        <div class="right"><div class="arrow"></div></div>
                    </div>
                    <div class="vertical-box-set" style="display: inline-block;vertical-align: top">
                        <div class="box red fragment background green" data-fragment-index="5"></div>
                        <div class="box red"></div>
                        <div class="box red fragment background green" data-fragment-index="5"></div>
                    </div>
                    <div style="margin:25px 25px 0 25px;display: inline-block;vertical-align: top">
                        <div class="up-r"><div class="arrow"></div></div><br/>
                        <div class="right"><div class="arrow"></div></div><br/>
                        <div class="down-r"><div class="arrow"></div></div>
                    </div>
                    <div style="margin-top:50px;"></div>
                    <div class="vertical-box-set" style="display: inline-block;vertical-align: top">
                        <div class="fragment flip" data-fragment-index="3">
                            <div class="box blue fragment background green" data-fragment-index="2"></div>
                        </div>
                        <div class="fragment unflip" data-fragment-index="3">
                            <div class="box blue"></div>
                        </div>
                        <div class="box blue fragment background green" data-fragment-index="5"></div>
                        <div class="box blue"></div>
                    </div>
                    <div style="margin:25px 25px 0 25px;display:inline-block;vertical-align: top">
                        <div class="right"><div class="arrow"></div></div><br/>
                        <div class="right"><div class="arrow"></div></div><br/>
                        <div class="right"><div class="arrow"></div></div>
                    </div>
                    <div class="vertical-box-set" style="display:inline-block;vertical-align: top">
                        <div class="fragment flip" data-fragment-index="3">
                            <div class="box blue fragment background green" data-fragment-index="1"></div>
                        </div>
                        <div class="fragment unflip" data-fragment-index="3">
                            <div class="box blue"></div>
                        </div>
                        <div class="box blue fragment background green" data-fragment-index="5"></div>
                        <div class="box blue"></div>
                    </div>
                    <div style="margin:25px 25px 0 25px;display:inline-block;vertical-align: top">
                        <div class="up-r"><div class="arrow"></div></div><br/>
                        <div class="right"><div class="arrow"></div></div><br/>
                        <div class="down-r"><div class="arrow"></div></div>
                    </div>
                </div>
                <div style="display:inline-block;vertical-align: top;margin-top:42px;">
                    <div class="vertical-box-set" style="display: inline-block;vertical-align: middle">
                        <div class="box purple"></div>
                        <div class="box purple"></div>
                        <div class="box purple fragment background green" data-fragment-index="4"></div>
                        <div class="box purple"></div>
                        <div class="box purple"></div>
                        <div class="box purple"></div>
                    </div>
                </div>
            </section>

            <section data-transition="none none">
                <aside class="notes">
                    Let's say I want to <b>make a join</b> between an RDD containing the people in <b>Canada</b>
                    with one that contains all the canadian <b>provinces/territories</b>.<br/><br/>

                    *** Well, this would result in a <b>very uneven sharding</b> as some provinces like <b>Quebec</b>
                    are much more populated than territories like<b>Nunavut</b>.<br/><br/>

                    Moreover, we would get a <b>very limited level of parallelism</b> as we would only have 13
                    <b>partitions</b>, and adding more <b>machines</b> to get the job done would not change <b>anything</b>.
                </aside>
                <table>
                    <tr>
                        <td><img
                                style="margin-top:30px; margin-right:60px;box-shadow: 0; background-color: #FFFFFF; width:100%"
                                src="images/spark_scala_meetup/shuffle_3.png"/></td>
                        <td style="vertical-align: middle">
                            canadians.join(provinces)
                            <ul class="fragment" style="width:350px; white-space: nowrap;">
                                <li>Uneven sharding</li>
                                <li>Limited parallelism</li>
                            </ul>
                        </td>
                    </tr>
                </table>
            </section>
            <section data-transition="none none">
                <aside class="notes">
                    Actually, a better way to do this is to use a <b>broadcast variable</b> cointaining all the canadian
                    <b>provinces and territories</b>. This variable could be then sent to all <b>the worker nodes</b> in order,
                    which would avoid a <b>shuffle</b>. Now of course, this is only possible if the <b>dataset</b> being sent can
                    <b>fit in the memory</b> of a single node.
                </aside>
                <table>
                    <tr>
                        <td><img
                                style="margin-top:30px; margin-right:60px;box-shadow: 0; background-color: #FFFFFF; width:100%"
                                src="images/spark_scala_meetup/shuffle_4.png"/></td>
                        <td style="width:350px; vertical-align: middle">
                            <ul style=" white-space: nowrap;">
                                <li>Even Sharding</li>
                                <li>No Shuffle Required</li>
                            </ul>
                        </td>
                    </tr>
                </table>
            </section>
            <section data-transition="none none">
                <aside class="notes">
                    Now what if you cannot use a <b>broadcast variable</b> ? what if the data in <b>both RDDs</b> is too
                    big ? Let's say for example that we need to <b>join</b> an <b>RDD</b> containing all the <b>people in Canada</b>
                    with one containing all the <b>people in the world</b>. Joining those <b>RDDs</b> would result in a <b>lot of
                    data shuffled</b> across the <b>network</b> and potentially in a <b>space problem</b> on the destination nodes.
                </aside>
                <table>
                    <tr>
                        <td><img
                                style="margin-top:30px; width:100%; margin-left:10px;box-shadow: 0; background-color: #FFFFFF"
                                src="images/spark_scala_meetup/shuffle_7.png"/></td>
                        <td style="vertical-align: middle">
                            <ul style="width:350px; white-space: nowrap;">
                                <li>Even Sharding</li>
                                <li>Good Parallelism</li>
                                <li>Space may be a problem</li>
                            </ul>
                        </td>
                    </tr>
                </table>
            </section>
            <section data-transition="none none">
                <aside class="notes">
                    So instead of doing this, we could just create a <b>partial RDD</b> using a <b>filter</b>, and <b>reduce</b>
                    the amount of data to be <b>transferred</b>. The whole idea here is to make sure that you <b>transfer</b> only
                    the data that is required by <b>further</b> operations.<br/><br/>

                    Overall, this leads to one fundamental question. The main reason why Shuffles is required is because
                    data is not properly distributed. So how can we optimize that? If data is distributed properly, we
                    could prevent shuffling from being so expensive. So let's think about this.
                </aside>
                <table>
                    <tr>
                        <td><img
                                style="margin-top:30px; width:100%; margin-left:10px;box-shadow: 0; background-color: #FFFFFF;"
                                src="images/spark_scala_meetup/shuffle_8.png"/></td>
                        <td style="vertical-align: middle;">
                            <ul style="width:350px; white-space: nowrap;">
                                <li>Even Sharding</li>
                                <li>Good Parallelism</li>
                                <li>Less Space required</li>
                                <li>Less Data shuffled</li>
                            </ul>
                        </td>
                    </tr>
                </table>
            </section>
        </section>
        <section data-transition="none none">
            <h2>Efficient Data distribution</h2>
            <section data-transition="none none">
                <aside class="notes">
                    Alright so what do we know about partitioning so far....


                    <!--The second thing you should always keep in mind when working with Spark is how your data is-->
                    <!--distributed over the Spark cluster. A bad distribution may lead to heavy shuffles, unbalanced-->
                    <!--partitions, bad performances, or even worse, failed jobs.-->

                    We saw that in Spark, the data is split into multiple partitions which are spread among the executors.
                    We've also seen that the initial number of partition can be provided when loading the data, and that
                    by default Spark uses either its configuration or the one specific to each datasource.<br/><br/>

                    Partitions cannot span over several executors, therefore, you may end up having partitions of different
                    size, and their number may change over time depending on the transformations performed.<br/><br/>

                    As data distribution is such a big deal, it should be possible to manage it somehow. Well, this is
                    exactly what Spark allows you to do.<br/><br/>
                    <!---->
                    <!--Why do we care? Well the number of partitions processed by Spark at a given moment can have a-->
                    <!--significant impact on performances and can some time make the difference between a job which-->
                    <!--completes from one which does not. For these reasons, Spark provides different ways to redistribute-->
                    <!--the data in order to make it more manageable.-->

                    <!--Partitioning:-->
                    <!--* Problems: Skewed data, not enough partitions, too many partitions-->
                </aside>
               <ul>
                    <li>Data is split into multiple partitions</li>
                    <li>The initial number of partition can be configured</li>
                    <li>Partitions do not span</li>
                    <li>The number of partition can change</li>
                </ul><br/><br/>
                A good data distribution is the key for efficient Spark jobs.
            </section>

            <section data-transition="none none">
                <aside class="notes">
                    Let's first look at how partitioning can be changed across transformations. Just like data loading
                    functions, some transformations provide an additional parameter defining the number of partitions
                    in which the data will be split into once transformed.<br/><br/>

                    So as you can see here, we've created an RDD of integers, which is grouped by ranges.<br/><br/>

                    Most of transformations allowing you to set the <b>number of output partitions</b> also allow you to
                    provide a specific <b>partitioner</b>. By default, Spark uses the <b>HashPartitioner</b>, but it also provides
                    a RangePartitioner, and you can also implement your own partitioner.<br/><br/>

                    I won't get into details here, the documentation is pretty straightforward for that.<br/><br/>

                    You can also pass a <b>partitioner</b> without making any transformation using the <b>partitionBy</b> function.
                    However just remember to cache the rdd obtained in order to prevent the re-partitioning to be done every time the
                    RDD is computed.<br/><br/>

                    Also keep in mind that <b>partitioning</b> info can be lost depending on the transformations you apply.
                    In the case of key-value pair RDD, the <b>data distribution</b> usually depends on the keys. If you map this
                    rdd, nothing guarantees that you won't change the keys. Therefore the <b>partitioning</b> info gets lost.
                </aside>
<pre><code class="scala"  style="font-size: 25px;">// use the default number of partitions
rdd.groupBy(...)

// distributes the resulting data in 2 partitions
rdd.groupBy(..., 2)</code></pre>

                <pre class="fragment"><code class="scala"  style="font-size: 25px;">// p(k) = k % num_partitions
val partitioner = new HashPartitioner(nbPartitions)
rdd.groupBy( ... , partitioner)</code></pre>

                <pre class="fragment"><code class="scala"  style="font-size: 25px;">val partitioneddRdd = ints.partitionBy(partitioner)
// prevents future repartitioning
partitioneddRdd.cache()</code></pre>

                <pre class="fragment"><code class="scala"  style="font-size: 25px;">val rdd1 = pairs.partitionBy(partitioner)
val rdd2 = rdd1.map { ... }

rdd1.partitioner // returns Some(...)
rdd2.partitioner // returns None</code></pre>
            </section>

            <section data-transition="none none">
                <aside class="notes">
                    Finally, last thing about partitioning, is that you can also <b>explicitly</b> ask Spark to
                    <b>redistribute</b> the data using <b>coalesce</b> and <b>repartition</b>. Overall these two functions
                    consists in <b>changing the number</b> of partitions used to store the data.<br/><br/>

                    <b>Coalesce</b> will allow you to reduce this number while <b>repartition</b> can increase or reduce it. The main
                    difference is that <b>coalesce</b> will just regroup the <b>partitions</b> in a smaller amount of partitions,
                    without <b>worrying</b> about how well the <b>data</b> is spread, while repartition <b>garantees</b> the data to be spread
                    as evenly as possible.<br/><br/>

                    <!--Now considering what we said so far, could you tell me what kind of transformation are coalesce and-->
                    <!--repartition?-->
                </aside>
                <pre><code class="scala"  style="font-size: 25px;">val ints = sc.parallelize(Seq(1,2,3,4,5,6,7,8,9,0), 4)</code></pre>
<pre><code class="scala"  style="font-size: 25px;">List(3, 4, 5)
List(6, 7)
List(1, 2)
List(8, 9, 0)</code></pre>

<pre class="fragment" data-fragment-index="1"><code class="scala"  style="font-size: 25px;">ints.coalesce(3)</code></pre>
<pre class="fragment" data-fragment-index="1"><code class="scala"  style="font-size: 25px;">List(3, 4, 5)
List(6, 7, 8, 9, 0)
List(1, 2)
</code></pre>

<pre class="fragment" data-fragment-index="2"><code class="scala"  style="font-size: 25px;">ints.repartition(3)</code></pre>
<pre class="fragment" data-fragment-index="2"><code class="scala"  style="font-size: 25px;">List(5, 7, 8)
List(1, 3, 9)
List(2, 4, 6, 0)
</code></pre>
            </section>

            <!--<section data-transition="none none">-->
                <!--<aside class="notes">-->
                    <!--Now back to shuffling. In order to prevent them we can optimize the partitioning. So let's get to-->
                    <!--one previous examples. So here I'd like to join these two RDDs. So first I can force a shuffle on-->
                    <!--each of them and make sure they are partitioned using the same key. Once loaded, the RDD's data-->
                    <!--will be copartitioned, that is they were share the same partitioner, but also they will be colocated.-->
                    <!--In other words, all the pairs having to be joined will be in the same node. Now if I want to perform-->
                    <!--the join, no shuffle is required.-->
                <!--</aside>-->
                <!--<div>-->
                <!--</div>-->
            <!--</section>-->

            <section data-transition="none none">
                <aside class="notes">
                    Alright, overall, as we kept mentioning during this talk, <b>data distribution</b> should be one of your
                    primary concern when writing a <b>Spark job</b>. Spark provides different ways to manage this in an <b>optimal</b>
                    way. Now one question you may ask, how many <b>partition</b> should I use to distribute the data.<br/><br/>

                    Well a good metric is to take the <b>number of cores available</b> and to multiply it by 2 or 3. The goal is to
                    make the job as parallelizable as possible.<br/><br/>

                    You basically don't won't to have <b>too few partitions</b> otherwise some resources of your cluster would be idle.<br/><br/>

                    Having <b>too many partitions</b> on the other hand may also be a problem, as this would imply more <b>IO</b>.<br/><br/>

                    <!--Finally, sometime partitions may be too heavy to load into an executor's memory, in which case, you-->
                    <!--would have to trade some IO.<br/><br/>-->

                    Overall, you have to figure out the good balance between parallelism and the size of each partition,
                    along with how data is distributed.<br/><br/>

                    So unfortunately, we won't be able to cover this but overall, if you have problems like the join of the
                    Canadians people RDD and the People of the World RDD, I really suggest you to look at
                    copartitioning / colocation techniques, along with salting strategies. And if you are curious, we
                    can actually talk about it after this talk.
                </aside>
                <ul>
                    <li>Optimal number of partition: number of cores * 2 or 3</li>
                    <li>Too few partition would not take fully advantage of your cluster</li>
                    <li>Too many partition would imply too much IO</li>
                    <li>Keep the balance between memory, parallelism and data distribution</li>
                </ul>
            </section>
        </section>


        <section data-transition="none none">
            <aside class="notes">
                We are reaching the end of this talk, as you can see Spark is a pretty wide topic. Its base principles
                are not hard to grasp though, and no matter if you do machine learning, streaming or data querying,
                you will constantly interact with the same API. The main primitives may be a bit different, but overall
                the same ides come back over and over again.<br/><br/>

                Unfortunately we don't have time to cover the higher level APIs, but if you are interested, I can show
                you some slides presenting them aside of the talk. I guess the main difficulties with Spark is the
                inherent non-determinism of Big Data technologies. Sometime it's just too complexe, and there are way
                too many variables involved, but by following the best practices we just talked about you should be good.<br/><br/>

                One last advice, whenever you get lost, take some paper and a pen, an write down how your data pipeline
                works. Where does the data comes from, how is it transformed, how much data are talking about, what are
                your resources available? This, along with the Spark UI should help you to get rid of most of your problems.
            </aside>
            <h2>Conclusion</h2>
            <ul>
                <li><span style="font-size: 30px;color: #FF6600;">Learning Spark : </span><span style="font-size: 25px">by Andy Konwinski, Holden Karau, and Patrick Wendell</span>
                <li><span style="font-size: 30px;color: #00CC33;">High Performance Spark : </span><span style="font-size: 25px">by Holden Karau, and Rachel Warren</span></li>
                <li><span style="font-size: 30px;color: #FF6600;">Advanced Analytics with Spark : </span><span style="font-size: 25px; ">by Josh Wills, Sandy Ryza, Sean Owen, and Uri Laserson</span></li>
                <li><span style="font-size: 30px;color: #00CC33;">http://blog.cloudera.com/</span></li>

            </ul>
        </section>
        <section data-transition="none none">
            <aside class="notes">
                I haven't introduced my self, my name is Francis Toth, I'm a happy Scala developer and Trainer working
                at Yoppworks, and one of the co-organizer of the Lambda Montreal meetup. A couple of words regarding
                Yoppworks, well we are a company providing consulting and training services in partnership with
                Hortonworks and Lightbend and are looking for people interested by Big Data, Functional Programming
                And Scala related technologies. If you are interested, just come see me and I'll be glad to talk
                about a potential collaboration with you. Thank you again, and now it's time for questions.
            </aside>
            <img style="background-color: transparent"
                 src="images/yoppworks.logo-corner-dark.png"/>
            <span style="font-size: 40px">Thank you ! Questions ?</span><br/>
            <span style="font-size: 25px">francis.toth@yoppworks.com</span>
        </section>

        <section>
            <h2>Spark's high-level APIs</h2>
            <aside class="notes">
                So this is all about Spark's core. I know it's a lot of material but at least this gives you an overview
                of how Spark works. Now let's move on, an look at some higher level API's. As we've said in the
                begining of this talk, we won't have the time to cover them all and will only focus on the broad lines.
            </aside>
        </section>

        <section>
            <h2>Spark SQL</h2>

            <section data-transition="none none">
                <aside class="notes">
                    So, Spark SQL. In a couple of words, Spark SQL is an API*** released in 2015***, which allows you to
                    interact with a dataset through SQL, as long as it can be represented in a tabular format. So the
                    data must be structured and have a schema. This allows you to read and write data in a variety****
                    of structured format like Json, Hive or Parquet in a very unified and transparent way.****<br/><br/>

                    In some way Spark SQL is very close to Hive as they both deal with interactive querying. Actually,
                    they are not exclusive.**** Spark SQL supports indeed most of Hive features, and allows you to access
                    Hive tables, User-Defined functions, SerDes for serialization and deserialization formats, and the Hive Query
                    Language.<br/><br/>

                    Now it's important to notice that using Hive features with Spark SQL does not require a Hive
                    installation. A library called spark-hive provides the glue between these two frameworks which let you take
                    advantage of Hive without even installing it.

                    <!--The reason why you can choose to use Hive or not, is that some-->
                    <!--users may have dependency conflict between the version of Hive used by their cluster and the one-->
                    <!--they wish to use with Spark.-->
                </aside>
                <ul>
                    <li class="fragment">Released in February 2015</li>
                    <li class="fragment">Structured dataset querying through SQL</li>
                    <li class="fragment">Integrates with Hive, Parquet, JSon...</li>
                    <li class="fragment">Supports most of Hive's feature</li>
                </ul>
            </section>
            <section data-transition="none none">
                <aside class="notes">
                    Under the hood, Spark SQL is based on an extension of the RDD model called a Dataframe. Dataframes
                    are overall similar to tables in a relational database and contain Row objects, which are just
                    wrappers around arrays of basic types.<br/><br/>

                    Each row represents a record, and as they are structured, they can be stored in a much more efficient
                    manner than native RDDs. Actually, some benchmarks show that a Dataframe is twice faster than an RDD
                    if the Scala API is used, and ten time faster if using the one written in Python.
                </aside>
                <img style="margin-top:30px; width:100%; margin-left:50px;box-shadow: 0; background-color: #FFFFFF;"
                     src="images/spark_scala_meetup/dataframe_0.png"/>
            </section>

            <section data-transition="none none">
                <aside class="notes">
                    Now enough talking now, let's see some code.<br/><br/>

                    First thing you need to do in order to play with Spark-SQL, is to import a context.****<br/><br/>

                    Depending on whether you use Hive or not, Spark SQL has two main entry points : The SQLContext and the
                    HiveContext. According the official documentation, it is actually recommended to use Hive's features as a lot of
                    support material is already available for them. ****A dataframe can be created by converting an existing RDD
                    or ****from external sources like a json or an hdfs file. For some sources, Sparq SQL will be able
                    to infer the schema of your data, and this is exactly what happens when I create a dataframe from a
                    json file. **** This schema can by the way be printed using the method show() provided by the dataframe
                    object.
                </aside>

                <div class="fragment">
                    <pre><code class="scala">import org.apache.spark.sql.hive.HiveContext
val hiveCtx = new HiveContext(new SparkContext(...))</code></pre>

                    <pre><code class="scala">import org.apache.spark.sql.SQLContext
val sqlCtx = new SQLContext(new SparkContext(...))</code></pre>
                </div>

                <div class="fragment">
                    <pre><code class="scala">val dataframe = sqlCtx.createDataFrame(rdd)</code></pre>
                    <pre><code class="scala">val dataframe = hiveCtx.jsonFile(inputFile)</code></pre>
                </div>

                <pre class="fragment"><code class="console">scala> dataframe.show()
root
|-- foo: struct (nullable = true)
|    |-- waldos: array (nullable = true)
|    |    |-- element: string (containsNull = false)
|-- bar: boolean (nullable = true)
|-- qux: string (nullable = true)</code></pre>
            </section>

            <section data-transition="none none">
                <aside class="notes">
                    Now, in order to query my data, I just have to register a table name and use the sql method. This
                    brings me back a dataframe containing all the data I've requested, so I can make more computation on top of
                    it. Actually, depending on the kind of source the dataframe has been created from, Spark-SQL will be
                    able to select only a subset of the fields and smartly scan only the data for those fields, instead of
                    scanning everything.****<br/><br/>

                    Now if I decide to apply some transformations on that dataframe, I have two options. I can either
                    use one of dataframe functions like select, filter or groupBy, or I can convert the resulting
                    dataframe into an rdd, and apply the functions provided by it.
                </aside>
                <pre><code class="scala">input.registerTempTable("foobar")
val foobarz = hiveCtx.sql("SELECT * FROM foobar ORDER BY qux LIMIT 10")</code></pre>

                <pre class="fragment"><code class="scala">foobarz.select(...).filter(...).groupBy(...)
foobarz.rdd.filter(...).map(...)</code></pre>
            </section>

            <section data-transition="none none">
                <aside class="notes">
                    Regarding how dataframes integrate with the rest of the world, Spark SQL provides a datasource API,
                    which allows you to integrate Spark SQL with Avro, HBase, ElasticSearch, and Cassandra to name only a few.
                </aside>
                <img style="margin-top:30px; width:100%; margin-left:10px;box-shadow: 0; background-color: #FFFFFF;"
                     src="images/spark_scala_meetup/dataframe_5.png"/>
            </section>

        </section>

        <section>
            <h2>Spark Streaming</h2>
            <section data-transition="none none">
                <aside class="notes">
                    Not let's move on to another API which is Spark Streaming. So, Spark Streaming was released in 2013,
                    *** that is, in early versions of Spark and has been always pretty stable since. ***<br/><br/>

                    It's real-time analytics library which is used in cases such as monitoring the flow of users on a website or
                    detecting fraud transaction in real time. <br/><br/>

                    *** Spark-Streaming caught a lot of attention recently , as real time is today extremely in demand.
                    According to Databricks, the company created by the founders of Spark, 56% more Spark's users globally
                    ran Spark Streaming applications in 2015 compared to 2014., <br/><br/>

                    and among those users, you'll find names like Uber, Netflix and Pinterest to name only a few.
                    <!--http://www.datanami.com/2015/11/30/spark-streaming-what-is-it-and-whos-using-it/-->
                    <!--https://opensource.com/business/15/4/guide-to-apache-spark-streaming-->

                </aside>
                <ul>
                    <li class="fragment">Released in 2013</li>
                    <li class="fragment">Very stable and robust</li>
                    <li class="fragment">Real-Time analytics API</li>
                    <li class="fragment">Trendy</li>
                </ul>
            </section>
            <section data-transition="none none">
                <aside class="notes">
                    Spark-Streaming relies on micro-batch architecture. So the streaming computation is divided in small
                    continuous series of batches created at regular time intervals. At the beginning of each interval,
                    a new batch is created, and any data that arrives during this interval gets added to that batch.
                    Once the batch is done growing, it is processed as an RDD just like we've seen earlier.
                </aside>
                <img style="margin-top:50px; width:100%; margin-left:40px;box-shadow: 0; background-color: #FFFFFF;"
                     src="images/spark_scala_meetup/streaming_archi.png"/>
            </section>

            <section data-transition="none none">
                <aside class="notes">
                    The fundamental abstraction of Spark-Streaming is a <b>DStream</b>, or discretized stream. A DStream is
                    a sequence of RDD where each RDD has one time slice of the data in the stream.<br/><br/>

                    Just like RDDs, DStreams can be created from external sources or by applying transformations to other
                    DStreams. Once created and properly transformed, we can apply ouput actions such as print() to the
                    DStream.<br/><br/>

                    Output actions are actually similar to RDD actions in that they write data to external systems.
                    However, these actions run periodically on each time step, producing output in batches.<br/><br/>

                    Now these DStreams won't be started as long as I don't call the start method here. It's pretty
                    much like the concept of actions with RDDs. This means that all the DStreams which compose my
                    program must be specified before calling the start method. Once it has been called, any modification
                    of these streams won't be taken in account. The streaming will be effective until the
                    streams are finished, either manually or du to an error, and this is what the awaitTermination is
                    for.<br/><br/>

                    As you can see, the syntax used to create an RDD and a DStream is pretty similar, and actually
                    this is what really makes Spark shine. No matter if you are doing batch or streaming processing,
                    the programming model is always the same.
                </aside>

                <pre><code class="scala">// Boilerplate
val conf = new SparkConf()
    .setMaster("local[2]").setAppName("NetworkWordCount")
val ssc = new StreamingContext(conf, Seconds(1))

// Create a DStream listening on port 9999
val lines: DStream[String] = ssc.socketTextStream("localhost", 9999)

lines.flatMap(_.split(" "))
    .map(word => (word, 1))
    .reduceByKey(_ + _)
    .print()

ssc.start() // Start the computation
ssc.awaitTermination() // Wait for the computation to terminate</code></pre>
            </section>

            <section data-transition="none none">
                <aside class="notes">
                    Regarding fault-tolerance, DStreams provide the same properties than RDDs. As long as a copy of the
                    data is available, a DStream can recompute any state derived from it, using the lineage of its RDDs.<br/><br/>

                    Therefore, received data is replicated across two nodes by default, allowing a DStream to tolerate single
                    worker failures. Secondly, as it can be expansive to recompute a long stream of data, Spark-Streaming
                    provides a mechanism called checkpointing that saves state periodically to a reliable filesystem.<br/><br/>

                    So in case of failure, Spark will only have to go back to the last checkpoint. Finally, Spark-Streaming
                    can also be configured in order to run 24/7. Properly configured, it is able to ensure that no matter
                    if the driver, a worker or a receiver fails, it will always be able to get back on its feet. Of
                    course, this comes with some challenges and configurations but this is the idea.<br/><br/>
                </aside>
                <table>
                    <tr>
                        <td style="vertical-align: top;white-space: nowrap">Fault-tolerance :</td>
                        <td style="white-space: nowrap">
                            <ul>
                                <li>Data replication</li>
                                <li>Checkpointing</li>
                                <li>Can run 24/7</li>
                            </ul>
                        </td>
                    </tr>
                </table>
            </section>

            <section data-transition="none none">
                <aside class="notes">
                    Regarding how you can integrate Spark-Streaming with other frameworks, well there is nothing
                    surprising.
                    Spark-Streaming is able to integrate with Akka, Kafka, Flume, and Kinesis to name only a few.
                </aside>
                <img style="margin-top:50px; width:100%; margin-left:40px;box-shadow: 0; background-color: #FFFFFF;"
                     src="images/spark_scala_meetup/streaming_integration.png"/>
            </section>
        </section>

        <section>
            <h2>Spark APIs : What's left ?</h2>
            <aside class="notes">
                So, we're reaching the end of this presentation, and unfortunately we won't have time to cover Spark ML
                and GraphX. But let's have a very quick word about them. ***<br/><br/>

                Spark ML is Spark's Machine Learning Library and is designed to run in parallel on clusters. It provides
                mostly algorithms which are designed to run in parallel such as distributed random forests, K-means or
                alternating least squares, and are best suited for running on very large dataset, more than on small
                ones. ***<br/><br/>

                On the other hand, GraphX is Spark's graph processing framework and intends to unify Data-parallel and
                graph parallel systems. It is designed for parallel iterative graph computation, provides a wide range
                of
                graph algorithm, and it's also very fast, actually about 10 times faster than traditional Hadoop graph
                algorithms. Secondly, if you've already worked with Graphs, you probably know that the graph processing
                is only a small part of the problem. Actually as data get bigger and bigger, the graph creation and
                post-processing tend to become a bottleneck. And this why GraphX is interesting. It provides support for
                graph construction and post-processing through a unified API.
            </aside>
            <table>
                <tr class="fragment">
                    <td style="vertical-align: top; white-space: nowrap">Spark ML :</td>
                    <td>
                        <ul>
                            <li>Native Machine Learning Framework</li>
                            <li>Supports Classification, Regression, Clustering</li>
                            <li>Designed to run on large distributed data in parallel</li>
                        </ul>
                    </td>
                </tr>
                <tr class="fragment">
                    <td style="vertical-align: top; white-space: nowrap">GraphX :</td>
                    <td>
                        <ul>
                            <li>Native Graph Processing framework</li>
                            <li>Similar to Pregel, Giraph, and Graphlab</li>
                            <li>Designed for network-oriented analytics</li>
                            <li>Very fast !</li>
                        </ul>
                    </td>
                </tr>
            </table>
        </section>
    </div>
</div>

<script src="lib/js/head.min.js"></script>
<script src="js/reveal.js"></script>

<script>

    // Full list of configuration options available at:
    // https://github.com/hakimel/reveal.js#configuration
    Reveal.initialize({
        controls: true,
        progress: true,
        history: true,
        center: true,

        transition: 'slide', // none/fade/slide/convex/concave/zoom

        // Optional reveal.js plugins
        dependencies: [
            { src: 'lib/js/classList.js', condition: function() { return !document.body.classList; } },
            { src: 'plugin/markdown/marked.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
            { src: 'plugin/markdown/markdown.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
            { src: 'plugin/highlight/highlight.js', async: true, condition: function() { return !!document.querySelector( 'pre code' ); }, callback: function() { hljs.initHighlightingOnLoad(); } },
            { src: 'plugin/zoom-js/zoom.js', async: true },
            { src: 'plugin/notes/notes.js', async: true }
        ]
    });

</script>
</body>
</html>

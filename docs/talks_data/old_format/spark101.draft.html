<!doctype html>
<html lang="en">

<head>
    <meta charset="utf-8">

    <title>Introduction to Spark - by Francis Toth</title>

    <meta name="description" content="A framework for easily creating beautiful presentations using HTML">
    <meta name="author" content="Hakim El Hattab">

    <meta name="apple-mobile-web-app-capable" content="yes" />
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />

    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">

    <link rel="stylesheet" href="css/reveal.css">
    <link rel="stylesheet" href="css/custom.css">
    <link rel="stylesheet" href="css/theme/francis.css" id="theme">

    <!-- Code syntax highlighting -->
    <link rel="stylesheet" href="lib/css/zenburn.css">

    <!-- Printing and PDF exports -->
    <script>
        var link = document.createElement( 'link' );
        link.rel = 'stylesheet';
        link.type = 'text/css';
        link.href = window.location.search.match( /print-pdf/gi ) ? 'css/print/pdf.css' : 'css/print/paper.css';
        document.getElementsByTagName( 'head' )[0].appendChild( link );
    </script>

    <!--[if lt IE 9]>
    <script src="lib/js/html5shiv.js"></script>
    <![endif]-->
</head>

<body>


<aside style="display: block; position: fixed; bottom: 10px; right: 10px; z-index: 30;">
    <a href="http://www.yoppworks.com"><img src="images/spark_scala_meetup/spark_scala_meetup/yoppworks-logo-landscape-large-text-dark-bg.png"
                                            height="30"></a>
</aside>

<div class="reveal">
    <div class="slides">
        <section>
            <aside class="notes">
                Hi everyone, my name is Francis, and today I'll give you a brief introduction to Spark.

                Big Data is about two things:
                - how do can data be stored
                - how can we process it

                by the past, Map Reduce ...how does Spark solve this problem?

            </aside>
            <h2 style="display: inline-block;"><img
                    style="vertical-align: text-bottom; margin: 18px 0; border: 0; background-color: #243044"
                    src="images/spark_scala_meetup/spark_logo.transparent.png"/></h2>
        </section>
        <section>
            <h2>Quick overview</h2>
            <aside class="notes">
                So Spark was <b>founded in 2009</b> at the AmpLab in California.<br/><br/>

                ***It is a <b>cluster computing platform</b> which extends the traditional <b>MapReduce model</b>,
                and which is <b>designed to support</b> various types of computations on large <b>dataset</b>.
                Originally, Spark intended to address <b>common problems</b> people were facing while doing Machine
                learning. Therefore, it is highly <b>optimized for iterative and cyclic</b> operations on the same
                set of data.<br/><br/>

                A data pipeline is nothing more than a <b>sequence of steps</b>, each being responsible for <b>processing</b>
                the data, and for <b>passing</b> the result to the next step in the sequence. With <b>Hadoop</b>,
                this is usually done by <b>chaining</b> different frameworks with each others and by storing the data
                resulting from each step.<br/><br/>

                This approach has actually <b>two main issues</b>, first, in terms of <b>performances</b>, as each
                step requires a certain amount of IO to store/read the intermediary data, and secondly in terms of
                <b>maintenance</b>, as it requires to juggle with sometime different languages, paradigms or tools.
                <b>Spark</b> addresses this by providing a unified framework. The main idea is that every Spark's
                library is based on the same core concepts which makes the switch between one library to the other
                very easy.<br/><br/>

                <!--<b>Spark</b> makes <b>easy</b> and <b>inexpensive</b> the combination of different <b>processing-->
                <!--types</b> using a <b>unified-->
                <!--framework</b>, which is extremely useful when maintaining <b>data analysis pipelines</b>, and which-->
                <!--contributes-->
                <!--to <b>reduce the management</b> of separate tools which is a very <b>common thing</b> in-->
                <!--Hadoop.<br/><br/>-->

                ***Compared to <b>traditional MapReduce engines</b>, benchmarks have shown a <b>significant increase</b>
                in performances which have to be qualified however, as a benchmark may show different results
                depending on the context. Some will tell you that compared to Hadoop, Spark is a hundred faster in
                memory and ten time on disk. Well, this is actually true for short-running jobs dealing with very
                specific problems. In the real world, that is in production, the gain is closer to twice to three times
                faster than Hadoop. Anyway, these performances can be actually explained by the fact that Spark makes a
                heavy use of caching in the contrary of Hadoop which is mostly disk dependent.

                Finally, Spark is today <b>one of the most popular</b> and <b>active</b> project regarding<b>large-scale
                data processing</b>. It is used by <b>Amazon</b>, <b>Ebay</b>, and <b>Netflix</b> among others, and
                has seen its number of <b>contributors</b> growing tremendously since it's hosted by <b>Apache</b>.
            </aside>
            <ul>
                <li>Created in 2009</li>
                <li class="fragment">Large-scale data processing engine</li>
                <li class="fragment">Unified framework</li>
                <li class="fragment">Extremely effective</li>
                <li class="fragment">Popular</li>
            </ul>
        </section>

        <section>
            <h2>Agenda</h2>
            <aside class="notes">
                So today, we'll mainly focus on <b>Spark's core aspects</b>, present its <b>execution model</b> and the
                ways it deals with <b>large-scale data computation</b>. If we have some time, we'll then talk about its
                higher level API's.

                <!--<b>features</b>-->
                <!--it supports. Then we'll briefly cover some of its higher level API's with Spark-SQL, a lib responsible for-->
                <!--doing interactive queries and Spark-Streaming which deals with continuous and close-to-real-time-->
                <!--processing. Unfortunately, we won't have time to cover MLib which is a lib dedicated to machine-->
                <!--learning, and GraphX, which is Spark's graph processing lib.-->
            </aside>
            <div style="display: block; text-align: center">
                <img style="border: 0; background-color: #FFFFFF" src="images/spark_scala_meetup/spark-stack.png"/>
            </div>
        </section>

        <section>
            <h2>Spark Architecture</h2>
            <aside class="notes">
                In terms of architecture, a Spark application relies on three components. First, the driver. The driver is
                the central coordinator of a Spark program. It is responsible for defining distributed dataset on the
                cluster and for launching parallel computations on them.

                In order to perform those tasks, the driver relies on a certain amount of executors. An executor is
                responsible for running the tasks requested by the driver. It provides in-memory storage used by Spark
                for caching purpose along with cores allowing it to run any computation requested by the driver.

                The Driver and its executors have all their own Java processes allowing a Spark application to be completely
                isolated from one another. On the other hand, this also means that data cannot be shared between each
                executor except by writing it to an external storage.

                Now in order to run, these processes need to allocate physical resources right? Things like disk-space,
                cpus or memory have to come from somewhere! Well these are actually provided by the cluster manager which
                makes the glue between the Driver and its executors. This basically allows to abstract how a job is
                executed from what it has to perform. So depending on the cluster manager, you may end up with running the
                Driver and the executors on the same laptop or on a distributed cluster composed of several worker nodes
                like it is usually the case in production. For this reason, the cluster manager is a completely independent
                plugin which does not rely on a specific implementation. This allows Spark to run transparently on top
                of Yarn, Mesos, or its built-in standalone cluster manager.

                <!--  The cluster manager is responsible for telling how those computations will be executed, that is
                using what resources. -->
            </aside>
            <div class="fragment" data-fragment-index="1" style="border: 1px solid white; width: 20%; height: 100px;margin-left: 40%; margin-right: 40%;line-height: 100px;">
                Driver
            </div>
            <div class="fragment" data-fragment-index="5">
                <div class="down" style="margin: 20px 0 10px 0;"><div class="arrow"></div></div>
                <div style="border: 1px solid white; width: 20%; height: 100px;margin-left: 40%; margin-right: 40%">Cluster<br/>Manager</div>
                <div style="margin: 20px 0 10px 0;">
                    <div class="down-l"><div class="arrow"></div></div>
                    <div class="down" style="margin: 0 30px 0 30px;"><div class="arrow"></div></div>
                    <div class="down-r"><div class="arrow"></div></div>
                </div>
            </div>
            <div class="fragment" data-fragment-index="2" style="display: inline-block;">
                <div>
                    <div><span style="background-color: #243044">&nbsp;Executor&nbsp;</span></div>
                    <div style="border: 3px solid #5C97D6;padding: 10px 10px 0 10px;position: relative; z-index: -1;top:-20px;">
                        <div class="fragment" data-fragment-index="4" style="display:inline-block;margin-top:10px;">
                            <div class="partition-block blue" style="width:30px; height:30px;">&nbsp;</div>
                            <div class="partition-block blue" style="width:30px; height:30px;">&nbsp;</div><br/>
                            <div class="partition-block blue" style="width:30px; height:30px;">&nbsp;</div>
                            <div class="partition-block blue" style="width:30px; height:30px;">&nbsp;</div>
                        </div>
                        <div class="fragment" data-fragment-index="3" style="display:inline-block;margin-top:10px;vertical-align: top;border: 1px solid white;padding: 5px;line-height: 65px;">Cache</div>
                    </div>
                </div>
                <div class="fragment" data-fragment-index="6">
                    <div class="line horizontal"></div>
                    <div><span style="background-color: #243044">&nbsp;Node 1&nbsp;</span></div>
                </div>
            </div>
            <div class="fragment" data-fragment-index="2" style="display: inline-block;">
                <div style="display: inline-block;">
                    <div><span style="background-color: #243044">&nbsp;Executor&nbsp;</span></div>
                    <div style="border: 3px solid #5C97D6;padding: 10px 10px 0 10px;position: relative; z-index: -1;top:-20px;">
                        <div class="fragment" data-fragment-index="4" style="display:inline-block;margin-top:10px;">
                            <div class="partition-block blue" style="width:30px; height:30px;">&nbsp;</div>
                            <div class="partition-block blue" style="width:30px; height:30px;">&nbsp;</div><br/>
                            <div class="partition-block blue" style="width:30px; height:30px;">&nbsp;</div>
                            <div class="partition-block blue" style="width:30px; height:30px;">&nbsp;</div>
                        </div>
                        <div class="fragment" data-fragment-index="3" style="display:inline-block;margin-top:10px;vertical-align: top;border: 1px solid white;padding: 5px;line-height: 65px;">Cache</div>
                    </div>
                </div>
                <div style="display: inline-block;">
                    <div><span style="background-color: #243044">&nbsp;Executor&nbsp;</span></div>
                    <div style="border: 3px solid #5C97D6;padding: 10px 10px 0 10px;position: relative; z-index: -1;top:-20px;">
                        <div class="fragment" data-fragment-index="4" style="display:inline-block;margin-top:10px;">
                            <div class="partition-block blue" style="width:30px; height:30px;">&nbsp;</div>
                            <div class="partition-block blue" style="width:30px; height:30px;">&nbsp;</div><br/>
                            <div class="partition-block blue" style="width:30px; height:30px;">&nbsp;</div>
                            <div class="partition-block blue" style="width:30px; height:30px;">&nbsp;</div>
                        </div>
                        <div class="fragment" data-fragment-index="3" style="display:inline-block;margin-top:10px;vertical-align: top;border: 1px solid white;padding: 5px;line-height: 65px;">Cache</div>
                    </div>
                </div>
                <div class="fragment" data-fragment-index="6">
                    <div class="line horizontal"></div>
                    <div><span style="background-color: #243044">&nbsp;Node 2&nbsp;</span></div>
                </div>
            </div>
        </section>

        <section>
            <h2>Resilient Distributed Dataset</h2>
            <section data-transition="none none">
                <aside class="notes">
                    The core fundamental of Spark is the RDD. RDD stands for Resilient Distributed Dataset. It's
                    basically an interface to your data, or more precisely, a representation of the distributed data
                    coming into your system on top of which computations can be done.

                    Now concretely, the data is split into multiple partitions, each referencing a subset of the data,
                    which are stored all over the Spark cluster. Overall, the purpose of an RDD is to allow you to
                    define data computations over those partitions in a transparent way, that is without worrying about
                    how this is done under the hood.

                    Now why resilient? Well as we'll see it during this talk, an RDD provides some mechanism preventing
                    any data loss du to a failed computation.

                    <!--One thing to keep in mind here, is that, in Spark, a data partition cannot span over several-->
                    <!--executors, therefore you may end up having partitions of different size, like it is the case here.-->
                </aside>

                <div style="display:table;margin: auto">
                    <div style="display:table-cell;vertical-align: middle;width:100px;">
                         <div class="rectangle blue-border">
                            <div class="child blue-border"><div class="child blue-border" style="line-height:70px;"><span style="font-size: 30px;">Data</span></div></div>
                        </div>
                    </div>
                    <div style="display:table-cell;vertical-align: middle;width:150px;">
                        <div style="margin-top:50px;" class="up-r"><div class="arrow"></div></div><br/>
                        <div class="right"><div class="arrow"></div></div><br/>
                        <div class="down-r"><div class="arrow"></div></div>
                    </div>
                    <div style="display:table-cell;vertical-align: middle;">
                        <div class="executor">
                            <div class="title"><span>&nbsp;Executor&nbsp;</span></div>
                            <div class="body">
                                <div class="partition-list">
                                    <div class="block blue">&nbsp;</div>
                                    <div class="block blue" style="opacity: 0.7;">&nbsp;</div>
                                    <div class="block blue" style="opacity: 0.2;">&nbsp;</div>
                                    <div class="block blue" style="background-color: #243044">&nbsp;</div>
                                </div>
                            </div>
                        </div>
                        <div class="executor">
                            <div class="title"><span>&nbsp;Executor&nbsp;</span></div>
                            <div class="body">
                                <div class="partition-list">
                                    <div class="block blue">&nbsp;</div>
                                    <div class="block blue" style="opacity: 0.7;">&nbsp;</div>
                                    <div class="block blue" style="opacity: 0.2;">&nbsp;</div>
                                    <div class="block hidden">&nbsp;</div>
                                </div>
                                <div class="partition-list">
                                    <div class="block blue">&nbsp;</div>
                                    <div class="block blue" style="opacity: 0.7;">&nbsp;</div>
                                    <div class="block hidden">&nbsp;</div>
                                    <div class="block hidden">&nbsp;</div>
                                </div>
                            </div>
                        </div>
                    </div>
                    <div style="display:table-cell;vertical-align: middle;">
                        <div class="line vertical" style="height: 300px;margin-left: 50px;"></div>
                    </div>
                    <div style="display:table-cell;vertical-align: middle;">
                        <span style="margin-left: 30px">RDD</span>
                    </div>
                </div>

                <!--<table style="margin-top: 40px">-->
                    <!--<tr>-->
                        <!--<td style="border: 3px solid white;vertical-align: middle;text-align: center" rowspan="4">-->
                            <!--<div class="fragment cover" data-fragment-index="5">-->
                                <!--<div class="fragment" data-fragment-index="2">-->
                                    <!--<div class="partition-block blue" style="width:30px; height:30px;">&nbsp;</div>-->
                                    <!--<div class="partition-block blue" style="width:30px; height:30px;opacity: 0.8;">&nbsp;</div><br/>-->
                                    <!--<div class="partition-block blue" style="width:30px; height:30px;opacity: 0.4;">&nbsp;</div>-->
                                    <!--<div class="partition-block blue" style="width:30px; height:30px;">&nbsp;</div><br/>-->
                                    <!--<div class="partition-block blue" style="width:30px; height:30px;opacity: 0.8;">&nbsp;</div>-->
                                <!--</div>-->
                            <!--</div>-->
                            <!--Data<br/>-->
                            <!--<div class="fragment cover" data-fragment-index="5">-->
                                <!--<div class="fragment" data-fragment-index="2">-->
                                    <!--<div class="partition-block blue" style="width:30px; height:30px;opacity: 0.4;">&nbsp;</div><br/>-->
                                    <!--<div class="partition-block blue" style="width:30px; height:30px;">&nbsp;</div>-->
                                    <!--<div class="partition-block blue" style="width:30px; height:30px;opacity: 0.8;">&nbsp;</div><br/>-->
                                    <!--<div class="partition-block blue" style="width:30px; height:30px;opacity: 0.4;">&nbsp;</div>-->
                                    <!--<div class="partition-block blue" style="width:30px; height:30px;">&nbsp;</div>-->
                                <!--</div>-->
                            <!--</div>-->
                        <!--</td>-->
                        <!--<td class="fragment" data-fragment-index="1" style="vertical-align: middle">-->
                            <!--<div class="right"><div class="arrow"></div></div>-->
                        <!--</td>-->
                        <!--<td class="fragment" data-fragment-index="1" style="text-align: center;">-->
                            <!--<div style=""><span style="background-color: #243044">&nbsp;Executor&nbsp;</span></div>-->
                            <!--<div style="border: 3px solid #5C97D6;padding: 10px;position: relative; z-index: -1;top:-20px;">-->
                                <!--<div class="fragment" data-fragment-index="5" style="display:inline-block;margin-top:10px;border: 1px solid white;padding: 5px;line-height: 30px;">-->
                                    <!--<div class="partition-block blue" style="width:30px; height:30px;">&nbsp;</div>-->
                                    <!--<div class="partition-block blue" style="width:30px; height:30px;opacity: 0.7;">&nbsp;</div>-->
                                    <!--<div class="partition-block blue" style="width:30px; height:30px;opacity: 0.2;">&nbsp;</div>-->
                                    <!--<div class="partition-block blue" style="width:30px; height:30px;background-color: #243044">&nbsp;</div>-->
                                <!--</div>-->
                            <!--</div>-->
                        <!--</td>-->
                        <!--&lt;!&ndash;<td class="fragment" data-fragment-index="2" style="vertical-align: middle"><div class="right"><div class="arrow"></div></div></td>&ndash;&gt;-->
                        <!--<td class="fragment" data-fragment-index="1" rowspan="4" style="height: 1px;">-->
                            <!--<div style="border-right:3px solid white;border-top:3px solid white;border-bottom:3px solid white;height: 100%">-->
                                <!--&nbsp;&nbsp;-->
                            <!--</div>-->
                        <!--</td>-->
                        <!--<td style="vertical-align: middle;text-align: center" rowspan="4">-->
                            <!--<div class="fragment right" data-fragment-index="1"><div class="arrow"></div></div>&nbsp;&nbsp;RDD-->
                        <!--</td>-->
                    <!--</tr>-->
                    <!--<tr>-->
                        <!--<td class="fragment" data-fragment-index="1" style="vertical-align: middle">-->
                            <!--<div class="right"><div class="arrow"></div></div>-->
                        <!--</td>-->
                        <!--<td class="fragment" data-fragment-index="1" style="text-align: center">-->
                            <!--<div><span style="background-color: #243044">&nbsp;Executor&nbsp;</span></div>-->
                            <!--<div style="border: 3px solid #5C97D6;padding: 10px;position: relative; z-index: -1;top:-20px;">-->
                                <!--<div style="margin-top:10px;">-->
                                    <!--<div class="fragment" data-fragment-index="5" style="display:inline-block;border: 1px solid white;padding: 5px;line-height: 30px;">-->
                                        <!--<div class="partition-block blue" style="width:30px; height:30px;">&nbsp;</div>-->
                                        <!--<div class="partition-block blue" style="width:30px; height:30px;opacity: 0.7;">&nbsp;</div>-->
                                        <!--<div class="partition-block blue" style="width:30px; height:30px;background-color: #243044">&nbsp;</div>-->
                                        <!--<div class="partition-block blue" style="width:30px; height:30px;background-color: #243044">&nbsp;</div>-->
                                    <!--</div>-->
                                <!--</div>-->

                                <!--<div style="margin-top: 5px">-->
                                    <!--<div class="fragment" data-fragment-index="5" style="display:inline-block;border: 1px solid white;padding: 5px;line-height: 30px;">-->
                                        <!--<div class="partition-block blue" style="width:30px; height:30px;">&nbsp;</div>-->
                                        <!--<div class="partition-block blue" style="width:30px; height:30px;opacity: 0.7;">&nbsp;</div>-->
                                        <!--<div class="partition-block blue" style="width:30px; height:30px;opacity: 0.2;">&nbsp;</div>-->
                                        <!--<div class="partition-block blue" style="width:30px; height:30px;background-color: #243044">&nbsp;</div>-->
                                    <!--</div>-->
                                <!--</div>-->
                            <!--</div>-->
                        <!--</td>-->
                        <!--&lt;!&ndash;<td class="fragment" data-fragment-index="2" style="vertical-align: middle"><div class="right"><div class="arrow"></div></div></td>&ndash;&gt;-->
                    <!--</tr>-->
                    <!--<tr>-->
                        <!--<td class="fragment" data-fragment-index="1" style="vertical-align: middle">-->
                            <!--<div class="right"><div class="arrow"></div></div>-->
                        <!--</td>-->
                        <!--<td class="fragment" data-fragment-index="1" style="text-align: center">-->
                            <!--<div style=""><span style="background-color: #243044">&nbsp;Executor&nbsp;</span></div>-->
                            <!--<div style="border: 3px solid #5C97D6;padding: 10px;position: relative; z-index: -1;top:-20px;">-->
                                <!--<div class="fragment" data-fragment-index="5" style="display:inline-block;margin-top:10px;border: 1px solid white;padding:-->
                                        <!--5px;line-height: 30px;">-->
                                    <!--<div class="partition-block blue" style="width:30px; height:30px;">&nbsp;</div>-->
                                    <!--<div class="partition-block blue" style="width:30px; height:30px;opacity: 0.7;">&nbsp;</div>-->
                                    <!--<div class="partition-block blue" style="width:30px; height:30px;background-color: #243044">&nbsp;</div>-->
                                    <!--<div class="partition-block blue" style="width:30px; height:30px;background-color: #243044">&nbsp;</div>-->
                                <!--</div>-->
                            <!--</div>-->
                        <!--</td>-->
                        <!--&lt;!&ndash;<td class="fragment" data-fragment-index="2" style="vertical-align: middle"><div class="right"><div class="arrow"></div></div></td>&ndash;&gt;-->
                    <!--</tr>-->
                <!--</table>-->
            </section>
            <section data-transition="none none">
                <aside class="notes">
                    An RDD can be created using several methods. Overall, Spark can <b>interface</b> with a
                    wide variety of <b>distributed storage</b> including <b>HDFS</b>, <b>OpenStack Swift</b>, <b>Cassandra</b>,
                    <b>S3</b>, or even <b>custom solutions</b>. You can also load simple scala collections which is
                    great for testing purpose.

                    <!--Now you may wonder how does Spark decide about the number of partitions created and how does that-->
                    <!--affect its performances? Well it depends on the context.-->

                    As you can notice, each data loading method takes an additional parameter defining in how many
                    partitions the data should be split into once loaded. If it is not provided, Spark will fall back on
                    some default values either defined in its configuration or by the datasource itself. For example,
                    in the case of HDFS, Spark will create a single partition for each input split. Another example,
                    if you parallelize a Scala collection, you'll get as many partitions as there are cores available
                    in the cluster.

                    <!--Alright, why do we care? Well big data is about processing data which does not fit on a single machine.-->
                    <!--Therefore partitioning cannot be avoided. Secondly, the number of partitions processed by Spark at-->
                    <!--a given moment can have a significant impact on performances and can some time make the difference-->
                    <!--between a job which completes from one which does not. For these reasons, Spark provides different-->
                    <!--ways to redistribute the data in order to make it more manageable-->
                </aside>
                    <pre><code class="scala" data-trim data-noescape style="font-size: 1.3em; line-height: 1.2em">
/* sc stands for Spark's context */

// numSlices is optional
val ints: RDD[String] =
  sc.parallelize(Seq(1, 2, 3,4), numSlices)

// minPartitions is optional
val seqFiles: RDD[(Text, Text)] =
  sc.sequenceFile[Text, text](input, minPartitions)

// minPartitions is optional
val rdd3: RDD[String] =
  sc.textFile("hdfs://...", minPartitions)
                    </code></pre>
            </section>
        </section>
        <section>
            <h2>Transformation & Actions</h2>
            <section data-transition="none none">
                <aside class="notes">
                    Overall, an <b>RDD</b> supports <b>two types</b> of operation. <b>Transformations</b>, which create a new RDD
                    from an <b>existing</b> one, and <b>actions</b> which perform a computation on an RDD's dataset.
                    Most common <b>transformations</b> are <b>map</b>, <b>filter</b> or <b>flatMap</b>, while <b>actions</b>
                    are more about operations consisting in evaluating, <b>collecting</b>, <b>counting</b>, or <b>sampling</b> the data.
                    Actions will be typically responsible for <b>persisting</b> the result of a computation, or sending it back
                    to the <b>driver</b>.
                </aside>
                <div style="margin-top: 80px">
                    Transformations
                    <pre><code class="scala" style="font-size: 25px;">val wc: RDD[(String, Int)] = rdd
    .flatMap(_.split(" "))
    .map((_, 1))
    .reduceByKey(_ + _)</code></pre>

                    Actions
                    <pre><code class="scala" style="font-size: 25px;">
// Retrieves the data of each partition
val result: Seq[(String, Int)] = wc.collect()

// Writes the data in text files
val result: Unit = wc.saveAsTextFile("...")

// Counts the number of element in an RDD
val result: Long = wc.count()

// Retrieves the first four elements of an RDD
val result: Seq[(String, Int)] = wc.take(4)
                    </code></pre>
                </div>
            </section>
            <section data-transition="none none">
                <aside class="notes">
                    One very interesting aspect of <b>data loading</b> and <b>transformations</b>, is that they are <b>lazily
                    evaluated</b>. Actually, as long as <b>no action has been performed</b>, Spark may not have brought
                    data or computed <b>anything</b> at all. Each transformation actually creates a new RDD which
                    <b>maintains a pointer to its ancestors</b> along with the metadata regarding its relationship with
                    them. In <b>Spark</b> terminology, this is what we call the <b>lineage</b> of an RDD. Therefore
                    defining an RDD goes back to define a <b>specification</b> of what an executor should do in order to
                    compute a result.<br/><br/>

                    Now, concretely, an RDD's <b>lineage</b> is stored in an <b>directed acyclic graph</b> which has two
                    main purposes. First, it allows the <b>recomputing</b> of an RDD which data is lost or not available,
                    and <b>Secondly</b>, it is used by Spark to figure out an <b>optimal execution plan</b> we'll talk
                    about in a bit.
                </aside>
                <div style="display: block; text-align: center; margin-top:80px">
                    Lineage<br/>
                    <div style="display: inline-block; width:290px;padding:10px;margin: 10px;font-size: 35px;">
                        <div style="margin:10px;padding:5px;border: 1px solid white">textFile(...)</div>
                        <div class="up"><div class="arrow"></div></div>
                        <div style="margin:10px;padding:5px;border: 1px solid white">.filter(...)</div>
                        <div class="up"><div class="arrow"></div></div>
                        <div style="margin:10px;padding:5px;border: 1px solid white">.map(...)</div>
                        <div class="up"><div class="arrow"></div></div>
                        <div style="margin:10px;padding:5px;border: 1px solid white">.reduceByKey(...)</div>
                    </div>
                    <div style="display: inline-block; width:290px;font-size: 35px;">
                        <div style="margin:10px;padding:5px;border: 1px solid white">HadoopRDD</div>
                        <div class="up"><div class="arrow"></div></div>
                        <div style="margin:10px;padding:5px;border: 1px solid white">FilteredRDD</div>
                        <div class="up"><div class="arrow"></div></div>
                        <div style="margin:10px;padding:5px;border: 1px solid white">MappedRDD</div>
                        <div class="up"><div class="arrow"></div></div>
                        <div style="margin:10px;padding:5px;border: 1px solid white">ShuffledRDD</div>
                    </div>
                </div>
            </section>
            <!--<section data-transition="none none">-->
                <!--<aside class="notes">-->
                    <!--Now, if you want to have an idea about how an RDD is computed, you can give a glance at the-->
                    <!--<b>toDebugString method</b>, which provides a description of the RDD along with its dependencies.-->
                    <!--So if there is one thing you should take away from these first slides, it's that an RDD is nothing-->
                    <!--but a sequence of steps defining how each partition of the dataset should be processed in order to-->
                    <!--get the final result.-->

                    <!--Alright, so let's have a quick glance at how this looks like under the hood-->
                <!--</aside>-->
                <!--<pre><code class="console" style="font-size: 23px;">scala> wordOccurrences.toDebugString-->

<!--(4) MapPartitionsRDD[16] at map at &lt;console&gt;:26 []-->
<!--|  ShuffledRDD[15] at reduceByKey at &lt;console&gt;:26 []-->
<!--+-(4) MapPartitionsRDD[14] at map at &lt;console&gt;:26 []-->
<!--|  MapPartitionsRDD[13] at flatMap at &lt;console&gt;:26 []-->
<!--|  ParallelCollectionRDD[0] at parallelize at &lt;console&gt;:23 []</code></pre>-->
            <!--</section>-->
            <section data-transition="none none">
                <aside class="notes">
                    Whenever and RDD is evaluated, the sequence of computations which has to be performed is sent from
                    the driver to each executor, which start processing the partitions in parallel. The number of cores
                    available for each executor defines the number of partitions you can process at the same time. Once
                    the computation executed, its result is stored in memory or on disk for further processing.

                    In some cases, the data obtained after a transformation may require to be redistributed. This is
                    typically the case when you need to group the data in a certain way. As an example, in order to
                    count the number of occurrences of each word in a distributed set of documents, you need to regroup
                    all the occurrences of a specific word in the same partition.

                    The operation consisting in redistributing the data is called shuffling. Overall, it is performed in
                    three steps. First, the data is serialized on disk, then it is sent over the wire to the recipient
                    node and finally it is deserialized once received. So as you can guess, this may be a pretty expensive
                    operation especially if the data to shuffle reaches a certain size. Actually we'll see that this is
                    just the tip of the iceberg.
                </aside>
                <div style="display:table;margin: auto">
                    <div style="display:table-cell;vertical-align: middle;width:100px;">
                        <div class="rectangle blue-border">
                            <div class="child blue-border"><div class="child blue-border" style="line-height:70px;"><span style="font-size: 30px;">Data</span></div></div>
                        </div>
                    </div>
                    <div style="display:table-cell;vertical-align: middle;width:150px;">
                        <div style="margin-top:50px;" class="up-r"><div class="arrow"></div></div><br/>
                        <div class="right"><div class="arrow"></div></div><br/>
                        <div class="down-r"><div class="arrow"></div></div>
                    </div>
                    <div style="display:table-cell;vertical-align: middle;">
                        <div class="executor" style="margin-top:30px;">
                            <div class="title"><span>&nbsp;Executor&nbsp;</span></div>
                            <div class="body">
                                <div class="partition-list" style="display: inline-block">
                                    <div class="block blue">&nbsp;</div>
                                    <div class="block blue" style="opacity: 0.7;">&nbsp;</div>
                                    <div class="block blue" style="opacity: 0.2;">&nbsp;</div>
                                </div>
                                <div class="right" style="display: inline-block"><div class="arrow"></div></div>
                                <div class="partition-list" style="display: inline-block">
                                    <div class="block red">&nbsp;</div>
                                    <div class="block red" style="opacity: 0.7;">&nbsp;</div>
                                    <div class="block red" style="opacity: 0.2;">&nbsp;</div>
                                </div>
                            </div>
                        </div>
                        <div class="executor">
                            <div class="title"><span>&nbsp;Executor&nbsp;</span></div>
                            <div class="body">
                                <div class="partition-list" style="display: inline-block">
                                    <div class="block blue">&nbsp;</div>
                                    <div class="block blue" style="opacity: 0.7;">&nbsp;</div>
                                    <div class="block blue" style="opacity: 0.2;">&nbsp;</div>
                                </div>
                                <div class="right" style="display: inline-block"><div class="arrow"></div></div>
                                <div class="partition-list" style="display: inline-block">
                                    <div class="block red">&nbsp;</div>
                                    <div class="block red" style="opacity: 0.7;">&nbsp;</div>
                                    <div class="block red" style="opacity: 0.2;">&nbsp;</div>
                                </div>
                            </div>
                        </div>
                    </div>
                    <div class="fragment" data-fragment-index="2" style="display:table-cell;width:70px;vertical-align: middle;">
                        <div class="up-r" style="margin-top:50px;"  ><div class="arrow"></div></div><br/>
                        <div class="right" ><div class="arrow"></div></div><br/>
                        <div class="down-r"><div class="arrow"></div></div>
                    </div>
                    <div class="fragment" data-fragment-index="2" style="display:table-cell;vertical-align: middle;">
                        <div class="executor" style="margin-top:30px;">
                            <div class="title"><span>&nbsp;Executor&nbsp;</span></div>
                            <div class="body">
                                <div class="partition-list" style="display: inline-block">
                                    <div class="block red">&nbsp;</div>
                                    <div class="block red" style="opacity: 0.7;">&nbsp;</div>
                                </div>
                            </div>
                        </div>
                        <div class="executor">
                            <div class="title"><span>&nbsp;Executor&nbsp;</span></div>
                            <div class="body">
                                <div class="partition-list" style="display: inline-block">
                                    <div class="block red">&nbsp;</div>
                                    <div class="block red" style="opacity: 0.2;">&nbsp;</div>
                                </div>
                                <div class="partition-list" style="display: inline-block">
                                    <div class="block red" style="opacity: 0.7;">&nbsp;</div>
                                    <div class="block red" style="opacity: 0.2;">&nbsp;</div>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </section>
            <section data-transition="none none">
                <aside class="notes">
                    Having this in mind, transformations can be regrouped in two categories:
                    - Narrow transformations: that is transformations which do not involve any data shuffling.
                    - Wide transformations: that is transformations which may trigger a data redistribution.

                    The words Narrow and Wide actually refer to the way a partition resulting from a transformation is
                    obtained. A narrow transformation implies that a parent partition may be used to define only one
                    single child partition. On the other hand, a wide transformation implies that a parent partition may
                    be used to define one to multiple child partitions.


                    <!--So for example when you need to regroup the data in-->
                    <!--a specific way. As a good reminder, keep in mind how the arrows are oriented. In the case of a-->
                    <!--narrow transformation, the arrows are pointing to the same direction while with wide transformations,-->
                    <!--it's the opposite.-->

                    <!--Let's look at some examples.-->

                    <!--An example of a narrow transformation is map. Map only needs to know about the partition it's-->
                    <!--transforming in order to be performed. In contrast and given a Pair RDD, groupByKey regroups the-->
                    <!--tuples having the same key in the same partition. It therefore needs to scan the whole dataset and-->
                    <!--to split the data of each parent partition.-->

                    Notice also that in contrast with wide transformations, narrow transformations can be pipelined. For
                    now just keep that in mind, we'll get into this a little later while covering Spark's execution plan.
                </aside>

                <div>
                    <div style="display: inline-block;text-align: left;padding-right: 40px;">
                        Narrow tranformations:<br/>
                        <ul style="margin-left: 70px;">
                            <li>No Shuffle</li>
                            <li>Can be pipelined</li>
                        </ul>
                    </div>
                    <div style="display: inline-block; vertical-align: top;padding-left: 30px;text-align: center;">
                        <div style="display: inline-block;text-align: center">
                            <div style="border: 3px solid white; height: 50px; width: 50px">1</div>
                            <div style="margin-top: 20px;margin-left:20px;" class="down-r"><div class="arrow"></div></div>
                        </div>
                        <div style="display: inline-block;text-align: center;margin-left: 30px">
                            <div style="border: 3px solid white; height: 50px; width: 50px">2</div>
                            <div style="margin-top: 20px;margin-right:20px;" class="down-l"><div class="arrow"></div></div>
                        </div>
                        <div><div style=" display:inline-block; border: 3px solid white; height: 50px; width: 50px;">1,2</div></div>
                    </div>
                </div>
                <div style="margin-top: 60px;">
                    <div style="display: inline-block;text-align: left;padding-right: 32px;">
                        Wide tranformations:<br/>
                        <ul style="margin-left: 70px;">
                            <li>May Shuffle</li>
                            <li>Cannot be pipelined</li>
                        </ul>
                    </div>
                    <div style="display: inline-block; vertical-align: top;padding-left: 25px;text-align: center;">
                        <div><div style=" display:inline-block; border: 3px solid white; height: 50px; width: 50px;">1,2</div></div>
                        <div style="display: inline-block;text-align: center">
                            <div style="margin-top: 10px;" class="down-l"><div class="arrow"></div></div>
                            <div style="border: 3px solid white; height: 50px; width: 50px">1</div>
                        </div>
                        <div style="display: inline-block;text-align: center;margin-left: 30px">
                            <div style="margin-top: 10px;"  class="down-r"><div class="arrow"></div></div>
                            <div style="border: 3px solid white; height: 50px; width: 50px">2</div>
                        </div>
                    </div>
                </div>
            </section>
            <section data-transition="none none">
                <aside class="notes">
                    Here are some examples of common transformations you may use while writing a Spark application
                    along with how they are performed.

                    Map, flatMap, filter overall all work the same way. Whenever they are applied, they iterate over the
                    partition and output the result in another partition. These operations can be applied in parallel
                    on the whole dataset without having to move any data over the network. Therefore those are narrow
                    transformations.
                </aside>
                <div style="display:table;margin: auto;">
                    <div style="display:table-cell;vertical-align: middle;padding: 20px;">
                        <ul>
                            <li>map</li>
                            <li>flatMap</li>
                            <li>filter</li>
                            <li>...</li>
                        </ul>
                    </div>
                    <div style="display:table-cell;vertical-align: middle;">
                        <div class="executor" >
                            <div class="title"><span>&nbsp;Executor&nbsp;</span></div>
                            <div class="body">
                                <div class="partition-list" style="display: inline-block">
                                    <div class="block blue">&nbsp;</div>
                                    <div class="block blue" style="opacity: 0.7;">&nbsp;</div>
                                    <div class="block blue" style="opacity: 0.2;">&nbsp;</div>
                                </div>
                                <div class="right" style="display: inline-block"><div class="arrow"></div></div>
                                <div class="partition-list" style="display: inline-block">
                                    <div class="block red">&nbsp;</div>
                                    <div class="block red" style="opacity: 0.7;">&nbsp;</div>
                                    <div class="block red" style="opacity: 0.2;">&nbsp;</div>
                                </div>
                                <div></div>
                                <div class="partition-list" style="display: inline-block">
                                    <div class="block blue">&nbsp;</div>
                                    <div class="block blue" style="opacity: 0.7;">&nbsp;</div>
                                    <div class="block blue" style="opacity: 0.2;">&nbsp;</div>
                                </div>
                                <div class="right" style="display: inline-block"><div class="arrow"></div></div>
                                <div class="partition-list" style="display: inline-block">
                                    <div class="block red">&nbsp;</div>
                                    <div class="block red" style="opacity: 0.7;">&nbsp;</div>
                                    <div class="block red" style="opacity: 0.2;">&nbsp;</div>
                                </div>
                            </div>
                        </div>
                        <div class="executor">
                            <div class="title"><span>&nbsp;Executor&nbsp;</span></div>
                            <div class="body">
                                <div class="partition-list" style="display: inline-block">
                                    <div class="block blue">&nbsp;</div>
                                    <div class="block blue" style="opacity: 0.7;">&nbsp;</div>
                                    <div class="block blue" style="opacity: 0.2;">&nbsp;</div>
                                </div>
                                <div class="right" style="display: inline-block"><div class="arrow"></div></div>
                                <div class="partition-list" style="display: inline-block">
                                    <div class="block red">&nbsp;</div>
                                    <div class="block red" style="opacity: 0.7;">&nbsp;</div>
                                    <div class="block red" style="opacity: 0.2;">&nbsp;</div>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
                <!--<div style="display: inline-block; text-align: left; margin-right:60px;">-->
                    <!--<ul style="list-style: none; vertical-align: top">-->
                    <!--<li style="">sparkContext</li>-->
                    <!--<li class="fragment highlight-current-light-blue highlight-light-blue" data-fragment-index="1" style="padding-left: 50px">.textFile("hdfs://...")</li>-->
                    <!--<li class="fragment highlight-current-light-blue highlight-light-blue" data-fragment-index="2" style="padding-left: 50px">.flatMap(ws => ws.split(" "))</li>-->
                    <!--<li class="fragment highlight-current-light-blue highlight-light-blue" data-fragment-index="3" style="padding-left: 50px">.map(w => (w, 1))</li>-->
                    <!--<li class="fragment highlight-current-red highlight-red" data-fragment-index="4" style="padding-left: 50px">.reduceByKey(_ + _)</li>-->
                    <!--</ul>-->
                <!--</div>-->
                <!--<div style="display: inline-block;vertical-align: top">-->
                    <!--<div class="fragment border" data-fragment-index="6" style="display: inline-block;">-->
                        <!--<div class="fragment highlight-current-light-blue highlight-light-blue" data-fragment-index="1" style="">"a b"</div>-->
                        <!--<div style="margin-top: 10px;" class="down"><div class="arrow"></div></div>-->
                        <!--<div class="fragment highlight-current-light-blue highlight-light-blue" data-fragment-index="2" style="">"a", "b"</div>-->
                        <!--<div style="margin-top: 10px;" class="down"><div class="arrow"></div></div>-->
                        <!--<div class="fragment highlight-current-light-blue highlight-light-blue" data-fragment-index="3" style="">("a", 1)<br/>("b", 1)</div>-->
                    <!--</div>-->
                    <!--<div class="fragment border" data-fragment-index="6" style="display: inline-block; vertical-align: top; margin-left:30px;">-->
                        <!--<div class="fragment highlight-current-light-blue highlight-light-blue" data-fragment-index="1" style="">"a c"</div>-->
                        <!--<div style="margin-top: 10px;" class="down"><div class="arrow"></div></div>-->
                        <!--<div class="fragment highlight-current-light-blue highlight-light-blue" data-fragment-index="2" style="">"a", "c"</div>-->
                        <!--<div style="margin-top: 10px;" class="down"><div class="arrow"></div></div>-->
                        <!--<div class="fragment highlight-current-light-blue highlight-light-blue" data-fragment-index="3" style="">("a", 1)<br/>("c", 1)</div>-->
                    <!--</div>-->
                    <!--<div>-->
                        <!--<div style="display:inline-block;margin-top: 10px;" class="down-l"><div class="arrow"></div></div>-->
                        <!--<div style="display:inline-block;margin: 10px 40px 0 40px;" class="down"><div class="arrow"></div></div>-->
                        <!--<div style="display:inline-block;margin-top: 10px;" class="down-r"><div class="arrow"></div></div>-->
                    <!--</div>-->
                    <!--<div style="display: inline-block; vertical-align: top;">-->
                        <!--<div class="fragment highlight-current-red highlight-red" data-fragment-index="4" >("a", 2)<br/>("b", 1)</div>-->
                    <!--</div>-->
                    <!--<div style="display: inline-block; vertical-align: top; margin-left:30px;">-->
                        <!--<div class="fragment highlight-current-red highlight-red" data-fragment-index="4" >("c", 1)</div>-->
                    <!--</div>-->
                <!--</div>-->
            </section>
            <section data-transition="none none">
                <aside class="notes">
                    On the other hand, transformations like ReduceByKey, groupByKey, or sortByKey require a complete
                    knowledge of the whole dataset in order to be performed. And actually this makes totally sense when you think
                    about it, as for example how would you sort a dataset without scanning it completely?

                    So these are some transformations examples you are very likely to work with. Now let's look at how
                    Spark concretely evaluates them.
                </aside>
                <div style="display:table;margin: auto;">
                    <div style="display:table-cell;vertical-align: middle;padding: 20px;">
                        <ul>
                            <li>reduceByKey</li>
                            <li>groupByKey</li>
                            <li>join*</li>
                            <li>...</li>
                        </ul>
                    </div>
                    <div style="display:table-cell;vertical-align: middle">
                        <div class="executor">
                            <div class="title"><span>&nbsp;Executor&nbsp;</span></div>
                            <div class="body">
                                <div class="partition-list" style="display: inline-block">
                                    <div class="block blue">&nbsp;</div>
                                    <div class="block blue" style="opacity: 0.7;">&nbsp;</div>
                                    <div class="block blue" style="opacity: 0.2;">&nbsp;</div>
                                    <div class="block hidden">&nbsp;</div>
                                </div>
                            </div>
                        </div>
                        <div class="executor">
                            <div class="title"><span>&nbsp;Executor&nbsp;</span></div>
                            <div class="body">
                                <div class="partition-list" style="display: inline-block">
                                    <div class="block blue">&nbsp;</div>
                                    <div class="block blue" style="opacity: 0.7;">&nbsp;</div>
                                    <div class="block blue" style="opacity: 0.2;">&nbsp;</div>
                                    <div class="block hidden">&nbsp;</div>
                                </div>
                            </div>
                        </div>
                    </div>
                    <div style="display:table-cell;width:70px;vertical-align: middle;">
                        <div class="up-r" style="margin-top:15px;"  ><div class="arrow"></div></div><br/>
                        <div class="right" ><div class="arrow"></div></div><br/>
                        <div class="down-r"><div class="arrow"></div></div>
                    </div>
                    <div style="display:table-cell;vertical-align: middle;">
                        <div class="executor">
                            <div class="title"><span>&nbsp;Executor&nbsp;</span></div>
                            <div class="body">
                                <div class="partition-list" style="display: inline-block">
                                    <div class="block red">&nbsp;</div>
                                    <div class="block red" style="opacity: 0.7;">&nbsp;</div>
                                </div>
                            </div>
                        </div>
                        <div class="executor">
                            <div class="title"><span>&nbsp;Executor&nbsp;</span></div>
                            <div class="body">
                                <div class="partition-list" style="display: inline-block">
                                    <div class="block red">&nbsp;</div>
                                    <div class="block red" style="opacity: 0.2;">&nbsp;</div>
                                </div>
                                <div class="partition-list" style="display: inline-block">
                                    <div class="block red" style="opacity: 0.7;">&nbsp;</div>
                                    <div class="block red" style="opacity: 0.2;">&nbsp;</div>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </section>
        </section>
        <section>
            <h2>Spark Execution Plan</h2>
            <section data-transition="none none">
                <aside class="notes">
                    Now that you have a <b>basic understanding</b> of how an RDD works, we can get a little bit more
                    into details and see how it is <b>computed</b> by Spark<br/><br/>

                    <!--So, once again an RDD is <b>lazily evaluated</b>, and starts being computed when an action is called on it.-->
                    <!--Secondly it is represented as a <b>directed acyclic graph</b> which is a <b>logical representation</b> -->
                    <!--of what needs to be computed in order to get a result. Now, this <b>graph</b> is actually translated by -->
                    <!--Spark into a <b>physical execution plan</b>.<br/><br/>-->

                    So, whenever an action is called on an RDD, Spark translates the lineage of the RDD to a
                    execution plan. First it traverses the whole graph starting by the last transformation requested
                    and goes backward until the creation of the RDD. Once this done, Spark divides the work to be done
                    into one to multiples stages depending on the nature of the transformations required to obtain
                    the final result.<br/>

                    A stage is basically a set of <b>one to many</b> transformations which do not require any <b>data
                    movement</b> or <b>shuffling</b>. By shuffling, I mean re-grouping the data in a different way
                    among the partitions.

                    In the wordcount example, the first set of transformation can be pipelined, and performed in
                    parallel on each partition without changing the way the data is distributed. This kind of
                    transformation is actually called narrow transformations.

                    However, once we get here, all the tuples having the same key have to be stored in the same
                    partition in order to be counted. As some of those tuples may be stored in different partitions,
                    a shuffle is required. Concretely, this consists in writing the data on disk and in sending it
                    over the network in order to be redistributed. Every time we get in this situation, a new Stage is
                    created. Transformations involving data movement are referred to as wide-transformations.

                    Concretely a Stage is an optimization performed by Spark. Whenever one is created, its narrow
                    transformations are actually composed with each other in order to produce one single function. So
                    a Stage is a super operation in which transformations have been fused together in order to
                    prevent multiple reading of the same data and to avoid the overhead of each operation if they
                    were executed one after the other. Alright so this is the overall concept of Stages.

                    What's next? Well now, Spark has to perform those stages on the dataset. The operation consisting
                    in applying a Stage on a partition is called a Task. Given a stage, there will be as many tasks
                    executed as there are partitions available when the stage is executed. The reason I am saying that
                    is because the number of partitions may change between each Stage depending on the
                    transformations done, but we'll come back to this later. Overall, just keep in mind that given a
                    stage and a set of partitions, the number of tasks generated for this stage is equal to the
                    number of partitions.
                </aside>
                <div style="display:table;margin: 100px auto auto auto">
                    <div style="display:table-row;">
                        <div style="display:table-cell;vertical-align: middle;width:100px;">
                            <div class="rectangle blue-border">
                                <div class="child blue-border"><div class="child blue-border" style="line-height:70px;"><span style="font-size: 30px;">Data</span></div></div>
                            </div>
                        </div>
                        <div style="display:table-cell;vertical-align: middle;width:150px;">
                            <div style="margin-top:50px;" class="up-r"><div class="arrow"></div></div><br/>
                            <div class="right"><div class="arrow"></div></div><br/>
                            <div class="down-r"><div class="arrow"></div></div>
                        </div>
                        <div class="fragment border" data-fragment-index="1" style="display:table-cell;vertical-align: middle;margin-top:25px;padding:0 5px 0 5px;">
                            <div style="display:table-cell;vertical-align: middle;">
                                <div class="executor">
                                    <div class="title"><span>&nbsp;Executor&nbsp;</span></div>
                                    <div class="body">
                                        <div class="partition-list" style="display: inline-block">
                                            <div class="block blue">&nbsp;</div>
                                            <div class="block blue" style="opacity: 0.7;">&nbsp;</div>
                                            <div class="block blue" style="opacity: 0.2;">&nbsp;</div>
                                        </div>
                                        <div class="right" style="display: inline-block"><div class="arrow"></div></div>
                                        <div class="partition-list" style="display: inline-block">
                                            <div class="block red">&nbsp;</div>
                                            <div class="block red" style="opacity: 0.7;">&nbsp;</div>
                                            <div class="block red" style="opacity: 0.2;">&nbsp;</div>
                                        </div>
                                    </div>
                                </div>
                                <div class="executor">
                                    <div class="title"><span>&nbsp;Executor&nbsp;</span></div>
                                    <div class="body">
                                        <div class="partition-list" style="display: inline-block">
                                            <div class="block blue">&nbsp;</div>
                                            <div class="block blue" style="opacity: 0.7;">&nbsp;</div>
                                            <div class="block blue" style="opacity: 0.2;">&nbsp;</div>
                                        </div>
                                        <div class="right" style="display: inline-block"><div class="arrow"></div></div>
                                        <div class="partition-list" style="display: inline-block">
                                            <div class="block red">&nbsp;</div>
                                            <div class="block red" style="opacity: 0.7;">&nbsp;</div>
                                            <div class="block red" style="opacity: 0.2;">&nbsp;</div>
                                        </div>
                                    </div>
                                </div>
                            </div>
                        </div>
                        <div style="display:table-cell;width:70px;vertical-align: middle;">
                            <div class="up-r"  ><div class="arrow"></div></div><br/>
                            <div class="right" ><div class="arrow"></div></div><br/>
                            <div class="down-r"><div class="arrow"></div></div>
                        </div>
                        <div class="fragment border" data-fragment-index="2" style="display:table-cell;margin-top:25px;padding:0 5px 0 5px;vertical-align: middle;">
                            <div class="executor">
                                <div class="title"><span>&nbsp;Executor&nbsp;</span></div>
                                <div class="body">
                                    <div class="partition-list" style="display: inline-block">
                                        <div class="block red">&nbsp;</div>
                                        <div class="block red" style="opacity: 0.7;">&nbsp;</div>
                                    </div>
                                </div>
                            </div>
                            <div class="executor">
                                <div class="title"><span>&nbsp;Executor&nbsp;</span></div>
                                <div class="body">
                                    <div class="partition-list" style="display: inline-block">
                                        <div class="block red">&nbsp;</div>
                                        <div class="block red" style="opacity: 0.2;">&nbsp;</div>
                                    </div>
                                    <div class="partition-list" style="display: inline-block">
                                        <div class="block red" style="opacity: 0.7;">&nbsp;</div>
                                        <div class="block red" style="opacity: 0.2;">&nbsp;</div>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </div>
                    <div style="display:table-row;vertical-align: middle;width:100px;">
                        <div style="display:table-cell;"></div>
                        <div style="display:table-cell;"></div>
                        <div class="fragment" data-fragment-index="1" style="display:table-cell;text-align: center">
                            <span class="fragment flip" data-fragment-index="3">Stage 1</span>
                            <div class="fragment" data-fragment-index="3">
                                <div class="down"><div class="arrow orange"></div></div>
                                <div class="rectangle small orange-border" style="margin: 0 auto 0 auto">
                                    <div class="child small orange-border"><div class="child small orange-border">&nbsp;</div></div>
                                </div>
                            </div>
                        </div>
                        <div style="display:table-cell;"></div>
                        <div class="fragment" data-fragment-index="2" style="display:table-cell;text-align: center">
                            <span class="fragment flip" data-fragment-index="3">Stage 2</span>
                            <div class="fragment" data-fragment-index="3">
                                <div class="down"><div class="arrow orange"></div></div>
                                <div class="rectangle small orange-border" style="margin: 0 auto 0 auto">
                                    <div class="child small orange-border"><div class="child small orange-border">&nbsp;</div></div>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </section>

            <section data-transition="none none">
                <aside class="notes">
                    Fault Tolerance with Shuffling
                </aside>
                <div style="display:inline-block;vertical-align: top">
                    <div class="vertical-box-set" style="display: inline-block;vertical-align: top">
                        <div class="box red fragment background green" data-fragment-index="5"></div>
                        <div class="box red fragment background green" data-fragment-index="5"></div>
                        <div class="box red fragment background green" data-fragment-index="5"></div>
                    </div>
                    <div style="margin:25px 25px 0 25px;display:inline-block;vertical-align: top">
                        <div class="right"><div class="arrow"></div></div><br/>
                        <div class="right"><div class="arrow"></div></div><br/>
                        <div class="right"><div class="arrow"></div></div>
                    </div>
                    <div class="vertical-box-set" style="display: inline-block;vertical-align: top">
                        <div class="box red fragment background green" data-fragment-index="5"></div>
                        <div class="box red fragment background green" data-fragment-index="5"></div>
                        <div class="box red fragment background green" data-fragment-index="5"></div>
                    </div>
                    <div style="margin:25px 25px 0 25px;display: inline-block;vertical-align: top">
                        <div class="up-r"><div class="arrow"></div></div><br/>
                        <div class="right"><div class="arrow"></div></div><br/>
                        <div class="down-r"><div class="arrow"></div></div>
                    </div>
                    <div style="margin-top:50px;"></div>
                    <div class="vertical-box-set" style="display: inline-block;vertical-align: top">
                        <div class="fragment flip" data-fragment-index="3">
                            <div class="box blue fragment background green" data-fragment-index="2"></div>
                        </div>
                        <div class="fragment unflip" data-fragment-index="3">
                            <div class="box blue fragment background green" data-fragment-index="5"></div>
                        </div>
                        <div class="box blue fragment background green" data-fragment-index="5"></div>
                        <div class="box blue fragment background green" data-fragment-index="5"></div>
                    </div>
                    <div style="margin:25px 25px 0 25px;display:inline-block;vertical-align: top">
                        <div class="right"><div class="arrow"></div></div><br/>
                        <div class="right"><div class="arrow"></div></div><br/>
                        <div class="right"><div class="arrow"></div></div>
                    </div>
                    <div class="vertical-box-set" style="display:inline-block;vertical-align: top">
                        <div class="fragment flip" data-fragment-index="3">
                            <div class="box blue fragment background green" data-fragment-index="1"></div>
                        </div>
                        <div class="fragment unflip" data-fragment-index="3">
                            <div class="box blue fragment background green" data-fragment-index="5"></div>
                        </div>
                        <div class="box blue fragment background green" data-fragment-index="5"></div>
                        <div class="box blue fragment background green" data-fragment-index="5"></div>
                    </div>
                    <div style="margin:25px 25px 0 25px;display:inline-block;vertical-align: top">
                        <div class="up-r"><div class="arrow"></div></div><br/>
                        <div class="right"><div class="arrow"></div></div><br/>
                        <div class="down-r"><div class="arrow"></div></div>
                    </div>
                </div>
                <div style="display:inline-block;vertical-align: top;margin-top:42px;">
                    <div class="vertical-box-set" style="display: inline-block;vertical-align: middle">
                        <div class="box purple"></div>
                        <div class="box purple"></div>
                        <div class="box purple fragment background green" data-fragment-index="4"></div>
                        <div class="box purple"></div>
                        <div class="box purple"></div>
                        <div class="box purple"></div>
                    </div>
                </div>
            </section>






            <section data-transition="none none">
                <aside class="notes">
                    Ok but this comes with some tradeoff however. No matter how you distribute the data, you are still
                    constrained by the resources available for each executor, which includes the memory. Therefore if
                    you try to process a partition which does not fit in an executor's memory, you just end up with
                    out of memory exceptions. On the other hand, having many small partition may also be a problem as
                    it would require a high amount of IO generated whenever a partition is loaded in memory.

                    Another typical problem, is skewed data. Having terrabytes of memory along with hundreds of cores
                    won't help you if the data is all stored in the same executor which is limited to use only a subset
                    of the cores available. So it is critical to make sure, first that the data is properly distributed,
                    and secondly that each partition can be processed efficiently by each executor.

                    Finally, as those executors may run on different machines, whenever you redistribute the data, it
                    has to be serialized on disk, then sent over the wire, and then deserialized once it has been received.
                    As you can guess, this is a pretty expansive operation which, in the Spark jargon is referred to as
                    a shuffle. During a shuffle and depending on what caused it, the number of partitions may change
                    and so their size, which may also affect Spark's performances.

                    So long story short, data redistribution has to be be done cautiously, first because it may be just
                    slow down or even prevent a job from completing and secondly because shuffling can be a very expensive
                    operation. Now fortunately, these problems can be avoided, or at least minimized using the right
                    operators. There are several aspects to consider so let's start with the data operations provided by
                    Spark's API.
                </aside>

                <table style="margin-top: 40px">
                    <tr>
                        <td></td>
                        <td></td>
                        <td class="fragment" data-fragment-index="2" style="vertical-align: middle;text-align: center">T</td>
                        <td></td>
                        <td class="fragment" data-fragment-index="6" style="vertical-align: middle;text-align: center">T + 1</td>
                    </tr>
                    <tr>
                        <td style="border: 3px solid white;vertical-align: middle;text-align: center" rowspan="3">
                            <div class="fragment cover" data-fragment-index="5">
                                <div class="fragment" data-fragment-index="1">
                                    <div class="partition-block blue" style="width:30px; height:30px;opacity: 0.4;">&nbsp;</div>
                                    <div class="partition-block blue" style="width:30px; height:30px;">&nbsp;</div><br/>
                                    <div class="partition-block blue" style="width:30px; height:30px;opacity: 0.7;">&nbsp;</div>
                                </div>
                            </div>
                            Data<br/>
                            <div class="fragment cover" data-fragment-index="5" style="margin-top: 20px;">
                                <div class="fragment" data-fragment-index="1">
                                    <div class="partition-block blue" style="width:30px; height:30px;opacity: 0.4;">&nbsp;</div><br/>
                                    <div class="partition-block blue" style="width:30px; height:30px;">&nbsp;</div>
                                    <div class="partition-block blue" style="width:30px; height:30px;opacity: 0.7;">&nbsp;</div><br/>
                                    <div class="partition-block blue" style="width:30px; height:30px;opacity: 0.4;">&nbsp;</div>
                                </div>
                            </div>
                        </td>
                        <td class="fragment" data-fragment-index="2" style="vertical-align: middle">
                            <div class="right"><div class="arrow"></div></div>
                        </td>
                        <td class="fragment" data-fragment-index="2" style="text-align: center;">
                            <div style=""><span style="background-color: #243044">&nbsp;Executor&nbsp;</span></div>
                            <div style="border: 3px solid #5C97D6;padding: 10px;position: relative; z-index: -1;top:-20px;">
                                <div class="fragment" data-fragment-index="5"  style="display:inline-block;margin-top:10px;border: 1px solid white;padding: 5px;line-height: 30px;">
                                    <div class="partition-block blue" style="width:30px; height:30px;">&nbsp;</div>
                                    <div class="partition-block blue" style="width:30px; height:30px;opacity: 0.7;">&nbsp;</div>
                                    <div class="partition-block blue" style="width:30px; height:30px;background-color: #243044">&nbsp;</div>
                                    <div class="partition-block blue" style="width:30px; height:30px;background-color: #243044">&nbsp;</div>
                                </div>
                            </div>
                        </td>
                        <td class="fragment" data-fragment-index="6" style="vertical-align: middle">
                            <div class="up-r"><div class="arrow"></div></div><br/>
                            <div class="right"><div class="arrow"></div></div><br/>
                            <div class="down-r"><div class="arrow"></div></div><br/>
                        </td>
                        <td class="fragment" data-fragment-index="6" style="text-align: center">
                            <div style=""><span style="background-color: #243044">&nbsp;Executor&nbsp;</span></div>
                            <div style="border: 3px solid #5C97D6;padding: 10px;position: relative; z-index: -1;top:-20px;">
                                <div class="fragment" data-fragment-index="5" style="display:inline-block;margin-top:10px;border: 1px solid white; padding: 5px;line-height: 30px;">
                                    <div class="partition-block blue" style="width:30px; height:30px;">&nbsp;</div>
                                    <div class="partition-block blue" style="width:30px; height:30px;opacity: 0.7;">&nbsp;</div>
                                    <div class="partition-block blue" style="width:30px; height:30px;opacity: 0.4;">&nbsp;</div>
                                    <div class="partition-block blue" style="width:30px; height:30px;background-color: #243044">&nbsp;</div>
                                </div>
                            </div>
                        </td>
                    </tr>
                    <tr>
                        <td class="fragment" data-fragment-index="2" style="vertical-align: middle">
                            <div class="right"><div class="arrow"></div></div>
                        </td>
                        <td class="fragment" data-fragment-index="2" style="text-align: center">
                            <div style=""><span style="background-color: #243044">&nbsp;Executor&nbsp;</span></div>
                            <div style="border: 3px solid #5C97D6;padding: 10px;position: relative; z-index: -1;top:-20px;">
                                <div style="margin-top:10px;">
                                    <div class="fragment" data-fragment-index="5" style="display:inline-block;border: 1px solid white;padding: 5px;line-height: 30px;">
                                        <div class="partition-block blue" style="width:30px; height:30px;">&nbsp;</div>
                                        <div class="partition-block blue" style="width:30px; height:30px;opacity: 0.7;">&nbsp;</div>
                                        <div class="partition-block blue" style="width:30px; height:30px;background-color: #243044">&nbsp;</div>
                                        <div class="partition-block blue" style="width:30px; height:30px;background-color: #243044">&nbsp;</div>
                                    </div>
                                </div>

                                <div style="margin-top: 5px">
                                    <div class="fragment" data-fragment-index="5" style="display:inline-block;border: 1px solid white;padding: 5px;line-height: 30px;">
                                        <div class="partition-block blue" style="width:30px; height:30px;">&nbsp;</div>
                                        <div class="partition-block blue" style="width:30px; height:30px;opacity: 0.7;">&nbsp;</div>
                                        <div class="partition-block blue" style="width:30px; height:30px;opacity: 0.4;">&nbsp;</div>
                                        <div class="partition-block blue" style="width:30px; height:30px;background-color: #243044">&nbsp;</div>
                                    </div>
                                </div>
                            </div>
                        </td>
                        <td class="fragment" data-fragment-index="6" style="vertical-align: middle">
                            <div class="up-r"><div class="arrow"></div></div><br/>
                            <div class="right"><div class="arrow"></div></div><br/>
                            <div class="down-r"><div class="arrow"></div></div><br/>
                        </td>
                        <td class="fragment" data-fragment-index="6" style="text-align: center;vertical-align: middle">
                            <div style=""><span style="background-color: #243044">&nbsp;Executor&nbsp;</span></div>
                            <div style="border: 3px solid #5C97D6;padding: 10px;position: relative; z-index: -1;top:-20px;">
                                <div style="margin-top: 10px">
                                    <div style="display:inline-block;border: 1px solid white;padding: 5px;line-height: 30px;">
                                        <div class="partition-block blue" style="width:30px; height:30px;">&nbsp;</div>
                                        <div class="partition-block blue" style="width:30px; height:30px;opacity: 0.7;">&nbsp;</div>
                                        <div class="partition-block blue" style="width:30px; height:30px;opacity: 0.4;">&nbsp;</div>
                                        <div class="partition-block blue" style="width:30px; height:30px;">&nbsp;</div>
                                    </div>
                                </div>
                            </div>
                        </td>
                    </tr>
                </table>
            </section>
        </section>

        <section>
            <h2>Transformation & Actions</h2>
            <section data-transition="none none">
                <aside class="notes">
                    In this example, we execute the well-known word count algorithm, designed to count the number of
                    occurrences of each word in a distributed set of documents. In a nutshell, the wordcount algorithm
                    process in two steps. It first prepares the data and map it to key value tuples, and then regroups
                    all the tuples having the same key in order to count the number of occurrences. This is a classic
                    map-reduce problem.

                    The mapping phase is composed of three instructions, textFile, flatMap and map. As you can see,
                    the first set of instruction can be performed on each partition, in parallel, without any knowledge
                    of the data stored in other partitions of the dataset. Therefore, each parent partition is used to
                    define only one child partition. These instructions are narrow transformations.

                    On the other hand the last instruction, that is reduceByKey, needs a complete scan of the dataset
                    in order to be performed. Indeed, regrouping all the tuples having the same key necessarily implies to
                    scan each partition. This is why reducebyKey is a wide transformation.

                    Earlier I told you that narrow transformations can be pipelined. This is possible thanks to the fact
                    that, once again, a narrow transformation does not need to know about the whole dataset to be performed.
                    In our example, textFile, flatMap and map could be regrouped in a single function without altering
                    the algorithm's behaviour. We'll see later that this is actually what Spark does in order to reduce
                    the amount of IO.
                </aside>
                <div style="display: inline-block; text-align: left; margin-right:60px;">
                    <ul style="list-style: none; vertical-align: top">
                    <li style="">sparkContext</li>
                    <li class="fragment highlight-current-light-blue highlight-light-blue" data-fragment-index="1" style="padding-left: 50px">.textFile("hdfs://...")</li>
                    <li class="fragment highlight-current-light-blue highlight-light-blue" data-fragment-index="2" style="padding-left: 50px">.flatMap(ws => ws.split(" "))</li>
                    <li class="fragment highlight-current-light-blue highlight-light-blue" data-fragment-index="3" style="padding-left: 50px">.map(w => (w, 1))</li>
                    <li class="fragment highlight-current-red highlight-red" data-fragment-index="4" style="padding-left: 50px">.reduceByKey(_ + _)</li>
                    </ul>
                </div>
                <div style="display: inline-block;vertical-align: top">
                    <div class="fragment border" data-fragment-index="6" style="display: inline-block;">
                        <div class="fragment highlight-current-light-blue highlight-light-blue" data-fragment-index="1" style="">"a b"</div>
                        <div style="margin-top: 10px;" class="down"><div class="arrow"></div></div>
                        <div class="fragment highlight-current-light-blue highlight-light-blue" data-fragment-index="2" style="">"a", "b"</div>
                        <div style="margin-top: 10px;" class="down"><div class="arrow"></div></div>
                        <div class="fragment highlight-current-light-blue highlight-light-blue" data-fragment-index="3" style="">("a", 1)<br/>("b", 1)</div>
                    </div>
                    <div class="fragment border" data-fragment-index="6" style="display: inline-block; vertical-align: top; margin-left:30px;">
                        <div class="fragment highlight-current-light-blue highlight-light-blue" data-fragment-index="1" style="">"a c"</div>
                        <div style="margin-top: 10px;" class="down"><div class="arrow"></div></div>
                        <div class="fragment highlight-current-light-blue highlight-light-blue" data-fragment-index="2" style="">"a", "c"</div>
                        <div style="margin-top: 10px;" class="down"><div class="arrow"></div></div>
                        <div class="fragment highlight-current-light-blue highlight-light-blue" data-fragment-index="3" style="">("a", 1)<br/>("c", 1)</div>
                    </div>
                    <div>
                        <div style="display:inline-block;margin-top: 10px;" class="down-l"><div class="arrow"></div></div>
                        <div style="display:inline-block;margin: 10px 40px 0 40px;" class="down"><div class="arrow"></div></div>
                        <div style="display:inline-block;margin-top: 10px;" class="down-r"><div class="arrow"></div></div>
                    </div>
                    <div style="display: inline-block; vertical-align: top;">
                        <div class="fragment highlight-current-red highlight-red" data-fragment-index="4" >("a", 2)<br/>("b", 1)</div>
                    </div>
                    <div style="display: inline-block; vertical-align: top; margin-left:30px;">
                        <div class="fragment highlight-current-red highlight-red" data-fragment-index="4" >("c", 1)</div>
                    </div>
                </div>
            </section>
            <section data-transition="none none">
                <aside class="notes">
                    Another very common transformation which is likely to perform a data re-partitioning consist in
                    joining two RDDs. Just like reduceByKey, the join function regroups all the tuples having the same
                    key in the same partition, and therefore may perform a full scan of the dataset. Join is a very common
                    transformation and needs to be done carefully. At the end of this talk, I'll give you some tricks to do properly
                    do this.
                </aside>
                <div style="display: inline-block">
                    <div style="display: table">
                        <div style="display:table-cell; vertical-align: middle;padding-right: 10px;">rdd1</div>
                        <div style="display:table-cell; border-left:3px solid white;border-top:3px solid white;border-bottom:3px solid white;vertical-align: top;">&nbsp;</div>
                        <div style="display:inline-block; border:1px solid white;margin-left: 10px;">
                            <div style="border:1px solid white;width:110px;">('a', 1)</div>
                            <div style="border:1px solid white;width:110px;">('b', 1)</div>
                            <div style="border:1px solid white;width:110px;">('a', 1)</div>
                            <div style="border:1px solid white;width:110px;">('c', 1)</div>
                        </div>
                        <div style="display:table-cell; vertical-align: middle;">
                            <div style="margin-left: 30px;" class="down-r"><div class="arrow"></div></div><br/>
                        </div>
                    </div>

                    <div style="display: table; margin-top: 30px;">
                        <div style="display:table-cell; vertical-align: middle;padding-right: 10px;">rdd2</div>
                        <div style="display:table-cell; border-left:3px solid white;border-top:3px solid white;border-bottom:3px solid white;vertical-align: top;">&nbsp;</div>
                        <div style="display:inline-block; border:1px solid white;margin-left: 10px;">
                            <div style="border:1px solid white;width:110px;">('a', 1)</div>
                            <div style="border:1px solid white;width:110px;">('b', 1)</div>
                            <div style="border:1px solid white;width:110px;">('a', 1)</div>
                        </div>
                        <div style="display:table-cell; vertical-align: middle;">
                            <div style="margin-left: 30px;" class="up-r"><div class="arrow"></div></div><br/>
                        </div>
                    </div>
                </div>
                <div style="display: inline-block; vertical-align: top;margin-top: 165px;">
                    <div style="display: inline-block; vertical-align: top;">rdd1.join(rdd2)</div>
                </div>
                <div style="display: inline-block; vertical-align: top;margin: 123px 0 0 30px;">
                    <div style="display: inline-block;">
                        <div class="up-r"><div class="arrow"></div></div><br/>
                        <div class="right"><div class="arrow"></div></div><br/>
                        <div class="down-r"><div class="arrow"></div></div>
                    </div>
                </div>
                <div style="display: inline-block; vertical-align: top;margin: 117px 0 0 10px">
                    <div style="display: inline-block;vertical-align: top;">
                        <div style="display: table;">
                            <div style="display:inline-block; border:1px solid white;margin-left: 10px;">
                                <div style="border:1px solid white;">('a', [1, 1, 1, 1])</div>
                                <div style="border:1px solid white;">('b', [1, 1])</div>
                                <div style="border:1px solid white;">('c', [1])</div>
                            </div>
                        </div>
                    </div>
                </div>
            </section>

            <section data-transition="none none">
                <aside class="notes">
                    Finally, you can also explicitly ask Spark to redistribute the data using coalesce and repartition.
                    Overall these two functions consists in changing the number of partitions used to store the data.
                    Coalesce will allow you to reduce this number while repartition increases it. Now under the hood,
                    coalesce is actually an optimized specialization of repartition. The main difference is that coalesce
                    will just regroup the partitions in a smaller amount of partitions, without worrying about how well
                    the data is spread, while repartition garantees the data to be as evenly spread as possible.

                    Now considering what we said so far, could you tell me what kind of transformation are coalesce and
                    repartition?
                </aside>
                <div class="partitioning-title">coalesce</div>
                <div style="display:inline-block;padding-right: 20px;vertical-align: top">
                    <div class="partition" style="margin:0 0 10px 10px;"><span>1, 2</span></div>
                    <div class="partition" style="margin:0 0 10px 10px;"><span>3</span></div>
                    <div class="partition" style="margin:0 0 10px 10px;"><span>4</span></div>
                </div>
                <div style="display:inline-block;width:5%">
                    <div class="down-r" style="margin: 0 0 0 0;"><div class="arrow"></div></div><br/>
                    <div class="right" style="margin: 0 0 0 0;"><div class="arrow"></div></div><br/>
                    <div class="right" style="margin: 0 0 0 0;"><div class="arrow"></div></div>
                </div>
                <div style="display:inline-block;border-right: 2px solid white;padding-right: 35px;">
                    <div class="partition hidden" style="margin:0 0 10px 10px;"><span>&nbsp;</span></div>
                    <div class="partition" style="margin:0 0 10px 10px;"><span>1, 2, 3</span></div>
                    <div class="partition" style="margin:0 0 10px 10px;"><span>4</span></div>
                </div>
                <div class="partitioning-pros">
                    <ul>
                        <li>Not evenly spread</li>
                        <li class="fragment" data-fragment-index="1">No shuffle</li>
                        <li class="fragment" data-fragment-index="1">Narrow transformation</li>
                    </ul>
                </div>
                <div style="width:100%;margin-top:50px;"></div>
                <div class="partitioning-title">repartition</div>
                <div style="display:inline-block;padding-right: 20px;vertical-align: top">
                    <div class="partition" style="margin:0 0 10px 10px;"><span>1, 2, 3</span></div>
                    <div class="partition" style="margin:0 0 10px 10px;"><span>4, 5</span></div>
                </div>
                <div style="display:inline-block;width:5%;margin-top:15px;">
                    <div class="up-r"   style="margin: 0 0 0 0;"><div class="arrow"></div></div><br/>
                    <div class="right"  style="margin: 0 0 0 0;"><div class="arrow"></div></div><br/>
                    <div class="down-r" style="margin: 0 0 0 0;"><div class="arrow"></div></div>
                </div>
                <div style="display:inline-block;border-right: 2px solid white;padding-right: 35px;vertical-align: top">
                    <div class="partition" style="margin:0 0 10px 10px;"><span>1, 3</span></div>
                    <div class="partition" style="margin:0 0 10px 10px;"><span>2, 4</span></div>
                    <div class="partition" style="margin:0 0 10px 10px;"><span>5</span></div>
                </div>
                <div class="partitioning-pros">
                    <ul>
                        <li>Evenly spread</li>
                        <li class="fragment" data-fragment-index="1">Full shuffle</li>
                        <li class="fragment" data-fragment-index="1">Wide transformation</li>
                    </ul>
                </div>
            </section>

            <section data-transition="none none">
                <aside class="notes">
                    Alright, couple of things you have to keep in mind regarding transformations. First of all,
                    transformations need memory in order to be executed, this may sound silly, but this is something
                    people tends to forget. Therefore any object created during a transformation will actually be
                    created as many time as there are record in the dataset. Well, this is not completely accurate as
                    we'll see that there is a special transformation which allows you to prevent that, but overall,
                    creating many objects increases the pressure on the memory and triggers more garbage collection
                    cycles, which slow down the overall execution. It is therefore critical to reduce a transformation's
                    memory footprint.
                </aside>
                Schema avec deux partitions et une fonction map entre les deux. Rajouter du code avec une création
                d'objet.
            </section>


        </section>

        <section data-transition="none none">
            <h2>Spark execution plan</h2>
            <section data-transition="none none">
                <aside class="notes">
                    Alright so what about this optimized plan? So I just told you that whenever you transform an RDD, you
                    actually create a new one which points to its parent. Whenever an <b>action</b> is called on it,
                    Spark's <b>scheduler</b> will visit each <b>RDD</b> starting from the <b>final RDD</b>, that is the
                    one obtained with the last <b>transformation</b>, and will then go <b>backward</b> to find what needs to
                    be computed until the <b>first RDD</b> has been reached. During this process, Spark will create
                    <b>computation stages</b>.

                    A stage is basically a set of <b>one to many</b> transformations which do not require any <b>data
                    movement</b> or <b>shuffling</b>. Therefore, a set of narrow transformations, which can be pipelined.
                    So a stage is some kind of a <b>super-operation</b> in which <b>transformations</b> have been
                    <b>fused</b> together in order to prevent multiple reading of the same data and to avoid the
                    <b>overhead</b> of each operation if they were <b>executed</b> one after the other.

                    In other words, as long as no data needs to be moved from one partition to another, operations are
                    performed during the same stage. But as we've seen it with the word count example, this is not always
                    possible, at some point, you may need to redistribute the data in order to get a result. And whenever
                    this happens, you actually create a new stage.

                    Now, each stage is composed of as <b>many task</b> as there are partitions available <b>when its
                    executed</b>. So if you have <b>10 partitions</b> for a given stage, the scheduler will create
                    <b>10 tasks</b> for it. As you can guess, the number of tasks processed in parallel depends on the
                    number of cores available in the cluster, therefore <b>the more task</b> your whole Spark process needs
                    to execute, <b>the longer</b> it will take to compute the final result.
                </aside>
                <img style="margin-top:100px;margin-left:50px; box-shadow: 0; background-color: #FFFFFF; width:100%" src="images/spark_scala_meetup/execution_plan_3.png"/>
            </section>
            <section data-transition="none none">
                <img style="margin-top:100px;margin-left:50px; box-shadow: 0; background-color: #FFFFFF; width:100%" src="images/spark_scala_meetup/execution_plan_2.png"/>
            </section>
            <section data-transition="none none">
                <img style="margin-top:100px;margin-left:50px; box-shadow: 0; background-color: #FFFFFF; width:100%" src="images/spark_scala_meetup/execution_plan_1.png"/>
            </section>
            <section data-transition="none none">
                <aside class="notes">
                    Earlier I told you that shuffling is expensive because of the data serialization it implies. Well,
                    as a matter of fact, this is just the tip of the iceberg.

                    One thing to keep in mind regarding how stages are executed, is that they are executed sequentially.
                    In other words, a stage will only start if all the tasks of the previous stage have been processed.
                    And once again, this is because a wide transformation needs to know about the whole dataset and
                    therefore to have all its parent partitions computed before starting.

                    Secondly, every time a task has been performed, its result is stored on disk and waits to be consumed
                    by the next stage. Therefore, if you have a high amount of tasks, you may get into memory and disk
                    space problems, along with a high amount of data serialized during the shuffle.
                </aside>
                Schema with two stages. The first stage contains some tasks processed in parallel and goes from the left
                to the right. Once done, those results are passed to the next stage...
            </section>
            <section data-transition="none none">
                <aside class="notes">
                    Shuffling can also have important implications for fault tolerance. Remember that an RDD is
                    resilient, so whenever a partition is lost, Spark is able to recompute it using the RDD's lineage.
                    However, there is a caveat here. If the partition results from a shuffle, Spark may have to
                    re-compute all the partitions involved in the shuffling in order to retrieve the missing partition.
                </aside>
                Schema with fault tolerance with map and groupBy or whatever
            </section>
            <!--<section data-transition="none none">-->
                <!--<aside class="notes">-->
                    <!--Regarding transformations, you also have to keep in mind that those need memory in order to be-->
                    <!--executed, and that any object created during a transformation will be created for each record of-->
                    <!--the data. Well, this is not completely accurate but overall, creating many objects increase the-->
                    <!--pressure on the memory and will trigger more garbage collection cycles, which slow down the overall-->
                    <!--execution. It is therefore critical to reduce a transformation's memory footprint.-->
                <!--</aside>-->
            <!--</section>-->
            <!--<section data-transition="none none">-->
                <!--<aside class="notes">-->
                    <!--Finally, having terrabytes of memory along with hundreds of cores won't help you if the data is all-->
                    <!--stored in the same executor which is limited to use only a subset of the cores available. So it is-->
                    <!--critical to make sure, first that the data is properly distributed, and secondly that each partition-->
                    <!--can be processed efficiently by each executor.-->

                    <!--Earlier we said that you can re-partition the into smaller or bigger partitions, but not-->
                    <!--without making some tradeoffs. Having many partitions may prevent an executor's memory from blowing-->
                    <!--up however, it would involve more IO, as each partition is loaded one by one.-->
                <!--</aside>-->
            <!--</section>-->
        </section>
        <section data-transition="none none">
            <h2>Recap</h2>
            <aside class="notes">
                Now that you have a better understanding of how Spark works under the hood, let's point out some
                of the problems we briefly mentioned.

                First of all, we actually mentioned that a couple of time, a good data distribution is one of
                the key for writing efficient Spark jobs. However, it's harder to do than it sounds like. Should I
                decrease the number of partition, should I increase it, how should I distribute the data? In the next
                slides, we'll attempt to provide some answers.

                Secondly, transformations. So transformations are pretty much the backbone of an RDD. So just like data
                distribution, they have to be taken care of and understanding how they are performed is essential.
                If they are badly written, they may have a high memory consumption which would slow down the overall
                execution of your Spark job. Optimizing transformations is easier then figuring out how data should
                be distributed but still, there is a couple of things to keep in mind.

                Last but least, shuffling. Shuffling is probably the most expensive operation you can perform in
                Spark, and therefore, if there is something you should take away from this talk, it's how you can
                handle it properly.

                So overall, there are different aspects of a Spark job you can work on in order to improve its
                performances. In the next slides we'll cover some techniques allowing you to do so.
            </aside>
            <div style="text-align: left">
                Spark in a nutshell:
                <ul style="margin-left: 80px;">
                    <li>Cluster of executors and driver</li>
                    <li>RDD, computation over a distributed dataset organized in partitions which may be re-arranged</li>
                    <li>Shuffling</li>
                    <li>Narrow and Wide transformations</li>
                    <li>Stages, tasks</li>
                </ul>
            </div>
            <div style="text-align: left">
                Optimizations possible:
                <ul style="margin-left: 80px;">
                    <li>Partitioning</li>
                    <li>Transformations</li>
                    <li>Shuffling</li>
                </ul>
            </div>
        </section>


        <!--<section data-transition="none none">-->
            <!--<h2>Recap</h2>-->
            <!--<aside class="notes">-->
                <!--All these reasons should convince you that properly distributing the data and shuffling efficiently-->
                <!--are the keys to write optimized Spark jobs. For these reasons, the next part is mostly best-->
                <!--practices you should keep in mind whenever a job gets slow or impossible to complete.-->


                <!--Efficient transformation-->
                <!-- - object creation-->
                <!-- - BCV/ACC-->
                <!-- - iterator 2 iterator-->
                <!-- - cache-->

                <!--Efficient partitioning-->
                <!-- - partitioner-->
                <!-- - data skewing???-->

                <!--Efficient shuffling-->
                <!-- - co/location/partitioner-->
                <!-- - Filter data + some other tricks-->
                <!-- - Reduce key space-->

                <!--Conclusion-->

                <!--&lt;!&ndash; Maybe separate this slide in 3:-->
                 <!--1) Stages + Tasks-->
                 <!--2) fault tolerance-->
                 <!--3) Data skewing-->
                 <!--&ndash;&gt;-->
            <!--</aside>-->
            <!--<div>-->
                <!--<ul>-->
                    <!--<li>Data distribution (skewing, too few, too many)</li>-->
                    <!--<li>Data shuffling (serialization, bottlenecks,...)</li>-->
                    <!--<li>Transformations (data creation....)</li>-->
                    <!--<li>Too much memory means garbage collection</li>-->
                <!--</ul>-->
            <!--</div>-->
        <!--</section>-->

        <section data-transition="none none">
            <h2>Efficient Transformations</h2>
            <section data-transition="none none">
                <aside class="notes">
                    Alright, efficient transformations. First let's quickly think about what could wrong in this code


                    def aggregate[U](zeroValue: U)(
                      seqOp: (U, T) ⇒ U,   // merging T into a U
                      combOp: (U, U) ⇒ U   // Merges two U's
                    ): U




class Result(var total: Int) extends Serializable
val inputrdd = sc.parallelize(Seq(("maths", 21), ("english", 22), ("science", 31)), 3)

val result = inputrdd.aggregate(new Result(0))(
  (acc, value) => new Result(acc.total + value._2),
  (acc1, acc2) => new Result(acc1.total + acc2.total)
)


class Result(var total: Int) extends Serializable
val inputrdd = sc.parallelize(Seq(("maths", 21), ("english", 22), ("science", 31)), 3)

val result = inputrdd.aggregate(new Result(0))(
  (acc, value) => {
    acc.total = acc.total + value._2
    acc
  },
  (acc1, acc2) => {
    acc1.total = acc1.total + acc2.total
    acc1
  }
)

                    Aggregate the elements of each partition, and then the results for all the partitions, using given
                    combine functions and a neutral "zero value". This function can return a different result type, U,
                    than the type of this RDD, T. Thus, we need one operation for merging a T into an U and one
                    operation for merging two U's, as in scala.TraversableOnce. Both of these functions are allowed to
                    modify and return their first argument instead of creating a new U to avoid memory allocation.



                    reasons why transformations should be optimized:
                    - garbage collection
                    - memory limited

                    limit object creation
                    - use primitives instead of Java objects
                    - Use mutable structures, for example in the case of an aggregation (foldLeft, foldRight, aggregate)
                </aside>
            </section>
            <section data-transition="none none">
                <aside class="notes">
                    One transformation we did not talk about is mapPartition. MapPartition is extremely powerful and
                    happens to be the one most other transformations are based on. Let's quickly look at this.

                    MapPartition let you implement custom transformations....however you need to worry about memory...
                    ...is designed to take an iterator and to return another one....the trap consists in evaluating
                    the input iterator usin toList, size....as soon as it is consummed, the whole partition is loaded
                    in memory...take advantage of Spark caching abilities, and let it spill records on the disk if required,
                    release the pressure on the memory.

                    can be composed...

                    iterator 2 iterator
                    what's an iterator -> goes back a function that iterates over the data.
                    -> can be composed

                    mapPartition -> allows you to prevent loading in memory and let Spark take advantage of the cache

                    rdd.mapPartitions((it: Iterator[A]) => it.map(f))

                    import scala.collection.mutable.ArrayBuffer

                    rdd.mapPartitions((it: Iterator[Int]) => {
                      val xs: ArrayBuffer[Int] = ArrayBuffer.empty[Int]
                      for(i <- it) { xs += i }
                      xs.iterator
                    })

                </aside>
            </section>
            <section data-transition="none none">
                <aside class="notes">BCV/AC</aside>
                actions like collect, and things returning to the driver?
            </section>
            <section data-transition="none none">
                <aside class="notes">cache</aside>
            </section>
        </section>

        <section data-transition="none none">
            <h2>Efficient Partitioning</h2>
            <section data-transition="none none">
                <aside class="notes">
                    partitioner,
                </aside>
            </section>
        </section>

        <section data-transition="none none">
            <h2>Efficient Shuffling</h2>
            <section data-transition="none none">
                <aside class="notes">Filter data + some other tricks</aside>
            </section>
            <section data-transition="none none">
                <aside class="notes">Reduce key space</aside>
            </section>
            <section data-transition="none none">
                <aside class="notes">co/location/partitioner</aside>
            </section>
        </section>




        <section data-transition="none none">
            <h2>Conclusion</h2>
        </section>


        <!-- ####################################################################################################### -->
        <!-- ####################################################################################################### -->
        <!-- ####################################################################################################### -->

        <section>
            <h2>Spark execution plan</h2>
            <section data-transition="none">
                <aside class="notes">
                    Now that you have a <b>basic understanding</b> of how an RDD works, we can get a little bit more
                    into details and see how it is <b>computed</b> by Spark<br/><br/>

                    <!--So, once again an RDD is <b>lazily evaluated</b>, and starts being computed when an action is called on it.-->
                    <!--Secondly it is represented as a <b>directed acyclic graph</b> which is a <b>logical representation</b> -->
                    <!--of what needs to be computed in order to get a result. Now, this <b>graph</b> is actually translated by -->
                    <!--Spark into a <b>physical execution plan</b>.<br/><br/>-->

                    So, whenever an action is called on an RDD, Spark translates the lineage of the RDD to a
                    execution plan. First it traverses the whole graph starting by the last transformation requested
                    and goes backward until the creation of the RDD. Once this done, Spark divides the work to be done
                    into one to multiples stages depending on the nature of the transformations required to obtain
                    the final result.<br/>

                    A stage is basically a set of <b>one to many</b> transformations which do not require any <b>data
                    movement</b> or <b>shuffling</b>. By shuffling, I mean re-grouping the data in a different way
                    among the partitions.

                    In the wordcount example, the first set of transformation can be pipelined, and performed in
                    parallel on each partition without changing the way the data is distributed. This kind of
                    transformation is actually called narrow transformations.

                    However, once we get here, all the tuples having the same key have to be stored in the same
                    partition in order to be counted. As some of those tuples may be stored in different partitions,
                    a shuffle is required. Concretely, this consists in writing the data on disk and in sending it
                    over the network in order to be redistributed. Every time we get in this situation, a new Stage is
                    created. Transformations involving data movement are referred to as wide-transformations.

                    Concretely a Stage is an optimization performed by Spark. Whenever one is created, its narrow
                    transformations are actually composed with each other in order to produce one single function. So
                    a Stage is a super operation in which transformations have been fused together in order to
                    prevent multiple reading of the same data and to avoid the overhead of each operation if they
                    were executed one after the other. Alright so this is the overall concept of Stages.

                    What's next? Well now, Spark has to perform those stages on the dataset. The operation consisting
                    in applying a Stage on a partition is called a Task. Given a stage, there will be as many tasks
                    executed as there are partitions available when the stage is executed. The reason I am saying that
                    is because the number of partitions may change between each Stage depending on the
                    transformations done, but we'll come back to this later. Overall, just keep in mind that given a
                    stage and a set of partitions, the number of tasks generated for this stage is equal to the
                    number of partitions.
                </aside>
                <div style="display:block;height:493px;width:100%;text-align:center">
                    <div class="fragment border" data-fragment-index="1"
                         style="padding:10px;text-align:center;vertical-align:top;display:inline-block;font-size:35px; width:42%;">
                        <div style="border:0 solid white;padding:10px;">textFile</div>
                        <div class="down"><div class="arrow"></div></div>
                        <div style="border:0 solid white;padding:10px;">flatMap</div>
                        <div class="down"><div class="arrow"></div></div>
                        <div style="border:0 solid white;padding:10px;">map<br/>&nbsp;</div>
                    </div>
                    <!-- <div class="right fragment fade-in" data-fragment-index="2" style="margin-left:10px;vertical-align:middle;"> -->
                        <!-- <div class="arrow" class="orange"></div> -->
                    <!-- </div> -->

                    <!--<span class="fragment" style="color: #a7d196;margin-left:10px;" data-fragment-index="1">Stage 1</span>-->

                    <!-- <div class="fragment flip sparkTask mainTask" data-fragment-index="2"> -->
                        <!-- <div class="sparkTask"><div class="sparkTask"></div></div> -->
                    <!-- </div> -->

                    <div style="font-size:35px;margin-top:10px;text-align:left;width:100%;">
                        <div style="text-align:center;display:inline-block;width:100%;">
                            <div class="down-l" style=""><div class="arrow"></div></div>
                            <div class="down" style="margin:0 10px 0 10px;"><div class="arrow"></div></div>
                            <div class="down-r"><div class="arrow"></div></div>
                        </div>
                    </div>
                    <div class="fragment border" data-fragment-index="1"
                         style="text-align:center;vertical-align:top;display:inline-block;font-size:35px;
                         width:42%;padding:10px;">
                        <div style="border:0 solid white;padding:10px;">reduceByKey</div>
                    </div>
                    <!-- <div class="right fragment fade-in" data-fragment-index="2"style="margin-left:10px;vertical-align:middle;"> -->
                        <!-- <div class="arrow" class="orange"></div> -->
                    <!-- </div> -->

                    <!--<span class="fragment" style="color: #a7d196;margin-left:10px;" data-fragment-index="1">Stage 2</span>-->

                    <!-- <div class="fragment flip sparkTask mainTask" data-fragment-index="2"> -->
                        <!-- <div class="sparkTask"><div class="sparkTask"></div></div> -->
                    <!-- </div> -->
                </div>
            </section>
            <section data-transition="none">
                <aside class="notes">


                </aside>
                <div style="display:block;height:493px;width:100%;text-align:center">
                    <!--<div style="text-align:center;vertical-align:top;display:inline-block;font-size:35px; padding:10px; width:25%;border: 2px solid #a7d196;">-->
                            <!--<div style="padding:10px;">textFile</div>-->
                            <!--<div class="down" style="visibility:hidden;"><div class="arrow"></div></div>-->
                            <!--<div style="padding:10px;">flatMap</div>-->
                            <!--<div class="down" style="visibility:hidden;"><div class="arrow"></div></div>-->
                            <!--<div style="padding:10px;">map<br/>&nbsp;</div>-->
                        <!--</div>-->

                    <!--<div class="fragment border" data-fragment-index="1"-->
                         <!--style="text-align:center;vertical-align:top;display:inline-block;font-size:35px; padding:-->
                         <!--10px;height:100%;">-->
                    <!--<span style="color: #a7d196;" data-fragment-index="1">Stage 1</span>-->
                    <!--<span class="fragment flip flipV" style="color: #a7d196" data-fragment-index="1">Stage 2</span>-->
                    <!--</div>-->

                    <div style="display:inline-block;width:25%;">
                        <span style="color: #a7d196;vertical-align: middle;">Stage 1</span>
                    </div>
                    <div class="green-border" data-fragment-index="1"
                         style="text-align:center;vertical-align:top;display:inline-block;font-size:35px; padding:
                         10px;width:42%;">

                        <div style="display:inline-block;">
                            <div style="padding:10px;">"a, b"</div>
                            <div class="down"><div class="arrow"></div></div>
                            <div style="padding:10px;">
                                <span style="color: #A3BCFF; ">"a"</span>,
                                <span style="color: #01B623; ">"b"</span>
                            </div>
                            <div class="down"><div class="arrow"></div></div>
                            <div style="padding:10px;">
                                (<span style="color: #A3BCFF; ">"a"</span>, 1)<br/>
                                (<span style="color: #01B623; ">"b"</span>, 1)
                            </div>
                        </div>
                        <div style="display:inline-block;">
                            <div style="padding:10px;">"a, c"</div>
                            <div class="down"><div class="arrow"></div></div>
                            <div style="padding:10px;">
                                <span style="color: #A3BCFF; ">"a"</span>,
                                <span style="color: #FF0905; ">"c"</span>
                            </div>
                            <div class="down"><div class="arrow"></div></div>
                            <div style="padding:10px;">
                                (<span style="color: #A3BCFF; ">"a"</span>, 1)<br/>
                                (<span style="color: #FF0905; ">"c"</span>, 1)
                            </div>
                        </div>
                        <div style="display:inline-block;">
                            <div style="padding:10px">"a, b"</div>
                            <div class="down"><div class="arrow"></div></div>
                            <div style="padding:10px;">
                                <span style="color: #A3BCFF; ">"a"</span>,
                                <span style="color: #01B623; ">"b"</span>
                            </div>
                            <div class="down"><div class="arrow"></div></div>
                            <div style="padding:10px;">
                                (<span style="color: #A3BCFF; ">"a"</span>, 1)<br/>
                                (<span style="color: #01B623; ">"b"</span>, 1)
                            </div>
                        </div>
                    </div>
                    <!--<div style="display:inline-block;width:25%;margin-top: 15%;">-->
                        <!--<span style="color: #a7d196;vertical-align: middle;">Stage 1</span>-->
                    <!--</div>-->
                    <div class="fragment" data-fragment-index="1" style="display:inline-block;width:25%;margin-top:
                    10%;">
                        <div class="right" style="vertical-align: middle"><div class="arrow orange"></div></div>
                        <div class="sparkTask mainTask">
                            <div class="sparkTask"><div class="sparkTask"></div></div>
                        </div>
                    </div>
                    <!--<div class="right fragment fade-in" data-fragment-index="2" style="margin-left:10px;vertical-align:middle;">-->
                        <!--<div class="arrow" class="orange"></div>-->
                    <!--</div>-->
                    <!--&lt;!&ndash;<span class="fragment flip flipV" style="color: #a7d196" data-fragment-index="1">Stage 1</span>&ndash;&gt;-->
                    <!--<div class="fragment flip sparkTask mainTask" data-fragment-index="2">-->
                        <!--<div class="sparkTask"><div class="sparkTask"></div></div>-->
                    <!--</div>-->


                                        <!--<div style="font-size:35px;margin-top:10px;text-align:left;width:100%;">-->
                        <!--<div style="text-align:center;display:inline-block;width:100%;">-->
                            <!--<div class="down-l" style=""><div class="arrow"></div></div>-->
                            <!--<div class="down" style="margin:0 10px 0 10px;"><div class="arrow"></div></div>-->
                            <!--<div class="down-r"><div class="arrow"></div></div>-->
                        <!--</div>-->
                    <!--</div>-->




                    <div style="font-size:35px;margin-top:10px;text-align:left;width:100%;">
                    <!-- <div style="font-size:35px;margin-top:10px;text-align:left;"> -->
                        <!-- <div style="display:inline-block;margin-right:40px;"> -->
                            <!-- <div class="down-l" style="margin-left:0px;"><div class="arrow"></div></div> -->
                            <!-- <div class="down" style="margin:0 10px 0 10px;"><div class="arrow"></div></div> -->
                            <!-- <div class="down-r"><div class="arrow"></div></div> -->
                        <!-- </div> -->
                        <div style="text-align:center;display:inline-block;width:100%;">
                            <div class="down"><div class="arrow"></div></div>
                            <div class="down-r"><div class="arrow"></div></div>
                            <div class="down-l" style="margin:0 5px 0 37px;"><div class="arrow"></div></div>
                            <div class="down-r" style="margin:0 37px 0 5px;"><div class="arrow"></div></div>
                            <div class="down-l"><div class="arrow"></div></div>
                            <div class="down"><div class="arrow"></div></div>
                        </div>
                    </div>
                    <!--<div style="border: 2px solid #a7d196;text-align:center;vertical-align:top;display:inline-block;font-size:35px; padding: 10px;width:25%;">-->
                        <!--<div style="display:inline-block;vertical-align:top;">-->
                            <!--<div style="padding:10px;width:40%;">reduceByKey</div>-->
                        <!--</div>-->
                    <!--</div>-->
                    <!--<span style="color: #a7d196" data-fragment-index="1">Stage 1</span>-->
                    <div style="display:inline-block;width:25%">
                        <span style="color: #a7d196">Stage 2</span>
                    </div>
                    <div class="green-border" data-fragment-index="1" style="text-align:center;font-size:35px;padding: 10px;display:inline-block;width:42%;">
                                            <div style="display:inline-block;">
                            <div style="padding: 10px;">(<span style="color: #A3BCFF; ">"a"</span>, 3)<br/></div>
                        </div>
                        <div style="display:inline-block;">
                            <div style="padding:10px;">(<span style="color: #01B623; ">"b"</span>, 2)</div>
                        </div>
                        <div style="display:inline-block;">
                            <div style="padding:10px;">(<span style="color: #FF0905; ">"c"</span>, 1)</div>
                        </div>
                    </div>
                    <div class="fragment" data-fragment-index="1" style="display:inline-block;width:25%;vertical-align: top">
                        <div class="right" style="vertical-align: middle"><div class="arrow orange"></div></div>
                    <!--&lt;!&ndash;<span class="fragment flip flipV" style="color: #a7d196" data-fragment-index="1">Stage 2</span>&ndash;&gt;-->
                        <div class="sparkTask mainTask">
                            <div class="sparkTask"><div class="sparkTask"></div></div>
                        </div>
                    <!-- </div> -->
                    </div>

                    <!--<div class="right fragment fade-in" data-fragment-index="2"style="margin-left:10px;vertical-align:middle;">-->
                        <!--<div class="arrow" class="orange"></div>-->
                    <!--</div>-->
                    <!--&lt;!&ndash;<span class="fragment flip flipV" style="color: #a7d196" data-fragment-index="1">Stage 2</span>&ndash;&gt;-->
                    <!--<div class="fragment flip sparkTask mainTask" data-fragment-index="2">-->
                        <!--<div class="sparkTask"><div class="sparkTask"></div></div>-->
                    <!--</div>-->
                    <!-- </div> -->
                </div>
            </section>

            <!--<section data-transition="none none">-->
                <!--<aside class="notes">-->
                    <!--A stage is basically a set of <b>one to many</b> transformations which do not require any <b>data-->
                    <!--movement</b> or <b>shuffling</b>. By shuffling, I mean the operation consisting in re-grouping the data-->
                    <!--in a different way.-->
                <!---->
                <!---->
                <!--and which can be therefore <b>pipelined</b>. It's some kind of a <b>super-operation</b>-->
                    <!--in which <b>transformations</b> have been <b>fused</b> together in order to prevent multiple reading of the same data-->
                    <!--and to avoid the <b>overhead</b> of each operation if they were <b>executed</b> one after the other.-->

                    <!--In other words, as long as no data needs to be moved from one partition to another, operations are performed during the-->
                    <!--same stage. But this is not always the case. Sometime the data needs to be grouped in a different way and therefore-->
                    <!--to be re-shuffled among the cluster's partitions. The word count is a good example. In order to figure out how many -->
                    <!--occurrences of the same word there is in a set of document, you actually need all the occurrences of that word to be in -->
                    <!--the same partition.-->

                    <!--Therefore you end up moving the data from one set of bucket to another but grouped in a diff, or using a more Spark-friendly terminology, from-->
                    <!--one set of partitions to another. And whenever this happens, you actually create a new stage as shown here.-->
                <!--</aside>-->
                <!--<img style="margin-top:100px;margin-left:50px; box-shadow: 0; background-color: #FFFFFF; width:100%"-->
                     <!--src="images/spark_scala_meetup/execution_plan_2.png"/>-->
            <!--</section>-->

            <!--<section data-transition="none none">-->
                <!--<aside class="notes">-->
                    <!--Now each Stage is actually performed as many time as there are partitions. So if you have <b>10 partitions</b> given a stage, -->
                    <!--the scheduler will create <b>10 tasks</b> to perform it. As you may guess, <b>the more task</b> a Spark job needs to execute, -->
                    <!--<b>the longer</b> it will take to compute the final result.-->
                <!--</aside>-->
                <!--<img style="margin-top:100px;margin-left:50px; box-shadow: 0; background-color: #FFFFFF; width:100%"-->
                     <!--src="images/spark_scala_meetup/execution_plan_1.png"/>-->
            <!--</section>-->



            <!-- -> Partitioning (repartitions + partitions)-->
            <!-- -> Controlling partitioning + Shuffling + Strategies-->
            <!--https://www.youtube.com/watch?v=w0Tisli7zn4

            What are the problems we mentioned so far ?
            - Iterative process on the same data -> Cache
            - Shuffling: How we can prevent that or optimize it when we need to do it?
            - Partitioning: How can we optimize the way the data is spread over the cluster? and therefore minimize the number of tasks to create
              and optimize the

            -->
        </section>

        <section data-transition="none none" data-background="white">
            <!--<h2 style="color: black">Three areas of optmization</h2>-->
            <aside class="notes">
                This is Spark in a Nutshell. Overall, this should give you an idea about what needs to be
                optimized whenever you create a Spark job. First, as the number of partitions defines the number
                of tasks performed while executing a Stage, thinking about how the data is distributed is the
                first thing you should think about. Then, each transformation can have a significant cost.
                Therefore it's important to understand what happens under the hood whenever you use them.
                Finally, shuffling should be avoided or at least done efficiently, as it can be extremely
                resource expensive.

                In the next section, we'll talk about what is provided by the Spark API in order to optimize all
                of these aspects. Any question so far?
            </aside>
            <div style="text-align: center; width:100%;color: black">
                <h2 style="color: black">Partitioning</h2>
                <!--<ul style="margin-left: 50px;">-->
                    <!--<li>Partitioning</li>-->
                    <!--<li>Transformations</li>-->
                    <!--<li>Shuffling</li>-->
                <!--</ul>-->
            </div>
            </section>

        <section data-transition="none none">
            <h2>Partitioning</h2>
            <aside class="notes">
                We've been talking a lot about partitions since the beginning of this talk. This is because properly
                partitioning a cluster's data is essential not only to achieve good performances but also just to
                complete a Spark job.

                Whenever you load data from a datasource, Spark, by default will distribute it in a certain amount of
                logical partitions across the cluster's executors as evenly as possible. By default, the total number of
                partitions is equal to the number of cores currently available but this can change depending on the
                datasource. For example, in the case of HDFS, Spark will create a single partition for each input split.
                You can also configure this parameter either in Spark's configuration or by providing a number to the
                datasource loading function. A partition never span on multiple machine, and one machine can contain
                one to multiple partitions.




                # show some code with numPartitions, data-loading, etc, evenly spread
                # forEachPartition / mapPartition
<!--n Spark will operate on one partition at a time (and load the data in that partition into memory)-->
                ######################################

                Once loaded, an RDD can be repartitioned:
                - coalesce / repartition
                - why would you even do that?
                => memory, IO, parallelism

                ######################################

                - Customize the partitioner.
                - so far, we said evenly spread (except for coalesce), but given a pair RDD, you can have a complete
                control of how data is distributed using partitioners
                - hashpartitioner / rangepartitioner
                - explain the bucket thing with k.hashcode % numPartitions
                - Customizing is only possible on Pair RDDs

                ######################################

                Whenever a task is performed on a partition, the data is loaded from the disk, or from the memory in
                the case of a simple sequence, and is stored into the executor's memory. So first of all, if the
                partitions are too big, you may end up with out of memory exceptions preventing the job from being
                completed. On the other hand, if the dataset is split into too many partitions, you may end up with a
                lot of IO which would slow down the job execution. Finally, as the number of partitions defines the
                number of tasks needed to be processed, and because one core can only process one task at a given
                time, it's essential to figure out the right number of partitions in order to optimize the cluster's
                resources utilization. You basically don't want to end with only half of your cluster being used.

                under-utilize

                So for all of theses reasons, caring about partitions is critical. Now how can we optimize this?
                - coalesce
                - repartition
                - partitioner

                impact. partitioner, repartition/coalesce, reduce memory pressure
                explain how a data is sent to a bucket (hash partitioning)
                - range partitioning (keys with ordering)

                p = k.hashcode % numPartition
                example of p[0], p[1] => wrong distribution
                data skewing
                -> range partitioning

                - Rules to decide about the right amount of partitions
                - Repartition/Coalesce when required
                - Use a partitioner, set the number of partitions among which the data is spread when loaded.
            </aside>
            <section data-transition="none none">
                <div style="display:inline-block; padding: 0 10px 0 0;vertical-align:top;margin: 20px 0 0 0;text-align:
                left; font-size:34px">
                        sc.parallelize<br/>sc.textFile<br/>sc.sequenceFiles[K,V]
                </div>
                <div class="right" style="margin: 0 0 50px 0;display:inline-block;"><div class="arrow"></div></div>
                <div style="display:inline-block;border-right: 2px solid white;padding-right: 35px;">
                    <div class="partition-block-set" style="margin:0 0 10px 10px">
                        <div class="partition-block red">&nbsp;</div>
                        <div class="partition-block red" style="opacity: 0.7;">&nbsp;</div>
                        <div class="partition-block red" style="opacity: 0.2;">&nbsp;</div>
                    </div>
                    <div class="partition-block-set" style="margin:0 0 10px 10px">
                        <div class="partition-block green">&nbsp;</div>
                        <div class="partition-block green" style="opacity: 0.7;">&nbsp;</div>
                        <div class="partition-block green" style="opacity: 0.2;">&nbsp;</div>
                    </div>
                    <div class="partition-block-set" style="margin:0 0 0 10px; text-align: left">
                        <div class="partition-block yellow">&nbsp;</div>
                        <div class="partition-block yellow" style="opacity: 0.2;">&nbsp;</div>
                    </div>
                </div>
                <div style="display:inline-block;padding-left: 10px;">
                    <ul>
                        <li>Evenly spread</li>
                        <li>Configurable</li>
                        <li>Do not span across nodes</li>
                    </ul>
                </div>
                <div style="width:100%;margin-top:100px;background-color:#3F3F3F;color: #DCDCDC">
                    rdd.getNumPartitions
                </div>
            </section>
            <section data-transition="none none">
                <div class="partitioning-title">coalesce</div>
                <div style="display:inline-block;padding-right: 20px;vertical-align: top">
                    <div class="partition" style="margin:0 0 10px 10px;"><span>1, 2</span></div>
                    <div class="partition" style="margin:0 0 10px 10px;"><span>3</span></div>
                    <div class="partition" style="margin:0 0 10px 10px;"><span>4</span></div>
                </div>
                <div style="display:inline-block;width:5%">
                    <div class="down-r" style="margin: 0 0 0 0;"><div class="arrow"></div></div><br/>
                    <div class="right" style="margin: 0 0 0 0;"><div class="arrow"></div></div><br/>
                    <div class="right" style="margin: 0 0 0 0;"><div class="arrow"></div></div>
                </div>
                <div style="display:inline-block;border-right: 2px solid white;padding-right: 35px;">
                    <div class="partition hidden" style="margin:0 0 10px 10px;"><span>&nbsp;</span></div>
                    <div class="partition" style="margin:0 0 10px 10px;"><span>1, 2, 3</span></div>
                    <div class="partition" style="margin:0 0 10px 10px;"><span>4</span></div>
                </div>
                <div class="partitioning-pros">
                    <ul>
                        <li>Not evenly spread</li>
                        <li>No shuffle</li>
                        <li>Narrow transformation</li>
                    </ul>
                </div>
                <div style="width:100%;margin-top:50px;"></div>
                <div class="partitioning-title">repartition</div>
                <div style="display:inline-block;padding-right: 20px;vertical-align: top">
                    <div class="partition" style="margin:0 0 10px 10px;"><span>1, 2, 3</span></div>
                    <div class="partition" style="margin:0 0 10px 10px;"><span>4, 5</span></div>
                </div>
                <div style="display:inline-block;width:5%;margin-top:15px;">
                    <div class="up-r"   style="margin: 0 0 0 0;"><div class="arrow"></div></div><br/>
                    <div class="right"  style="margin: 0 0 0 0;"><div class="arrow"></div></div><br/>
                    <div class="down-r" style="margin: 0 0 0 0;"><div class="arrow"></div></div>
                </div>
                <div style="display:inline-block;border-right: 2px solid white;padding-right: 35px;vertical-align: top">
                    <div class="partition" style="margin:0 0 10px 10px;"><span>1, 3</span></div>
                    <div class="partition" style="margin:0 0 10px 10px;"><span>2, 4</span></div>
                    <div class="partition" style="margin:0 0 10px 10px;"><span>5</span></div>
                </div>
                <div class="partitioning-pros">
                    <ul>
                        <li>Evenly spread</li>
                        <li>Full shuffle</li>
                        <li>Wide transformation</li>
                    </ul>
                </div>
            </section>
            <section data-transition="none none">
                <div style="display:inline-block; vertical-align: top">
                    <div class="partition" style="margin:0 0 10px 0;"><span>(3, c)</span></div>
                    <div class="partition" style="margin:0 0 10px 0;"><span>(1, a)</span></div>
                    <div class="partition" style="margin:0 0 10px 0;"><span>(2, b)</span></div>
                </div>
                <div style="display:inline-block; width:5%;margin-top:15px;">
                    <div class="down-r" style="margin: 0 0 0 0;"><div class="arrow"></div></div><br/>
                    <div class="right"  style="margin: 0 0 0 0;"><div class="arrow"></div></div><br/>
                    <div class="up-r"   style="margin: 0 0 0 0;"><div class="arrow"></div></div>
                </div>
                <div style="display:inline-block; vertical-align: middle;margin-bottom:100px;">
                    <div class="partition" style="margin:0"><span>f(k)</span></div>
                </div>
                <div style="display:inline-block; width:5%;margin-top:15px;">
                    <div class="up-r"   style="margin: 0 0 0 0;"><div class="arrow"></div></div><br/>
                    <div class="right"  style="margin: 0 0 0 0;"><div class="arrow"></div></div><br/>
                    <div class="down-r" style="margin: 0 0 0 0;"><div class="arrow"></div></div>
                </div>
                <div style="display:inline-block; vertical-align: top;border-right: 2px solid white;padding-right:
                35px">
                    <div class="partition" style="margin:0 0 10px 0;"><span>(1, a)</span></div>
                    <div class="partition" style="margin:0 0 10px 0;"><span>(2, b)</span></div>
                    <div class="partition" style="margin:0 0 10px 0;"><span>(3, c)</span></div>
                </div>
                <div class="partitioning-pros">
                    <ul>
                        <li>Only possible with pairs</li>
                        <li>Complete control</li>
                        <li>Can prevent shuffling</li>
                    </ul>
                </div>
                <div style="width:100%;margin-top:50px;">Where f(k) is a function resolving the partition to wich
                    a key belongs (eg. f(k) = k % total_partitions)</div>
            </section>
            <section data-transition="none none">
                Some code with partition creations, coalesce / repartitions, num partitions, custom partitioner
            </section>
        </section>


        <!-- ############################################################## -->



        <section>
            <h2>RDD : Persistence and Caching</h2>

            <section data-transition="none none">
                <aside class="notes">
                    As we've just seen it, Spark will <b>recompute</b> an RDD <b>any time</b> when a action is being
                    performed on it.
                    This is alwyas true except if the RDD in question has been <b>cached</b>. As you may guess,
                    recomputing an RDD can quickly become a burden depending on the size of its dataset.<br/><br/>

                    ***So in order to prevent that, Spark provides caching features allowing an RDDs data to be stored
                    either
                    in <b>memory, on disk, or both of them</b>. This is especially useful, if you need to iterate over
                    an
                    RDD, like when you're training a machine learning model for example.<br/><br/>
                </aside>
                <pre><code class="scala">val result = input.map(x => x*x)
println(result.count())
// RDD will be recomputed here
println(result.collect().mkString(","))</code></pre>

                <pre class="fragment"><code class="scala">import org.apache.spark.storage.StorageLevel

val result = input.map(x => x * x)
result.persist(StorageLevel.DISK_ONLY)

println(result.count())
// RDD won' be recomputed here
println(result.collect().mkString(","))</code></pre>

                <pre style="visibility: hidden;"><code class="scala">// Use default storage level (MEMORY_ONLY)
result.cache()</code></pre>

                <pre style="visibility: hidden;"><code class="scala">result.unpersist()</code></pre>
            </section>

            <section data-transition="none none">
                <aside class="notes">
                    Persisting an RDD allows <b>future actions</b> to be much faster, actually often by more than 10
                    times.
                    In order to do it, all you need is to marked the RDD as persisted, using the persist method and to
                    specify a storage level.<br/><br/>

                    ***You can also use the <b>cache method</b>, which basically is like persist but using the default
                    storage
                    that is in memory.<br/><br/>

                    ***Finally, RDDs come with a method <b>unpersist()</b> that lets you manually remove them from the
                    cache.
                </aside>
                <pre><code class="scala">val result = input.map(x => x*x)
println(result.count())
// RDD will be recomputed here
println(result.collect().mkString(","))</code></pre>

                <pre><code class="scala">import org.apache.spark.storage.StorageLevel

val result = input.map(x => x * x)
result.persist(StorageLevel.DISK_ONLY)

println(result.count())
// RDD won' be recomputed here
println(result.collect().mkString(","))</code></pre>

            <pre class="fragment"><code class="scala">// Use default storage level (MEMORY_ONLY)
result.cache()</code></pre>

                <pre class="fragment"><code class="scala">result.unpersist()</code></pre>
            </section>

            <section data-transition="none none">
                <aside class="notes">
                    Now no matter if you decide to cache an RDD's data in the memory or in the disk, keep in mind that
                    Spark will always favor memory over disk in order to allows operations to run as fast as possible.
                    So, data will be <b>spilled</b> on the disk only if you attempt to cache <b>too much data to fit in
                    memory</b>. Secondly, Spark will evict old cached partitions using an LRU cache policy.<br/><br/>

                    Here are the <b>standard ways</b> of caching an RDD's data. As you can see each has its <b>tradeoffs</b>
                    and <b>advantages</b>. You can also <b>replicate</b> an RDD's data over <b>several nodes</b> or, if
                    the RDD contains <b>too much</b> data to fit on a <b>single node</b>, caching it on multiple nodes.

                    Overall, this means that you should not worry too much about your <b>job breaking</b> if you ask
                    Spark to cache <b>too much data</b>. <b>However</b>, keep in mind that caching <b>unnecessary</b>
                    data can lead to more <b>data evictions</b> and <b>re-computation time</b>.
                </aside>
                <table>
                    <tr>
                        <td>Level</td>
                        <td>Space</td>
                        <td>CPU</td>
                        <td>RAM</td>
                        <td>Disk</td>
                    </tr>
                    <tr>
                        <td>MEMORY_ONLY</td>
                        <td>High</td>
                        <td>Low</td>
                        <td>Y</td>
                        <td>N</td>
                    </tr>
                    <tr>
                        <td>DISK_ONLY</td>
                        <td>Low</td>
                        <td>High</td>
                        <td>N</td>
                        <td>Y</td>
                    </tr>
                    <tr>
                        <td>MEMORY_AND_DISK</td>
                        <td>High</td>
                        <td>Medium</td>
                        <td>Some</td>
                        <td>Some</td>
                    </tr>
                    <tr>
                        <td>MEMORY_ONLY_SER*</td>
                        <td>Low</td>
                        <td>High</td>
                        <td>Y</td>
                        <td>N</td>
                    </tr>
                    <tr>
                        <td>MEMORY_AND_DISK_SER*</td>
                        <td>Low</td>
                        <td>High</td>
                        <td>Some</td>
                        <td>Some</td>
                    </tr>
                    <tr>
                        <td>&nbsp;</td>
                    </tr>
                    <tr>
                        <td colspan="5">(*) : Data is serialized before being persisted (takes less space)</td>
                    </tr>
                </table>
            </section>
        </section>

        <section>
            <h2>RDD : Execution</h2>
            <section data-transition="none none">
                <aside class="notes">
                    Now let's move on, and focus a little bit on how a <b>sequence of operations</b> is processed by
                    Spark.
                    One typical <b>mistake</b> Spark beginners tend to do is to forget about the <b>distributed nature
                    of an RDD</b>.
                    Writing a Spark program may sound <b>easy</b>, but it requires to understand how <b>computations</b>
                    are <b>distributed</b>
                    over the <b>cluster</b>. In order to illustrate this, let's go back to our <b>word count</b>. Here,
                    we have an
                    <b>RDD</b> creation, a couple of <b>transformations</b>, and an <b>action</b> in the end.
                </aside>
                <div style="text-align: left;margin-top:50px;display:inline-block">
                    <ul style="margin-left: 0;margin-right:80px; list-style-type:none;white-space: nowrap;">
                        <li>val output = sparkContext</li>
                        <li style="padding-left: 100px">.textFile("hdfs://...")</li>
                        <li style="padding-left: 100px">.flatMap(_.split(" "))</li>
                        <li style="padding-left: 100px">.map((_, 1))</li>
                        <li style="padding-left: 100px">.reduceByKey(_ + _)</li>
                        <li style="padding-left: 100px">.collect()</li>
                        <li>print output</li>
                    </ul>
                </div>

                <div style="text-align: left;vertical-align: top; margin-top:60px;display:inline-block; font-size: 30px;width:460px;">
                    <div style="vertical-align: middle;width:30px;height:30px;margin-bottom:10px;background-color:#A3BCFF;display:none;"></div>
                    <span style="visibility: hidden">&nbsp;Executed on the driver</span><br/>
                    <div style="vertical-align: middle;width:30px;height:30px;margin-bottom:10px;background-color:#01B623;display:none;"></div>
                    <span style="visibility: hidden">&nbsp;Executed on the executors</span><br/>
                    <div style="vertical-align: middle;width:30px;height:30px;margin-bottom:10px;background-color:#FF0905;display:none;"></div>
                    <span style="visibility: hidden">&nbsp;May return a value to the Driver</span>
                </div>
            </section>
            <section data-transition="none none">
                <aside class="notes">
                    The lines in <b>blue</b> will be executed on the <b>driver</b> as they are part of the main program
                </aside>
                <div style="text-align: left;margin-top:50px;display:inline-block">
                    <ul style="margin-left: 0;margin-right:80px; list-style-type:none;white-space: nowrap;">
                        <li style="color:#A3BCFF;">val output = sparkContext</li>
                        <li style="padding-left: 100px">.textFile("hdfs://...")</li>
                        <li style="padding-left: 100px">.flatMap(_.split(" "))</li>
                        <li style="padding-left: 100px">.map((_, 1))</li>
                        <li style="padding-left: 100px">.reduceByKey(_ + _)</li>
                        <li style="padding-left: 100px">.collect()</li>
                        <li style="color:#A3BCFF">print output</li>
                    </ul>
                </div>

                <div style="text-align: left;vertical-align: top; margin-top:60px;display:inline-block; font-size: 30px;width:460px;">
                    <div style="vertical-align: middle;width:30px;height:30px;margin-bottom:10px;background-color:#A3BCFF;display:inline-block;"></div>
                    <span style="visibility: visible">&nbsp;Executed on the driver</span><br/>
                    <div style="vertical-align: middle;width:30px;height:30px;margin-bottom:10px;background-color:#01B623;display:none;"></div>
                    <span style="visibility: hidden">&nbsp;Executed on the executors</span><br/>
                    <div style="vertical-align: middle;width:30px;height:30px;margin-bottom:10px;background-color:#FF0905;display:none;"></div>
                    <span style="visibility: hidden">&nbsp;May return a value to the Driver</span>
                </div>
            </section>
            <section data-transition="none none">
                <aside class="notes">
                    On the other hand, <b>creation and transformations</b>, that is the <b>textFile</b>, <b>flatMap</b>,
                    <b>map</b> and <b>reduceByKey</b> will be
                    executed on the worker nodes by the <b>executors</b>.
                </aside>

                <div style="text-align: left;margin-top:50px;display:inline-block">
                    <ul style="margin-left: 0;margin-right:80px; list-style-type:none;white-space: nowrap;">
                        <li style="color:#A3BCFF;">val output = sparkContext</li>
                        <li style="color:#01B623; padding-left: 100px">.textFile("hdfs://...")</li>
                        <li style="color:#01B623; padding-left: 100px">.flatMap(_.split(" "))</li>
                        <li style="color:#01B623; padding-left: 100px">.map((_, 1))</li>
                        <li style="color:#01B623; padding-left: 100px">.reduceByKey(_ + _)</li>
                        <li style="padding-left: 100px">.collect()</li>
                        <li style="color:#A3BCFF">print output</li>
                    </ul>
                </div>

                <div style="text-align: left;vertical-align: top; margin-top:60px;display:inline-block; font-size: 30px;width:460px;">
                    <div style="vertical-align: middle;width:30px;height:30px;margin-bottom:10px;background-color:#A3BCFF;display:inline-block;"></div>
                    <span style="visibility: visible">&nbsp;Executed on the driver</span><br/>
                    <div style="vertical-align: middle;width:30px;height:30px;margin-bottom:10px;background-color:#01B623;display:inline-block;"></div>
                    <span style="visibility: visible">&nbsp;Executed on the executors</span><br/>
                    <div style="vertical-align: middle;width:30px;height:30px;margin-bottom:10px;background-color:#FF0905;display:none;"></div>
                    <span style="visibility: hidden">&nbsp;May return a value to the Driver</span>
                </div>
            </section>
            <section data-transition="none none">
                <aside class="notes">
                    Finally, the code in <b>red</b>, that is the <b>collect</b>, is an <b>action</b>, and actions may
                    <b>transfer data</b> from the
                    workers to the Driver. Some <b>actions</b> actually don't do this, and can instead <b>persist</b>
                    the result on
                    an <b>external</b> storage. Let's see now, what happens when you perform a <b>collect</b> action.
                </aside>

                <div style="text-align: left;margin-top:50px;display:inline-block">
                    <ul style="margin-left: 0;margin-right:80px; list-style-type:none;white-space: nowrap;">
                        <li style="color:#A3BCFF;">val output = sparkContext</li>
                        <li style="color:#01B623; padding-left: 100px">.textFile("hdfs://...")</li>
                        <li style="color:#01B623; padding-left: 100px">.flatMap(_.split(" "))</li>
                        <li style="color:#01B623; padding-left: 100px">.map((_, 1))</li>
                        <li style="color:#01B623; padding-left: 100px">.reduceByKey(_ + _)</li>
                        <li style="color:#FF0905; padding-left: 100px">.collect()</li>
                        <li style="color:#A3BCFF">print output</li>
                    </ul>
                </div>

                <div style="text-align: left;vertical-align: top; margin-top:60px;display:inline-block; font-size: 30px; width:460px;">
                    <div style="vertical-align: middle;width:30px;height:30px;margin-bottom:10px;background-color:#A3BCFF;display:inline-block;"></div>
                    <span style="visibility: visible">&nbsp;Executed on the driver</span><br/>
                    <div style="vertical-align: middle;width:30px;height:30px;margin-bottom:10px;background-color:#01B623;display:inline-block;"></div>
                    <span style="visibility: visible">&nbsp;Executed on the executors</span><br/>
                    <div style="vertical-align: middle;width:30px;height:30px;margin-bottom:10px;background-color:#FF0905;display:inline-block;"></div>
                    <span style="visibility: visible">&nbsp;May return a value to the Driver</span>
                </div>
            </section>

            <section data-transition="none none">
                <aside class="notes">
                    So here we have our <b>workers</b>, each responsible for a <b>partition</b>.
                </aside>
                <div style="display: block; text-align: center; margin-left:50px"><img
                        style="border: 0; background-color: #FFFFFF; width:80%" src="images/spark_scala_meetup/collect_1.png"/></div>
                &nbsp;
            </section>
            <section data-transition="none none">
                <aside class="notes">
                    When we call collect, we are actually <b>telling Spark to collect and send all </b> the data from each
                    partition, to the driver. With small dataset, this shouldn't be a problem. <b>However</b>, as Spark
                    is designed to work with <b>datasets that are too big</b> to fit in the memory of a <b>single machine</b>,
                    collecting all the data of a cluster and putting it on the driver may trigger an <b>OutOfMemory</b>
                    exception.<br/><br/>

                    ***So a general rule of thumb is to avoid using <b>collect</b>, or similar actions, if your data is not
                    <b>bounded</b>. If this is not possible, consider storing it on an <b>external</b> storage.
                </aside>
                <div style="display: block; text-align: center; margin-left:50px"><img
                        style="border: 0; background-color: #FFFFFF; width:80%" src="images/spark_scala_meetup/collect_2.png"/></div>
                <span class="fragment">Don't use collect() on large RDDs !</span>
            </section>

            <section data-transition="none none">
                <aside class="notes">
                    One other common mistake is to write <b>code</b> such as this one. Anyone has an idea why this could
                    cause
                    a problem ?<br/><br/>Well, as we've seen it, Spark's unified API <b>abstracts</b> the fact that your
                    code may be
                    <b>executed</b> by the <b>driver</b> or not. So depending on how you <b>structure</b> your <b>transformations</b>
                    and <b>actions</b>,
                    some <b>operations</b> may cause an <b>overhead</b> or <b>behave</b> in an unexpected way.
                </aside>
                <ul style="list-style-type:none;padding-right: 30px">
                    <li>var counter = 0</li>
                    <li>var rdd = sc.parallelize(data)</li>
                    <li>&nbsp;</li>
                    <li>rdd.foreach { rddItem => counter += 1 }</li>
                    <li>print("Counter value: " + counter)</li>
                </ul>
            </section>

            <section data-transition="none none">
                <aside class="notes">
                    In this example, the <b>closure</b> inside the foreach function refers to the counter variable. In
                    order to
                    <b>distribute</b> this operation, Spark has to <b>serialize it</b> along with any variable it refers
                    to (so, in our
                    case, the <b>counter</b> variable).<br/><br/>

                    During this <b>serialization process</b>, the counter variable will be <b>copied</b> and sent along
                    with the <b>closure</b>.
                    So at some point, the <b>counter</b> variable used by the driver and its executors <b>are no longer
                    the same</b>.<br/><br/>

                    So, the <b>final value</b> of counter will always be <b>zero</b> since the we are
                    <b>incrementing</b> it in the foreach action.
                    Still, you may wonder how we can <b>achieve</b> this, well Spark provides a feature called <b>accumulators</b>
                    which is exactly <b>designed</b> for this purpose, but we'll come back to it in a few seconds.
                </aside>
                <ul style="list-style-type:none;padding-right: 30px">
                    <li style="background-color:#0029FF;">var counter = 0</li>
                    <li>var rdd = sc.parallelize(data)</li>
                    <li>&nbsp;</li>
                    <li style="background-color:#0029FF;">rdd.foreach { rddItem => counter += 1 }</li>
                    <li>print("Counter value: " + counter)</li>
                </ul>
            </section>
        </section>

        <section>
            <h2>RDD : Recap</h2>
            <section data-transition="none none">
                <aside class="notes">
                    Let's make a quick recap of what we've learnt so far.<br/><br/>

                    *** A Spark application is <b>composed</b> of a driver and its executors.<br/><br/>

                    *** Its core fundamental is the RDD or <b>Resilient Distributed Dataset</b>. An RDD is basically
                    an immutable collection distributed over a cluster nodes.<br/><br/>

                    *** An RDD supports <b>2 types of operation</b> :
                    Transformations and Actions. Transformations are <b>lazily evaluated</b> and compose together a
                    sequence of
                    operation which is played only when an action is performed.
                </aside>
                <ul>
                    <li class="fragment">Spark = driver + executors</li>
                    <li class="fragment">RDD is an immutable distributed collection</li>
                    <li class="fragment">An RDD supports Transformations and Actions</li>
                    <li style="visibility: hidden">An RDD's lineage is a sequence of Transformation</li>
                    <li style="visibility: hidden">An RDD can be persisted for caching purpose</li>
                    <li style="visibility: hidden">Watch out for an RDD's execution (Driver vs. Executors)</li>
                </ul>
            </section>

            <section data-transition="none none">
                <aside class="notes">
                    This sequence is called the <b>lineage</b> of an RDD, and allows its recomputation whenever its data
                    is not available.<br/><br/>

                    **** In order to <b>prevent uneccessary recomputations</b>, we can persist an RDD in <b>memory</b>,
                    in <b>disk</b> or
                    <b>both</b> depending on what we want to achieve and what resources are available.<br/><br/>

                    *** Finally, it is important to understand how a sequence of operations is <b>distributed</b>, as
                    some parts of it may
                    be executed on the driver while others may be performed on the executors.
                </aside>
                <ul>
                    <li>Spark = driver + executors</li>
                    <li>RDD is an immutable distributed collection</li>
                    <li>An RDD supports Transformations and Actions</li>
                    <!--<li>A Sequence of Transformations composes an RDD's lineage</li>-->
                    <li>An RDD's lineage is a sequence of ransformation</li>
                    <li class="fragment">An RDD can be persisted for caching purpose</li>
                    <li class="fragment">Watch out for an RDD's execution (Driver vs. Executors)</li>
                </ul>
            </section>
        </section>

        <section>
            <h2>Shared variables</h2>
            <section data-transition="none none">
                <aside class="notes">
                    Now let's go back to the <b>previous example</b>. The whole point of this code is to <b>share</b> a
                    variable between
                    the <b>Driver</b> and its <b>executor</b>. However, and as we've seen it, this cannot work fo the
                    reasons we've
                    mentioned earlier. But actually, it is even <b>worse</b> than that. <br/><!--*** Actually, the serialization process
                    we've talked about earlier is done <b>as many time as</b> there are partitions. So if your cluster contains
                    10 nodes with a hundred partitions each, this closure will be serialized and sent through the
                    network a thousand times.<br/> Now if the counter variable is actually a <b>huge object</b>, this may get very
                    <b>inefficient</b> at some point. So the whole problem here is not only about <b>sharing a variable</b>, but also
                    to do it while <b>minimizing IOs</b> and in a <b>safe way</b> in terms of concurrency.<br/>
                    ****
                    In order to resolve this problem, Sparks supports <b>2 types of shared variables</b> : Accumulators and
                    Broadcast variables. The first is designed to send a value from the executors to their driver,
                    while the second one is for the other way around. Now of course, there are some <b>subtleties</b>, but
                    this is the main idea. So let's start with accumulators.-->
                </aside>
                <div style="text-align: left">
                    <pre style="width:100%;"><code class="scala" style="font-size: 25px;">var counter = 0
var rdd = sc.parallelize(data)

rdd.foreach { rddItem => counter += 1 }
print("Counter value: " + counter)</code></pre>

                    <pre style="width:100%;visibility: hidden"><code class="scala" style="font-size: 25px;">val hugeArray = ...
var bigRddWithIndex = ...

bigRddWithIndex.map { rddItem => hugeArray[rddItem.key] }
...</code></pre>
                </div>
                <p style="visibility: hidden">Accumulators and Broadcast variables !</p>
            </section>

            <section data-transition="none none">
                <aside class="notes">
                    Actually, the <b>serialization process</b> we've talked about earlier is done <b>as many time as</b>
                    there
                    are partitions. So if your <b>cluster</b> contains <b>10 nodes</b> with a <b>hundred partitions</b>
                    each, this closure
                    will be <b>serialized</b> and sent through the network a <b>thousand</b> times.<br/><br/>

                    Now if the <b>counter</b> variable
                    is actually a <b>huge object</b>, this may get very <b>inefficient</b> at some point. So the whole
                    problem here is not only about <b>sharing a variable</b>, but also to do it while <b>minimizing
                    IOs</b>
                    and in a <b>safe way</b> in terms of <b>concurrency</b>.
                </aside>
                <div style="text-align: left">
                    <pre style="width:100%;"><code class="scala" style="font-size: 25px;">var counter = 0
var rdd = sc.parallelize(data)

rdd.foreach { rddItem => counter += 1 }
print("Counter value: " + counter)</code></pre>

                    <pre style="width:100%;"><code class="scala" style="font-size: 25px;">val hugeArray = ...
var bigRddWithIndex = ...

bigRddWithIndex.map { rddItem => hugeArray[rddItem.key] }
...</code></pre>
                </div>
                <p style="visibility: hidden">Accumulators and Broadcast variables !</p>
            </section>

            <section data-transition="none none">
                <aside class="notes">
                    In order to resolve this problem, Sparks supports <b>2 types of shared variables</b> : <b>Accumulators</b>
                    and
                    <b>Broadcast variables</b>. The first is designed to send a value from the <b>executors</b> to their
                    <b>driver</b>,
                    while the <b>second one</b> is for the <b>other way around</b>. Now of course, there are some <b>subtleties</b>,
                    but
                    this is the main idea. <!--So let's start with <b>accumulators</b>-->.
                </aside>
                <div style="text-align: left">
                    <pre style="width:100%;"><code class="scala" style="font-size: 25px;">var counter = 0
var rdd = sc.parallelize(data)

rdd.foreach { rddItem => counter += 1 }
print("Counter value: " + counter)</code></pre>

                    <pre style="width:100%;"><code class="scala" style="font-size: 25px;">val hugeArray = ...
var bigRddWithIndex = ...

bigRddWithIndex.map { rddItem => hugeArray[rddItem.key] }
...</code></pre>
                </div>
                <p>Accumulators and Broadcast variables !</p>
            </section>

            <section data-transition="none none">
                <aside class="notes">
                    Accumulators are overall designed to <b>aggregate</b> values coming from the executors to the
                    driver. In order to make this safely, an accumulator's value is <b>only visible by the driver</b>,
                    and can be <b>updated only by its executors</b>. <!--So from the executor's point of view, an accumulator
                    is <b>write-only</b>, while from the driver, it's a <b>read-only value</b>.--><br/><br/>

                    Now as you remember, an RDD may be <b>recomputed</b> for different reasons, which means that, if
                    updated inside a <b>transformation</b>, an <b>accumulator</b> may be modified more than required.
                    Actually, Spark <b>guarantees</b> that for a <b>given accumulator</b>, updates are applied <b>only
                    once</b> they are done inside an <b>action</b>, but this is <b>not true</b> if they are applied inside
                    a <b>transformation</b>. Actually, this feature has been a lot <b>criticized</b>, as it has been found out that
                    in some edge cases, <b>accumulators</b> may be updated more than required even if those <b>updates</b> are done
                    inside an action. Therefore, I strongly suggest you to use them only for <b>debugging purpose</b>, or if
                    they are idempotent.

                    <!--for <b>accumulators</b>
                    which are updated inside a <b>transformation</b>. Therefore, be very <b>cautious</b> with accumulators,
                    and don't <b>rely</b> on them if they are used inside transformations. <!-- Regarding how <b>accumulators</b>
                    can be extended, Spark also <b>supports custom accumulators</b> and custom aggregation operations as
                    long as they honor the <b>Accumulator's contract</b>. -->
                </aside>
                <table>
                    <tr>
                        <td style="vertical-align: middle">
                            Accumulators
                        <pre style="width:100%;"><code class="scala" style="font-size: 25px;">val counter = sc.accumulator(0)
var rdd = sc.parallelize(data)

rdd.foreach {
    rddItem => counter += 1
}
print(counter.value)</code></pre>
                        </td>
                        <td><img style="margin-top:60px; box-shadow: 0; background-color: #FFFFFF; width:100%"
                                 src="images/spark_scala_meetup/accumulators.png"/></td>
                    </tr>
                    <tr>
                        <td colspan="2">Accumulators aggregate values coming from the executors to the driver</td>
                    </tr>
                </table>
            </section>

            <section data-transition="none none">
                <aside class="notes">
                    In a previous example, we were wondering about how it would be possible to share a <b>heavy
                    object</b> without <b>impacting network throughput</b>, or dealing with <b>concurrency issues</b>.
                    Well <b>broadcast variables</b> are exactly designed for this purpose. Basically, a broadcast variable
                    is able to wrap a value and is guaranteed to be <b>sent only once</b> per worker node. This is
                    especially useful if you need to share a <b>lookup table</b> across a cluster's nodes. <br/><br/>

                    In this example, we <b>wrap a big array</b> inside a <b>broadcast variable</b>, and are able to access it by
                    calling the <b>value property</b>. <!--Now no matter if this value is <b>mutable or not</b>, any of its
                    <b>updates</b> won't ever be <b>propagated</b> to the other nodes and will remain <b>local only</b>.<br/><br/>-->

                    Now <b>broadcast variables</b> may be a <b>bottleneck</b> at some point, as serializing and deserializing
                    a data structure can be <b>sometime expensive</b>. So it is important to choose a proper <b>data structure</b>
                    along with a good <b>serialization library</b>. Spark comes by default with <b>Kryo</b> but you can
                    also use other libraries or or your own <b>serialization routines</b>.
                </aside>
                <table>
                    <tr>
                        <td style="vertical-align: middle">
                            Broadcast variables
                        <pre style="width:100%;"><code class="scala" style="font-size: 25px;">val bigArray = sc.broadcast(...)
var bigRddWithIndex = ...

bigRddWithIndex.map { e =>
  broadcastedArray.value[e.key]
}</code></pre>
                        </td>
                        <td><img style="margin-top:60px; box-shadow: 0; background-color: #FFFFFF; width:100%"
                                 src="images/spark_scala_meetup/broadcast_var.png"/></td>
                    </tr>
                    <tr>
                        <td colspan="2">Broadcast variables propagate a read-only value to all executors</td>
                    </tr>
                </table>
            </section>

            <section data-transition="none none">
                <aside class="notes">
                    So let's make a quick recap.<br/><br/>

                    Spark provides <b>two kinds</b> of shared variables, <b>accumulators</b> which are designed to
                    aggregate values coming from the <b>executors</b>, and <b>broadcast variables</b> that are great for
                    <b>sharing</b> objects between a cluster's nodes.<br/><br/>

                    <b>Accumulators</b> should be used for <b>debugging purpose only</b>, and only in <b>actions</b> as
                    they may be updated more than required in some cases. On the other hand <b>Broadcast variables</b> are
                    guaranteed to be sent only <b>once</b> per worker node, and require an <b>efficient data structure</b>
                    along with a <b>fast serialization library</b>.

                    <!--In order to <b>share variables</b> between the <b>driver</b> and its <b>executors</b>, we have to-->
                    <!--<b>rely</b> on <b>accumulators</b>-->
                    <!--and <b>broadcast variables</b>. Accumulators are <b>write-only variables</b> which can be only read-->
                    <!--by the <b>driver</b>.-->
                    <!--As their name <b>suggests it</b>, they are great for <b>aggregating data</b>. However, <b>don't-->
                    <!--rely</b> on them if they-->
                    <!--are used inside <b>transformations</b>, as a transformation maybe executed <b>more than once</b>. As-->
                    <!--long as they-->
                    <!--are used in <b>actions only</b>, or if they are <b>idempotent</b>, you should be fine.<br/><br/>-->

                    <!--On the other hand, <b>broadcast variables</b> are great for sharing <b>read-only</b> data across the-->
                    <!--cluster's-->
                    <!--nodes. They are <b>guaranteed</b> to be sent only once per <b>worker node</b>, and are a good-->
                    <!--candidate if you-->
                    <!--need to share a <b>lookup table</b>. However, they may be <b>expansive</b> if badly serialized,-->
                    <!--therefore, you-->
                    <!--have to use an <b>efficient data-structure</b> along with a good <b>serialization process</b>.-->
                </aside>
                <table>
                    <tr>
                        <td style="vertical-align: top">Accumulators :</td>
                        <td>
                            <ul>
                                <li>Can be only read by the driver</li>
                                <li>Great for aggregations</li>
                                <li>Recommended for debugging only (if used in transformations)</li>
                            </ul>
                        </td>
                    </tr>
                    <tr>
                        <td style="vertical-align: top; white-space: nowrap">Broadcast variables :</td>
                        <td>
                            <ul>
                                <li>Read-Only values</li>
                                <li>Great for lookup tables</li>
                                <li>Watch out for serialization process</li>
                            </ul>
                        </td>
                    </tr>
                </table>
            </section>
        </section>


        <section>
            <h2>Shuffling</h2>
            <section data-transition="none none">
                <aside class="notes">
                    As we've just said, Spark can <b>combine transitions</b> together as long as no data needs to be <b>shuffled</b>,
                    that is moved from one node to the other. Now <b>shuffling</b> may be <b>expansive</b>, still it is
                    required sometime,
                    and therefore needs to be <b>limited</b> in terms of impact on <b>performances</b>.<br/><br/>

                    In the next few slides, we'll discuss about <b>what makes a shuffle happening</b>, explain <b>why it
                    is expansive</b>, and <b>how to limit its impact</b>.<br/><br/>

                    ****
                    So first, let's see what <b>triggers</b> a shuffle. As we've seen it, these first lines here, the
                    <b>textFile</b>,
                    the <b>flatMap</b>, and the <b>map</b> transitions, are <b>regrouped</b> in a single <b>stage</b>
                    and all happening on the same
                    worker node.
                </aside>
                <table class="fragment">
                    <tr>
                        <td style="vertical-align: middle">
                            <ul style="margin-left: 0;margin-right:30px; list-style-type:none;white-space: nowrap;">
                                <li>sparkContext</li>
                                <li style="color:#A3BCFF; padding-left: 25px">.textFile("hdfs://...")</li>
                                <li style="color:#A3BCFF; padding-left: 25px">.flatMap(_.split(" "))</li>
                                <li style="color:#A3BCFF; padding-left: 25px">.map((_, 1))</li>
                                <li style="padding-left: 25px">.reduceByKey(_ + _)</li>
                            </ul>
                        </td>
                        <td><img
                                style="margin-top:30px; margin-left:100px;box-shadow: 0; background-color: #FFFFFF; width:100%"
                                src="images/spark_scala_meetup/shuffle_1.png"/></td>
                    </tr>
                </table>
            </section>
            <section data-transition="none none">
                <aside class="notes">
                    But once <b>reduceByKey</b> has been called, Spark will take <b>all the pairs</b> having the <b>same
                    word</b>, and put them on the <b>same machine</b> together. And this is typically the kind of situation which <b>triggers
                    a shuffle</b>. More generally, consider than any <b>all-to-all operations</b> such as a <b>groupByKey</b> or
                    a <b>join</b> will trigger that.<br/><br/>

                    Now why is this <b>expansive</b> ? Well this may sound <b>obvious</b> but <b>serializing data</b>,
                    <b>sending</b> it through the
                    <b>network</b>, and <b>deserializing</b> it on the <b>destination</b> node can quickly become very
                    <b>costly</b>, especially
                    if the dataset is <b>big</b>. So overall, in order to limit <b>shuffling</b>, there are <b>two</b>
                    main
                    aspects you should focus on <b>the quantity of data to shuffle</b>, and <b>the level of
                    parallelism</b>.
                </aside>
                <table>
                    <tr>
                        <td style="vertical-align: middle">
                            <ul style="margin-left: 0;margin-right:30px; list-style-type:none;white-space: nowrap;">
                                <li>sparkContext</li>
                                <li style="color:#A3BCFF; padding-left: 25px">.textFile("hdfs://...")</li>
                                <li style="color:#A3BCFF; padding-left: 25px">.flatMap(_.split(" "))</li>
                                <li style="color:#A3BCFF; padding-left: 25px">.map((_, 1))</li>
                                <li style="color:#9EE13C; padding-left: 25px">.reduceByKey(_ + _)</li>
                            </ul>
                        </td>
                        <td><img
                                style="margin-top:30px; margin-left:100px;box-shadow: 0; background-color: #FFFFFF; width:100%"
                                src="images/spark_scala_meetup/shuffle_2.png"/></td>
                    </tr>
                </table>
            </section>

            <section data-transition="none none">
                <aside class="notes">
                    Let's say I want to <b>make a join</b> between an RDD containing the people in <b>Canada</b>
                    with one that contains all the canadian <b>provinces/territories</b>.<br/><br/>

                    *** Well, this would result in a <b>very uneven sharding</b> as some
                    provinces like <b>Quebec</b> being much more populated than territories like
                    <b>Nunavut</b>.<br/><br/>

                    Moreover, we would get a <b>very limited level of parallelism</b> as we would only have 13 <b>partitions</b>,
                    and
                    adding more <b>machines</b> to get the job done would not change <b>anything</b>.
                </aside>
                <table>
                    <tr>
                        <td><img
                                style="margin-top:30px; margin-right:60px;box-shadow: 0; background-color: #FFFFFF; width:100%"
                                src="images/spark_scala_meetup/shuffle_3.png"/></td>
                        <td class="fragment" style="vertical-align: middle">
                            <ul style="width:350px; white-space: nowrap;">
                                <li>Uneven sharding</li>
                                <li>Limited parallelism</li>
                            </ul>
                        </td>
                    </tr>
                </table>
            </section>
            <section data-transition="none none">
                <aside class="notes">
                    Actually, a better way to do this is to use a <b>broadcast variable</b> cointaining all the canadian
                    <b>provinces and territories</b>. This variable could be then sent to all <b>the worker nodes</b>,
                    which would
                    avoid a <b>shuffle</b>. Now of course, this is only possible if the <b>dataset</b> being sent can
                    <b>fit in the memory</b>
                    of a single node.
                </aside>
                <table>
                    <tr>
                        <td><img
                                style="margin-top:30px; margin-right:60px;box-shadow: 0; background-color: #FFFFFF; width:100%"
                                src="images/spark_scala_meetup/shuffle_4.png"/></td>
                        <td style="width:350px; vertical-align: middle">
                            <ul style=" white-space: nowrap;">
                                <li>Even Sharding</li>
                                <li>No Shuffle Required</li>
                            </ul>
                        </td>
                    </tr>
                </table>
            </section>
            <section data-transition="none none">
                <aside class="notes">
                    Now what if you cannot use a <b>broadcast variable</b> ? what if the data in <b>both RDDs</b> is too
                    big ? Let's
                    say for example that we need to <b>join</b> an <b>RDD</b> containing all the <b>people in Canada</b>
                    with one
                    containing all the <b>people in the world</b>. Joining those <b>RDDs</b> would result in a <b>lot of
                    data shuffled</b>
                    across the <b>network</b> and potentially a <b>space problem</b> in the destination nodes.
                </aside>
                <table>
                    <tr>
                        <td><img
                                style="margin-top:30px; width:100%; margin-left:10px;box-shadow: 0; background-color: #FFFFFF"
                                src="images/spark_scala_meetup/shuffle_7.png"/></td>
                        <td style="vertical-align: middle">
                            <ul style="width:350px; white-space: nowrap;">
                                <li>Even Sharding</li>
                                <li>Good Parallelism</li>
                                <li>Space may be a problem</li>
                            </ul>
                        </td>
                    </tr>
                </table>
            </section>
            <section data-transition="none none">
                <aside class="notes">
                    So instead of doing this, we could just create a <b>partial RDD</b> using a <b>filter</b>, and <b>reduce</b>
                    the amount
                    of data to be <b>transferred</b>. The whole idea here is to make sure that you <b>transfer</b> only
                    the data that
                    is required by <b>further</b> operations.
                </aside>
                <table>
                    <tr>
                        <td><img
                                style="margin-top:30px; width:100%; margin-left:10px;box-shadow: 0; background-color: #FFFFFF;"
                                src="images/spark_scala_meetup/shuffle_8.png"/></td>
                        <td style="vertical-align: middle;">
                            <ul style="width:350px; white-space: nowrap;">
                                <li>Even Sharding</li>
                                <li>Good Parallelism</li>
                                <li>Less Space required</li>
                                <li>Less Data shuffled</li>
                            </ul>
                        </td>
                    </tr>
                </table>
            </section>
            <section data-transition="none none">
                <aside class="notes">
                    So these are a <b>couple of solutions</b>, but overall, there is no <b>strict rule</b> or <b>silver
                    bullet</b> to avoid <b>shuffling</b>. Use <b>efficient data-structures</b> along with a <b>fast serialization
                    library</b> and
                    <b>know your data</b>, so you can only <b>transfer the minimum required</b>. In any case, if you
                    have a doubt
                    about what's <b>happening</b>, Spark provides a <b>UI</b> containing <b>tons of graphics and
                    metrics</b>. It's very
                    easy to use and very well documented, and I highly suggest you to have a glance at it.
                </aside>
                <table>
                    <tr>
                        <td style="vertical-align: top;white-space: nowrap">Recommendations :</td>
                        <td style="white-space: nowrap">
                            <ul>
                                <li>Fast serialization library (Kryo)</li>
                                <li>Lightweight data-structures</li>
                                <li>Shuffle only the minimum required</li>
                                <li>Use Spark's UI</li>
                            </ul>
                        </td>
                    </tr>
                </table>
            </section>
        </section>

        <!-- ####################################################################################################### -->

        <section>
            <h2>Conclusion</h2>
            <aside class="notes">
                So this is the end of this <b>presentation</b>, and I hope that it met your <b>expectations</b>. Spark
                is probably one of the <b>biggest thing</b> happening nowadays in the <b>big data industry</b> as it <b>tackles</b> the
                Big Data problem from a completely <b>different angle</b>. A modern <b>stream processing pipeline</b> is actually not only
                about <b>processing</b> the data but also about the associated <b>pre-processing</b> and <b>post-processing</b> aspects. In
                other words, the <b>same stream of data</b> may be used in <b>batch</b> jobs, <b>machine learning</b>, <b>graphs</b> as
                much as to provide <b>live updates</b>. Some time ago, this implied that you had to <b>deal with many different frameworks</b> and <b>disparate
                programming models</b> in order to achieve this. Spark makes this actually <b>very simple</b>, thanks to a <b>unified
                API</b>, much <b>better performances</b> than traditional Map-Reduce engines, and an <b>ever growing community</b>.
            </aside>
            <ul>
                <li><span style="font-size: 30px;color: #FF6600;">Learning Spark : </span><span style="font-size: 25px">by Andy Konwinski, Holden Karau, and Patrick Wendell</span>
                </li>
                <li><span style="font-size: 30px;color: #00CC33;">Advanced Analytics with Spark : </span><span
                        style="font-size: 25px; ">by Josh Wills, Sandy Ryza, Sean Owen, and Uri Laserson</span></li>
                <li><span style="font-size: 30px;color: #FF6600;">Introduction to Apache Spark : </span> <span
                        style="font-size: 25px">by Paco Nathan</span></li>
            </ul>
        </section>

        <section>
            <aside class="notes">
                So I haven't introduced my self, my name is Francis Toth, I'm the Scala tam Lead at Yoppworks. Very quickly,
                Yoppworks has been created on the remains of Bold Radius, and provides consulting and training services
                in partnership with Lightbend and Hortonworks. We are quite new but have more projects than people,
                therefore we are hiring. We are mostly looking for people having experienced with the Lightbend's stack along with
                Big Data projects. So if you are interested, just come see me, I'll be happy to talk about a potential collaboration
                or anything else actually. Here are some business cards, feel free to take one of those. Thank you.
            </aside>
            <img style="background-color: transparent"
                 src="images/spark_scala_meetup/spark_scala_meetup/yoppworks-logo-landscape-large-text-dark-bg.png"/>
            Thank you ! Questions ?
        </section>

        <!-- ####################################################################################################### -->

        <section>
            <h2>Spark's high-level APIs</h2>
            <aside class="notes">
                So this is all about Spark's core. I know it's a lot of material but at least this gives you an overview
                of how Spark works. Now let's move on, an look at some higher level API's. As we've said in the
                begining of this talk, we won't have the time to cover them all and will only focus on the broad lines.
            </aside>
        </section>

        <section>
            <h2>Spark SQL</h2>

            <section data-transition="none none">
                <aside class="notes">
                    So, Spark SQL. In a couple of words, Spark SQL is an API*** released in 2015***, which allows you to
                    interact with a dataset through SQL, as long as it can be represented in a tabular format. So the
                    data must be structured and have a schema. This allows you to read and write data in a variety****
                    of structured format like Json, Hive or Parquet in a very unified and transparent way.****<br/><br/>

                    In some way Spark SQL is very close to Hive as they both deal with interactive querying. Actually,
                    they are not exclusive.**** Spark SQL supports indeed most of Hive features, and allows you to access
                    Hive tables, User-Defined functions, SerDes for serialization and deserialization formats, and the Hive Query
                    Language.<br/><br/>

                    Now it's important to notice that using Hive features with Spark SQL does not require a Hive
                    installation. A library called spark-hive provides the glue between these two frameworks which let you take
                    advantage of Hive without even installing it.

                    <!--The reason why you can choose to use Hive or not, is that some-->
                    <!--users may have dependency conflict between the version of Hive used by their cluster and the one-->
                    <!--they wish to use with Spark.-->
                </aside>
                <ul>
                    <li class="fragment">Released in February 2015</li>
                    <li class="fragment">Structured dataset querying through SQL</li>
                    <li class="fragment">Integrates with Hive, Parquet, JSon...</li>
                    <li class="fragment">Supports most of Hive's feature</li>
                </ul>
            </section>
            <section data-transition="none none">
                <aside class="notes">
                    Under the hood, Spark SQL is based on an extension of the RDD model called a Dataframe. Dataframes
                    are overall similar to tables in a relational database and contain Row objects, which are just
                    wrappers around arrays of basic types. Each row represents a record, and as they are structured, they can be
                    stored in a much more efficient manner than native RDDs. Actually, some benchmarks show that a Dataframe is
                    twice faster than an RDD if the Scala API is used, and ten time faster if using the one written in
                    Python.
                </aside>
                <img style="margin-top:30px; width:100%; margin-left:50px;box-shadow: 0; background-color: #FFFFFF;"
                     src="images/spark_scala_meetup/dataframe_0.png"/>
            </section>

            <section data-transition="none none">
                <aside class="notes">
                    Now enough talking now, let's see some code.

                    First thing you need to do in order to play with Spark-SQL, is to import a context.**** Depending
                    on whether you use Hive or not, Spark SQL has two main entry points : The SQLContext and the
                    HiveContext. According the official documentation, it is actually recommended to use Hive's features as a lot of
                    support material is already available for them. ****A dataframe can be created by converting an existing RDD
                    or ****from external sources like a json or an hdfs file. For some sources, Sparq SQL will be able
                    to infer the schema of your data, and this is exactly what happens when I create a dataframe from a
                    json file. **** This schema can by the way be printed using the method show() provided by the dataframe
                    object.
                </aside>

                <div class="fragment">
                    <pre><code class="scala">import org.apache.spark.sql.hive.HiveContext
val hiveCtx = new HiveContext(new SparkContext(...))</code></pre>

                    <pre><code class="scala">import org.apache.spark.sql.SQLContext
val sqlCtx = new SQLContext(new SparkContext(...))</code></pre>
                </div>

                <div class="fragment">
                    <pre><code class="scala">val dataframe = sqlCtx.createDataFrame(rdd)</code></pre>
                    <pre><code class="scala">val dataframe = hiveCtx.jsonFile(inputFile)</code></pre>
                </div>

                <pre class="fragment"><code class="console">scala> dataframe.show()
root
|-- foo: struct (nullable = true)
|    |-- waldos: array (nullable = true)
|    |    |-- element: string (containsNull = false)
|-- bar: boolean (nullable = true)
|-- qux: string (nullable = true)</code></pre>
            </section>

            <section data-transition="none none">
                <aside class="notes">
                    Now, in order to query my data, I just have to register a table name and use the sql method. This
                    brings me back a dataframe containing all the data I've requested, so I can make more computation on top of
                    it. Actually, depending on the kind of source the dataframe has been created from, Spark-SQL will be
                    able to select only a subset of the fields and smartly scan only the data for those fields, instead of
                    scanning everything.****

                    Now if I decide to apply some transformations on that dataframe, I have two options. I can either
                    use one of dataframe functions like select, filter or groupBy, or I can convert the resulting
                    dataframe into an rdd, and apply the functions provided by it.
                </aside>
                <pre><code class="scala">input.registerTempTable("foobar")
val foobarz = hiveCtx.sql("SELECT * FROM foobar ORDER BY qux LIMIT 10")</code></pre>

                <pre class="fragment"><code class="scala">foobarz.select(...).filter(...).groupBy(...)
foobarz.rdd.filter(...).map(...)</code></pre>
            </section>

            <section data-transition="none none">
                <aside class="notes">
                    Regarding how dataframes integrate with the rest of the world, Spark SQL provides a datasource API,
                    which allows you to integrate Spark SQL with Avro, HBase, ElasticSearch, and Cassandra to name only a few.
                </aside>
                <img style="margin-top:30px; width:100%; margin-left:10px;box-shadow: 0; background-color: #FFFFFF;"
                     src="images/spark_scala_meetup/dataframe_5.png"/>
            </section>

        </section>

        <section>
            <h2>Spark Streaming</h2>
            <section data-transition="none none">
                <aside class="notes">
                    Not let's move on to another API which is Spark Streaming. So, Spark Streaming was released in 2013,
                    *** that is, in early versions of Spark and has been always pretty stable since. *** It's real-time
                    analytics library which is used in cases such as monitoring the flow of users on a website or
                    detecting fraud transaction in real time. *** Spark-Streaming caught a lot of attention recently
                    , as real time is today extremely in demand. According to Databricks, the company created by the
                    founders of Spark, 56% more Spark's users globally ran Spark Streaming applications in 2015 compared
                    to 2014., and among those users, you'll find names like Uber, Netflix and Pinterest to name only a
                    few.
                    <!--http://www.datanami.com/2015/11/30/spark-streaming-what-is-it-and-whos-using-it/-->
                    <!--https://opensource.com/business/15/4/guide-to-apache-spark-streaming-->

                </aside>
                <ul>
                    <li class="fragment">Released in 2013</li>
                    <li class="fragment">Very stable and robust</li>
                    <li class="fragment">Real-Time analytics API</li>
                    <li class="fragment">Trendy</li>
                </ul>
            </section>
            <section data-transition="none none">
                <aside class="notes">
                    Spark-Streaming relies on micro-batch architecture. So the streaming computation is divided in small
                    continuous series of batches created at regular time intervals. At the beginning of each interval,
                    a new batch is created, and any data that arrives during this interval gets added to that batch.
                    Once the batch is done growing, it is processed as an RDD just like we've seen earlier.
                </aside>
                <img style="margin-top:50px; width:100%; margin-left:40px;box-shadow: 0; background-color: #FFFFFF;"
                     src="images/spark_scala_meetup/streaming_archi.png"/>
            </section>

            <section data-transition="none none">
                <aside class="notes">
                    The fundamental abstraction of Spark-Streaming is a DStream, or discretized stream. A DStream is
                    a sequence of RDD where each RDD has one time slice of the data in the stream. Just like RDDs,
                    DStreams can be created from external sources or by applying transformations to other DStreams.
                    Once created and properly transformed, we can apply ouput actions such as print() to the DStream.
                    Output actions are actually similar to RDD actions in that they write data to external systems.
                    However, these actions run periodically on each time step, producing output in batches.

                    Now these DStreams won't be started as long as I don't call the start method here. It's pretty
                    much like the concept of actions with RDDs. This means that all the DStreams which compose my
                    program must be specified before calling the start method. Once it has been called, any modification
                    of these streams won't be taken in account. The streaming will be effective until the
                    streams are finished, either manually or du to an error, and this is what the awaitTermination is
                    for.

                    As you can see, the syntax used to create an RDD and a DStream is pretty similar, and actually
                    this is what really makes Spark shine. No matter if you are doing batch or streaming processing,
                    the programming model is always the same.
                </aside>

                <pre><code class="scala">// Boilerplate
val conf = new SparkConf()
    .setMaster("local[2]").setAppName("NetworkWordCount")
val ssc = new StreamingContext(conf, Seconds(1))

// Create a DStream listening on port 9999
val lines: DStream[String] = ssc.socketTextStream("localhost", 9999)

lines.flatMap(_.split(" "))
    .map(word => (word, 1))
    .reduceByKey(_ + _)
    .print()

ssc.start() // Start the computation
ssc.awaitTermination() // Wait for the computation to terminate</code></pre>
            </section>

            <section data-transition="none none">
                <aside class="notes">
                    Regarding fault-tolerance, DStreams provide the same properties than RDDs. As long as a copy of the
                    data is
                    available, a DStream can recompute any state derived from it, using the lineage of its RDDs.
                    Therefore,
                    received data is replicated across two nodes by default, allowing a DStream to tolerate single
                    worker
                    failures. Secondly, as it can be expansive to recompute a long stream of data, Spark-Streaming
                    provides a mechanism called checkpointing that saves state periodically to a reliable filesystem. So
                    in case of failure, Spark will only have to go back to the last checkpoint. Finally, Spark-Streaming
                    can also be configured in order to run 24/7. Properly configured, it is able to ensure that no
                    matter
                    if the driver, a worker or a receiver fails, it will always be able to get back on its feet. Of
                    course,
                    this comes with some challenges and configurations but this is the idea.
                </aside>
                <table>
                    <tr>
                        <td style="vertical-align: top;white-space: nowrap">Fault-tolerance :</td>
                        <td style="white-space: nowrap">
                            <ul>
                                <li>Data replication</li>
                                <li>Checkpointing</li>
                                <li>Can run 24/7</li>
                            </ul>
                        </td>
                    </tr>
                </table>
            </section>

            <section data-transition="none none">
                <aside class="notes">
                    Regarding how you can integrate Spark-Streaming with other frameworks, well there is nothing
                    surprising.
                    Spark-Streaming is able to integrate with Akka, Kafka, Flume, and Kinesis to name only a few.
                </aside>
                <img style="margin-top:50px; width:100%; margin-left:40px;box-shadow: 0; background-color: #FFFFFF;"
                     src="images/spark_scala_meetup/streaming_integration.png"/>
            </section>
        </section>

        <section>
            <h2>Spark APIs : What's left ?</h2>
            <aside class="notes">
                So, we're reaching the end of this presentation, and unfortunately we won't have time to cover Spark ML
                and GraphX. But let's have a very quick word about them. ***

                Spark ML is Spark's Machine Learning Library and is designed to run in parallel on clusters. It provides
                mostly algorithms which are designed to run in parallel such as distributed random forests, K-means or
                alternating least squares, and are best suited for running on very large dataset, more than on small
                ones.
                ***

                On the other hand, GraphX is Spark's graph processing framework and intends to unify Data-parallel and
                graph parallel systems. It is designed for parallel iterative graph computation, provides a wide range
                of
                graph algorithm, and it's also very fast, actually about 10 times faster than traditional Hadoop graph
                algorithms. Secondly, if you've already worked with Graphs, you probably know that the graph processing
                is only a small part of the problem. Actually as data get bigger and bigger, the graph creation and
                post-processing tend to become a bottleneck. And this why GraphX is interesting. It provides support for
                graph construction and post-processing through a unified API.
            </aside>
            <table>
                <tr class="fragment">
                    <td style="vertical-align: top; white-space: nowrap">Spark ML :</td>
                    <td>
                        <ul>
                            <li>Native Machine Learning Framework</li>
                            <li>Supports Classification, Regression, Clustering</li>
                            <li>Designed to run on large distributed data in parallel</li>
                        </ul>
                    </td>
                </tr>
                <tr class="fragment">
                    <td style="vertical-align: top; white-space: nowrap">GraphX :</td>
                    <td>
                        <ul>
                            <li>Native Graph Processing framework</li>
                            <li>Similar to Pregel, Giraph, and Graphlab</li>
                            <li>Designed for network-oriented analytics</li>
                            <li>Very fast !</li>
                        </ul>
                    </td>
                </tr>
            </table>
        </section>

        <!--<section>-->
        <!--<h2>Conclusion</h2>-->
        <!--<aside class="notes">-->
        <!--So this is the end of this <b>presentation</b>, and I hope that it met your <b>expectations</b>. Spark-->
        <!--is probably one of the <b>biggest thing</b> happening nowadays in the <b>big data industry</b> as it <b>tackles</b> the-->
        <!--Big Data problem from a completely <b>different angle</b>. A modern <b>stream processing pipeline</b> is actually not only-->
        <!--about <b>processing</b> the data but also about the associated <b>pre-processing</b> and <b>post-processing</b> aspects. In-->
        <!--other words, the <b>same stream of data</b> may be used in <b>batch</b> jobs, <b>machine learning</b>, <b>graphs</b> as-->
        <!--much as to provide <b>live updates</b>. Some time ago, this implied that you had to <b>deal with many different frameworks</b> and <b>disparate-->
        <!--programming models</b> in order to achieve this. Spark makes this actually <b>very simple</b>, thanks to a <b>unified-->
        <!--API</b>, much <b>better performances</b> than traditional Map-Reduce engines, and an <b>ever growing community</b>.-->
        <!--</aside>-->
        <!--<ul>-->
        <!--<li><span style="font-size: 30px;color: #FF6600;">Learning Spark : </span><span style="font-size: 25px">by Andy Konwinski, Holden Karau, and Patrick Wendell</span>-->
        <!--</li>-->
        <!--<li><span style="font-size: 30px;color: #00CC33;">Advanced Analytics with Spark : </span><span-->
        <!--style="font-size: 25px; ">by Josh Wills, Sandy Ryza, Sean Owen, and Uri Laserson</span></li>-->
        <!--<li><span style="font-size: 30px;color: #FF6600;">Introduction to Apache Spark : </span> <span-->
        <!--style="font-size: 25px">by Paco Nathan</span></li>-->
        <!--</ul>-->
        <!--</section>-->

        <!--<section>-->
        <!--<img style="background-color: transparent"-->
        <!--src="images/spark_scala_meetup/spark_scala_meetup/yoppworks-logo-landscape-large-text-dark-bg.png"/>-->
        <!--Thank you ! Questions ?-->
        <!--</section>-->
    </div>
</div>

<script src="lib/js/head.min.js"></script>
<script src="js/reveal.js"></script>

<script>

    // Full list of configuration options available at:
    // https://github.com/hakimel/reveal.js#configuration
    Reveal.initialize({
        controls: true,
        progress: true,
        history: true,
        center: true,

        transition: 'slide', // none/fade/slide/convex/concave/zoom

        // Optional reveal.js plugins
        dependencies: [
            { src: 'lib/js/classList.js', condition: function() { return !document.body.classList; } },
            { src: 'plugin/markdown/marked.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
            { src: 'plugin/markdown/markdown.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
            { src: 'plugin/highlight/highlight.js', async: true, condition: function() { return !!document.querySelector( 'pre code' ); }, callback: function() { hljs.initHighlightingOnLoad(); } },
            { src: 'plugin/zoom-js/zoom.js', async: true },
            { src: 'plugin/notes/notes.js', async: true }
        ]
    });

</script>
</body>
</html>

            <!--<section data-transition="none none">-->
                <!--<aside class="notes">-->
                <!--def aggregate[U](zeroValue: U)(-->
                  <!--seqOp: (U, T) ⇒ U,   // merging T into a U-->
                  <!--combOp: (U, U) ⇒ U   // Merges two U's-->
                <!--): U-->
                    <!--Aggregate the elements of each partition, and then the results for all the partitions, using given-->
                    <!--combine functions and a neutral "zero value". This function can return a different result type, U,-->
                    <!--than the type of this RDD, T. Thus, we need one operation for merging a T into an U and one-->
                    <!--operation for merging two U's, as in scala.TraversableOnce. Both of these functions are allowed to-->
                    <!--modify and return their first argument instead of creating a new U to avoid memory allocation.-->
                <!--</aside>-->
                <!--<pre style="width:1000px;"><code class="scala" style="font-size: 25px;">class Result(var total: Int) extends Serializable-->
<!--val rdd = sc.parallelize(Seq(1, 2, 3, 4))</code></pre>-->

                <!--<pre style="width:1000px;"><code class="scala" style="font-size: 25px;">val result = rdd.aggregate(new Result(0))(-->
  <!--(result, value)    => new Result(result.total + value),-->
  <!--(result1, result2) => new Result(result1.total + result2.total)-->
<!--)</code></pre>-->
                <!--<pre style="width:1000px;"><code class="scala" style="font-size: 25px;">val result = rdd.aggregate(new Result(0))(-->
  <!--(acc, value) => {-->
    <!--acc.total = acc.total + value-->
    <!--acc-->
  <!--},-->
  <!--(acc1, acc2) => {-->
    <!--acc1.total = acc1.total + acc2.total-->
    <!--acc1-->
  <!--}-->
<!--)</code></pre>-->
            <!--</section>-->


            <section data-transition="none none">
                <aside class="notes">
                    Alright, let's quickly look at the famous world count example. In a nutshell, this algorithm is
                    designed to count the number of occurrences of each word in a distributed set of documents.

                    The first set of transformation is responsible for loading the data, and for mapping it to a format we
                    can easily work with. These transformations can actually be done in parallel without changing the
                    way the data has been distributed when loaded. Therefore, a first stage is created.

                    However, once we get here, all the tuples having the same key have to be stored in the same
                    partition in order to be counted. As some of those tuples may be stored in different partitions,
                    a shuffle is required. So Spark will scan the whole dataset and will redistribute the data
                    accordingly. Once redistributed, we can now count the number of occurrences for each word and do
                    whatever we want with the result.

                    <!-- Rajouter la slide moche pour l'exemple-->
                </aside>
                <div style="display:table;margin: 70px auto auto auto">
                    <div style="display:table-cell;vertical-align: top;text-align: left;">
                        <span class="fragment highlight-light-blue" data-fragment-index="1">sc.textFile("hdfs://...")</span><br/>
                        <ul style="list-style: none;margin-left: 50px;">
                            <li class="fragment highlight-light-blue" data-fragment-index="1">.flatMap(_.split(" "))</li>
                            <li class="fragment highlight-light-blue" data-fragment-index="1">.map(w ⇒ (w, 1))</li>
                            <li class="fragment highlight-red" data-fragment-index="3">.reduceByKey(_ + _)</li>
                            <li class="fragment highlight-red" data-fragment-index="3" >.collect()</li>
                        </ul>
                    </div>
                    <div style="display:table-cell;width:100px"></div>
                    <div style="display:table-cell;vertical-align: top;">
                        <div class="fragment border" data-fragment-index="2" >
                            <div class="fragment highlight-light-blue" data-fragment-index="1" style="border: 1px solid white;margin: 5px">textFile</div>
                            <div class="up"><div class="arrow"></div></div><br/>
                            <div class="fragment highlight-light-blue" data-fragment-index="1" style="border: 1px solid white;margin: 5px">flatMap</div>
                            <div class="up"><div class="arrow"></div></div><br/>
                            <div class="fragment highlight-light-blue" data-fragment-index="1" style="border: 1px solid white;margin: 5px">map</div>
                        </div>
                            <div class="up"><div class="arrow"></div></div><br/>
                        <div class="fragment border" data-fragment-index="4">
                            <div class="fragment highlight-red" data-fragment-index="3" style="border: 1px solid white;margin: 5px; width: 250px;">reduceByKey</div>
                            <div class="up"><div class="arrow"></div></div><br/>
                            <div class="fragment highlight-red" data-fragment-index="3" style="border: 1px solid white;margin: 5px">collect</div>
                        </div>
                    </div>
                </div>
            </section><section data-transition="none none">
            <h2>Recap</h2>
            <aside class="notes">
                Alright, so this was pretty much Spark in a nutshell. We covered how Spark represents the data
                and how it performs computations on it. Overall, an RDD is a sequence of steps that each executor
                has to perform on each partition in order to obtain the final result. We saw that depending on the
                operations involved, data may have to be redistributed among the cluster or not, which may have a
                significant impact on a Spark job.

                We've also covered how this looks like under the hood and saw that narrow transformations are actually
                fused together for optimization reasons, and that the operation resulting from this fusion is called
                a stage. Each stage is then run on each partition of the dataset by the executors.

                Having all this in mind, there are three things you should look at when a Spark job does not meet
                with your expectations in terms of performances:

                - The transformations implied by the job.
                <!--Are they expansive, easy to compute, where does the input-->
                <!--data come from, do they create extra objects whenever a record is processed, etc...-->

                - The data distribution:
                <!--This is probably the one you should care about the most. A bad-->
                <!--distribution may lead to heavy shuffles, skewed data, that is data which partitioning led to a set-->
                <!--of unbalanced partitions, and so forth...-->

                - Finally shuffles:
                <!--To be honest it is hard to avoid them however, there's some tricks you can use to-->
                <!--shuffle efficiently.-->

                In the next part of this talk, we'll focus on these three items and will cover some solutions you
                could think about whenever you face one of those issue.
            </aside>
            <div style="text-align: left">
                Spark in a nutshell:<br/>
                <ul style="margin-left: 70px;">
                    <li>RDD is an in interface to your dataset</li>
                    <li>Wide / Narrow Transformations</li>
                    <li>Data redistribution may be (very) expensive</li>
                </ul>
            </div>
            <br/>
            <div style="text-align: left">
                Considerations:<br/>
                <ul style="margin-left: 70px;">
                    <li>Transformations</li>
                    <li>Data distribution</li>
                    <li>Shuffles</li>
                </ul>
            </div>
        </section>

            <!-- ################################################################################################### -->
            <!-- ################################################################################################### -->
            <!-- ################################################################################################### -->
            <!-- ################################################################################################### -->
            <!-- ################################################################################################### -->

        <section data-transition="none none">
            <h2>Efficient Transformations</h2>
            <section data-transition="none none">
                <aside class="notes">
                    First of all, let's quickly look at which part of a Spark program is run on the driver and on the
                    executors. Any guess ?

                    includes also the lambda functions passed to those higher order functions. Actions on the other hand
                    may be run on the driver on the executors depending on the action.
                </aside>
                <div style="text-align: left;margin-top:50px;display:inline-block;">
                    <ul style="margin-left: 0;margin-right:80px; list-style-type:none;white-space: nowrap;vertical-align: top;">
                        <li class="fragment highlight-blue" data-fragment-index="1">val output = sparkContext</li>
                        <li class="fragment highlight-red" data-fragment-index="2" style="padding-left: 100px">.textFile("hdfs://...")</li>
                        <li class="fragment highlight-red" data-fragment-index="2" style="padding-left: 100px">.flatMap(_.split(" "))</li>
                        <li class="fragment highlight-red" data-fragment-index="2" style="padding-left: 100px">.map((_, 1))</li>
                        <li class="fragment highlight-red" data-fragment-index="2" style="padding-left: 100px">.reduceByKey(_ + _)</li>
                        <li class="fragment highlight-green" style="padding-left: 100px">.collect()</li>
                        <li class="fragment highlight-blue" data-fragment-index="1">print output</li>
                    </ul>
                </div>
                <div style="text-align: left; margin-top:60px;display:inline-block; font-size: 30px;width:460px;">
                    <ul style="list-style: none;vertical-align: top;">
                        <li><div class="box blue" style="display:inline-block;vertical-align: middle"></div>&nbsp;Executed on the driver</li>
                        <li><div class="box red" style="display:inline-block;vertical-align: middle"></div>&nbsp;Executed on the executors</li>
                        <li><div class="box green" style="display:inline-block;vertical-align: middle"></div>&nbsp;Executed on the driver</li>
                    </ul>
                </div>
            </section>
            <section data-transition="none none">
                <aside class="notes">
                    Having this in mind, is there anyone who could tell me what is the value printed out once this program
                    is run?

                    As we've seen it, Spark's API <b>abstracts</b> the fact that your code may be <b>executed</b> by the
                    <b>driver</b> or not. So depending on how you <b>structure</b> your <b>transformations</b>
                    and <b>actions</b>, some <b>operations</b> may cause an <b>overhead</b> or <b>behave</b> in an
                    unexpected way.
                </aside>
                <ul style="list-style-type:none;padding-right: 30px">
                    <li class="fragment highlight-green" data-fragment-index="1">var counter = 0</li>
                    <li class="fragment highlight-green" data-fragment-index="1">var rdd = sc.parallelize(Seq(1, 2, 3, ...))</li>
                    <li>&nbsp;</li>
                    <li>rdd.foreach { rddItem ⇒
                    <li class="fragment highlight-red" data-fragment-index="2">&nbsp;&nbsp;&nbsp;counter += 1</li>
                    <li>}<br/>&nbsp;</li>
                    <li class="fragment highlight-green" data-fragment-index="1">print("Counter value: " + counter)</li>
                </ul>
            </section>
            <section data-transition="none none">
                <aside class="notes">
                    Let's look now at some common mistakes. Is there anyone here who could tell me what is being done
                    here? Well whenever we run this program, a JDBC connection will be requested for each record of the
                    dataset. Ideally, we'd like to minimize the number of connections required to compute this
                    transformation.

                    One way to do this, is to use the mapPartition transformation. mapPartition allows you to execute
                    a function for each partition of the dataset. This function takes an iterator which iterates over
                    the records contained in a given partition, and returns another iterator. This code now will request
                    a JDBC connection for each partition instead of requesting one for each record.

                    We won't get into details here, but almost all transformations provided by Spark are actually
                    a specialization of the mapPartition function. One thing to watch for when you use mapPartition is
                    to resist the temptation of evaluating the iterator by getting its size or its content. Doing so
                    would force the loading of all the records of the partition being processed, which may throw an
                    OME.
                </aside>
                <pre><code class="scala" style="font-size: 25px;">val rdd = sc.parallelize(Seq(1, 2, 3, 4))

rdd.map { id ⇒
    val con = ... // get JDBC connection
    findOrderById(con, id).length
}</code></pre>

                <pre class="fragment"><code class="scala" style="font-size: 25px;">rdd.mapPartitions { idIterator ⇒
    val con = ... // get JDBC connection
    idIterator.map { id ⇒
        findOrderById(con, id).length
    }
}</code></pre>
            </section>

            <section data-transition="none none">
                <aside class="notes">
                    Now why should you watch for transformations memory footprint? Well it's because depending on the size
                    of your dataset, the memory consumption of a transformation can quickly become unmanageable, and if not,
                    can increase the pressure on the memory and the CPU by triggering more garbage collection cycles,
                    which slow down the overall execution. It is therefore critical to reduce a transformation's memory
                    footprint.

                    Overall, you should limit the number of objects created in your transformations. Ideally, try to
                    rely on primitives and mutable structures when its possible. Most aggregations can be done using mutable
                    structures as long as they are not modified outside the transformation. Finally, mapPartition is a
                    very powerful tool as long as you don't load the content of the record iterator.
                </aside>
                <ul>
                    <li>Minimize object creation</li>
                    <li>Try to rely on primitives</li>
                    <li>Use mutable structures when possible (aggregation)</li>
                    <li>Don't underestimate the power of mapPartition</li>
                </ul>
            </section>
        </section>

        <!-- ################################################################################################### -->
        <!-- ################################################################################################### -->
        <!-- ################################################################################################### -->
        <!-- ################################################################################################### -->
        <!-- ################################################################################################### -->

        <section>
            <h2>Shared variables</h2>
            <section data-transition="none none">
                <aside class="notes">
                    Now let's go back to the <b>previous example</b>. The whole point of this code is to <b>share</b> a
                    variable between the <b>Driver</b> and its <b>executor</b>. However, and as we've seen it, this
                    cannot work for the reasons we've mentioned earlier. But actually, it is even <b>worse</b> than that. <br/>

                    Actually, the <b>serialization process</b> we've talked about earlier is done <b>as many time as</b>
                    there are partitions. So if your <b>cluster</b> contains <b>10 nodes</b> with a <b>hundred partitions</b>
                    each, this closure will be <b>serialized</b> and sent through the network a <b>thousand</b> times.<br/><br/>

                    Now if the <b>counter</b> variable is actually a <b>huge object</b>, this may get very <b>inefficient</b>
                    at some point. So the whole problem here is not only about <b>sharing a variable</b>, but also to do
                    it while <b>minimizing IOs</b> and in a <b>safe way</b> in terms of <b>concurrency</b>.

                    In order to resolve this problem, Sparks supports <b>2 types of shared variables</b> : <b>Accumulators</b>
                    and <b>Broadcast variables</b>. The first is designed to send a value from the <b>executors</b> to their
                    <b>driver</b>, while the <b>second one</b> is for the <b>other way around</b>.
                </aside>
                <div style="text-align: left">
                    <pre style="width:100%;"><code class="scala" style="font-size: 25px;">var counter = 0
var rdd = sc.parallelize(data)

rdd.foreach { rddItem ⇒ counter += 1 }
print("Counter value: " + counter)</code></pre>

                    <pre class="fragment" style="width:100%;"><code class="scala" style="font-size: 25px;">val hugeArray = ...
var bigRddWithIndex = ...

bigRddWithIndex.map { rddItem ⇒ hugeArray[rddItem.key] }
...</code></pre>
                </div>
                <p class="fragment">Accumulators and Broadcast variables !</p>
            </section>

            <section data-transition="none none">
                <aside class="notes">
                    Accumulators are overall designed to <b>aggregate</b> values coming from the executors to the
                    driver. In order to make this safely, an accumulator's value is <b>only visible by the driver</b>,
                    and can be <b>updated only by its executors</b>.<br/><br/>

                    Now as you remember, an RDD may be <b>recomputed</b> for different reasons, which means that, if
                    updated inside a <b>transformation</b>, an <b>accumulator</b> may be modified more than required.
                    Actually, Spark <b>guarantees</b> that for a <b>given accumulator</b>, updates are applied <b>only
                    once</b> if they are done inside an <b>action</b>, but this is <b>not true</b> if they are applied inside
                    a <b>transformation</b>. Actually, this feature has been a lot <b>criticized</b>, as it has been found out that
                    in some edge cases, <b>accumulators</b> may be updated more than required even if those <b>updates</b> are done
                    inside an action. Therefore, I strongly suggest you to use them only for <b>debugging purpose</b>, or if
                    they are idempotent.
                </aside>
                <table>
                    <tr>
                        <td style="vertical-align: middle">
                            Accumulators
                        <pre style="width:100%;"><code class="scala" style="font-size: 25px;">val counter = sc.accumulator(0)
var rdd = sc.parallelize(data)

rdd.foreach {
    rddItem ⇒ counter += 1
}
print(counter.value)</code></pre>
                        </td>
                        <td><img style="margin-top:60px; box-shadow: 0; background-color: #FFFFFF; width:100%"
                                 src="images/spark_scala_meetup/accumulators.png"/></td>
                    </tr>
                    <tr>
                        <td colspan="2">Accumulators aggregate values coming from the executors to the driver</td>
                    </tr>
                </table>
            </section>

            <section data-transition="none none">
                <aside class="notes">
                    In a previous example, we were wondering about how it would be possible to share a <b>heavy
                    object</b> without <b>impacting network throughput</b>, or dealing with <b>concurrency issues</b>.
                    Well <b>broadcast variables</b> are exactly designed for this purpose. Basically, a broadcast variable
                    is able to wrap a value and is guaranteed to be <b>sent only once</b> per worker node. This is
                    especially useful if you need to share a <b>lookup table</b> across a cluster's nodes. <br/><br/>

                    In this example, we <b>wrap a big array</b> inside a <b>broadcast variable</b>, and are able to access it by
                    calling the <b>value property</b>. <!--Now no matter if this value is <b>mutable or not</b>, any of its
                    <b>updates</b> won't ever be <b>propagated</b> to the other nodes and will remain <b>local only</b>.<br/><br/>-->

                    Now <b>broadcast variables</b> may be a <b>bottleneck</b> at some point, as serializing and deserializing
                    a data structure can be <b>sometime expensive</b>. So it is important to choose a proper <b>data structure</b>
                    along with a good <b>serialization library</b>. Spark comes by default with <b>Kryo</b> but you can
                    also use other libraries or your own <b>serialization routines</b>.
                </aside>
                <table>
                    <tr>
                        <td style="vertical-align: middle">
                            Broadcast variables
                        <pre style="width:100%;"><code class="scala" style="font-size: 25px;">val bigArray = sc.broadcast(...)
var bigRddWithIndex = ...

bigRddWithIndex.map { e ⇒
  broadcastedArray.value[e.key]
}</code></pre>
                        </td>
                        <td><img style="margin-top:60px; box-shadow: 0; background-color: #FFFFFF; width:100%"
                                 src="images/spark_scala_meetup/broadcast_var.png"/></td>
                    </tr>
                    <tr>
                        <td colspan="2">Broadcast variables propagate a read-only value to all executors</td>
                    </tr>
                </table>
            </section>
        </section>

        <!-- ################################################################################################### -->
        <!-- ################################################################################################### -->
        <!-- ################################################################################################### -->
        <!-- ################################################################################################### -->
        <!-- ################################################################################################### -->



        <!-- ################################################################################################### -->
        <!-- ################################################################################################### -->
        <!-- ################################################################################################### -->
        <!-- ################################################################################################### -->
        <!-- ################################################################################################### -->

        <section data-transition="none none">
            <h2>Efficient Data distribution</h2>
            <section data-transition="none none">
                <aside class="notes">
                    The second thing you should always keep in mind when working with Spark is how your data is
                    distributed over the Spark cluster. A bad distribution may lead to heavy shuffles, unbalanced
                    partitions, bad performances, or even worse, failed jobs.

                    We saw that in Spark, the data is split into multiple partitions which are spread among the
                    executors. We've also seen that the initial number of partition can be provided when loading the
                    data, and that by default Spark uses either its configuration or the one specific to each datasource.

                    Partitions cannot span over several executors, therefore, you may end up having partitions of
                    different size, and their number may change over time depending on the transformations performed.

                    Why do we care? Well the number of partitions processed by Spark at a given moment can have a
                    significant impact on performances and can some time make the difference between a job which
                    completes from one which does not. For these reasons, Spark provides different ways to redistribute
                    the data in order to make it more manageable.

                    Partitioning:
                    * Problems: Skewed data, not enough partitions, too many partitions
                </aside>
               <ul>
                    <li>Data is split into multiple partitions</li>
                    <li>The initial number of partition can be configured</li>
                    <li>Partitions do not span</li>
                    <li>The number of partition can change</li>
                </ul><br/><br/>
                A good data distribution is the key for efficient Spark jobs.
            </section>

            <!--<section data-transition="none none">-->
                <!--<aside class="notes">-->
                    <!--We've already seen this, but I just wanted to give a quick reminder. So the initial number of-->
                    <!--partitions can be passed in parameter whenever you load data into Spark. If not provided, Spark falls-->
                    <!--back on this configuration setting which is set to the number of cores present in your cluster.-->
                <!--</aside>-->
                    <!--<pre><code class="scala"  style="font-size: 25px;">// numSlices is optional-->
<!--val ints: RDD[String] =-->
  <!--sc.parallelize(Seq(1, 2, 3,4), numSlices)-->

<!--// minPartitions is optional-->
<!--val seqFiles: RDD[(Text, Text)] =-->
  <!--sc.sequenceFile[Text, text](input, minPartitions)-->

<!--// minPartitions is optional-->
<!--val rdd3: RDD[String] =-->
  <!--sc.textFile("hdfs://...", minPartitions)</code></pre>-->
                    <!--<pre><code class="scala"  style="font-size: 25px;">spark.default.parallelism</code></pre>-->
            <!--</section>-->

            <section data-transition="none none">
                <aside class="notes">
                    Now as I just told you, you can also define the number of partitions resulting from a transformation
                    by providing it as a second parameter. Some transformations like map do not allow you to do so, mostly
                    because it would not make sense.

                    So as you can see here, we've created an RDD of integers, which is grouped according the function
                    passed to the groupBy function. Overall, this goes back to define a hash function responsible for
                    returning the key of the partition to which an element belongs.

                    In order to display the content of each partition, we use the function foreachPartition which
                    works pretty much like mapPartition. So it takes an iterator which iterates on the partition list.

                    groupBy can also take a second parameter which defines the number of partitions we want to output.
                </aside>
                <pre><code class="scala"  style="font-size: 25px;">val ints = sc.parallelize(Seq(1,2,11,12,21,22))

// use default number of partitions
ints.groupBy((i: Int) ⇒ i / 10)
    .foreachPartition(p => println(p.toList))</code></pre>
<pre><code class="scala"  style="font-size: 25px;">List()
List((0,Seq(1, 2)))
List((1,Seq(11, 12)))
List((2,Seq(21, 22)))</code></pre>

<pre><code class="scala"  style="font-size: 25px;">// distribute in 2 partitions
ints.groupBy((i: Int) ⇒ i / 10, 2)
    .foreachPartition(p => println(p.toList))</code></pre>
<pre><code class="scala"  style="font-size: 25px;">List((1,Seq(11, 12)))
List((0,Seq(1, 2)), (2,Seq(21, 22)))</code></pre>
            </section>
            <section data-transition="none none">
                <aside class="notes">
                    Most of transformations allowing you to set the number of output partitions also allow you to
                    provide a specific partitioner. By default, Spark uses the HashPartitioner, but it also provides
                    a RangePartitioner.

                    The Hashpartitioner simply hashes each value in order to resolve the partition to which the value
                    belongs. The RangePartitioner works a bit differently. First it requires an pair rdd which contains
                    the ranges in which each data item falls in. This pair rdd is then sampled in order to figure an
                    estimation of the ranges which are mostly used, and a RangePartitioner is created. Then we pass this
                    partitioner to the groupBy function and proceed as before.

                    Using a RangePartitioner, you can sometime get a more even distribution of the data. Aside the
                    HashPartitioner and the RangePartitioner, you can also implement your very own custom partitioner
                    by extending the Partitioner interface.
                </aside>
                <pre><code class="scala"  style="font-size: 25px;">val ints = sc.parallelize(Seq(1,2,11,12,21,22))
val partitioner = new HashPartitioner(4)
ints.groupBy((i:Int) => i / 10, partitioner)</code></pre>
<pre><code class="scala"  style="font-size: 25px;">List((0,Seq(1, 2)))
List((1,Seq(11, 12)))
List((2,Seq(21, 22)))
List()</code></pre>
<pre><code class="scala"  style="font-size: 25px;">val ints = sc.parallelize(Seq(8,96,240,400,401,800))
val ranges = ints.map(i => (i / 10, i))
val partitioner = new RangePartitioner(4, ranges))

ints.groupBy((i:Int) => i / 10, ranges)</code></pre>
<pre><code class="scala"  style="font-size: 25px;">List((40,Seq(400, 401)))
List((24,Seq(240)))
List((80,Seq(800)))
List((0,Seq(8)), (9,Seq(96)))</code></pre>
            </section>
            <section data-transition="none none">
                <aside class="notes">
                    You can also pass a partitioner without making any transformation using the partitionBy function.
                    However, whenever you do this, make sure to persist the resulting RDD, as partitionBy forces a
                    shuffle. And in order to prevent doing it every time the rdd is computed, the resulting rdd has to be
                    cached.

                    A couple methods allowing to get some information regarding the data partitioning.

                    Alright, last thing regarding partitioning. You have to be aware that in some cases, an RDD's partitioning
                    information may be lost. This happens, when you use a transformation which may change how the data
                    is distributed. For example, mapping a PairRDD may lead to change its keys. As the keys are used
                    by Spark to partition the data, the resulting RDD has no longer any information about how it is
                    supposed to be partitioned.
                </aside>
                <pre><code class="scala"  style="font-size: 25px;">val partitioneddRdd = ints.partitionBy(partitioner)
partitioneddRdd.cache()</code></pre>

                <pre><code class="scala"  style="font-size: 25px;">rdd.partitioner      // returns an Option[Partitioner]
rdd.getNumPartitions // returns an Int</code></pre>

                <pre><code class="scala"  style="font-size: 25px;">val pairs = sc.parallelize(
   Seq("a" -> 1, "b" -> 2, "c" -> 3)
)
val partitioner = new RangePartitioner(3, pairs)

val rdd1 = pairs.partitionBy(partitioner)
val rdd2 = rdd1.map { case (k,v) => (v,v) }

rdd1.partitioner // returns Some(RangePartitioner@...)
rdd2.partitioner // returns None</code></pre>
            </section>

            <section data-transition="none none">
                <aside class="notes">
                    Finally, you can also explicitly ask Spark to redistribute the data using coalesce and repartition.
                    Overall these two functions consists in changing the number of partitions used to store the data.
                    Coalesce will allow you to reduce this number while repartition can increase or reduce it. The main
                    difference is that coalesce will just regroup the partitions in a smaller amount of partitions,
                    without worrying about how well the data is spread, while repartition garantees the data to be spread
                    as evenly as possible.

                    Now considering what we said so far, could you tell me what kind of transformation are coalesce and
                    repartition?
                </aside>
                <pre><code class="scala"  style="font-size: 25px;">val ints = sc.parallelize(Seq(1,2,3,4,5,6,7,8,9,0), 4)</code></pre>
<pre><code class="scala"  style="font-size: 25px;">List(3, 4, 5)
List(6, 7)
List(1, 2)
List(8, 9, 0)</code></pre>

<pre><code class="scala"  style="font-size: 25px;">ints.coalesce(3)</code></pre>
<pre><code class="scala"  style="font-size: 25px;">List(3, 4, 5)
List(6, 7, 8, 9, 0)
List(1, 2)
</code></pre>

<pre><code class="scala"  style="font-size: 25px;">ints.repartition(3)</code></pre>
<pre><code class="scala"  style="font-size: 25px;">List(5, 7, 8)
List(1, 3, 9)
List(2, 4, 6, 0)
</code></pre>
            </section>

            <section data-transition="none none">
                <aside class="notes">
                    Alright, overall, as we kept mentioning during this talk, data distribution should be one of your
                    primary concern when writing a Spark job. Spark provides different ways to manage this in an optimal
                    way. Now one question you may ask, how many partition should I use to distribute the data. Well
                    a good metric is to take the number of cores available and to multiply it by 2 or 3. The goal is to
                    make the job as parallelizable as possible. You basically don't won't to have too few partitions
                    otherwise some resources of your cluster would be idle. Having too many partitions on the other hand
                    may also be a problem, as this would imply more IO. Finally, sometime partitions may be too heavy to
                    load into an executor's memory, in which case, you would have to trade some IO.

                    So overall, you have to figure out the good balance between parallelism and the size of each
                    partition, along with how data is distributed.
                </aside>
                <ul>
                    <li>Optimal number of partition: number of cores * 2 or 3</li>
                    <li>Too few partition would not take fully advantage of your cluster</li>
                    <li>Too many partition would imply too much IO</li>
                    <li>Keep the balance between memory, parallelism and data distribution</li>
                </ul>
            </section>
        </section>

        <!-- ################################################################################################### -->
        <!-- ################################################################################################### -->
        <!-- ################################################################################################### -->
        <!-- ################################################################################################### -->
        <!-- ################################################################################################### -->

        <section data-transition="none none">
            <h2>Efficient Shuffles</h2>
            <section data-transition="none none">
                <aside class="notes">
                    Alright, as we've mentioned it during this talk, shuffling may be pretty expensive depending on the
                    size of the dataset, so they should be avoided. But on the other hand, they are sometime hard to
                    prevent, so in the next slides we'll cover a couple of techniques allowing to shuffle better, to shuffle
                    less and sometime to completely avoid shuffling.

                    Just as a reminder, what does trigger a shuffle? Long story short, a shuffle happens everytime the
                    data needs to be redistributed over the cluster. It implies data serialization in order to be sent
                    over the network and deserialization once it has been received.

                    One thing we did not mention however, is that shuffles can also be a real bottleneck when a Spark
                    job is evaluated. Remember that a stage is basically created whenever data needs to be redistributed.
                    Stages are sequential and therefore, in order to pass from one stage to the other, Spark must make
                    sure that all the tasks related to a Stage have been done before going forward. So the more
                    shuffle you have, the more stages, the more tasks, the more serialization, the more performance
                    issues.

                    So let's look at some typical scenarios where shuffling is required and think about we could make
                    them efficient or even better not needed.
                </aside>
                <ul>
                    <li>Shuffles may be expansive but cannot be always avoided</li>
                    <li>Shuffles are triggered whenever data needs to be redistributed</li>
                    <li>Shuffles generate stages, which are sequential</li>
                    <li>Shuffles can quickly become a bottleneck</li>
                </ul>
            </section>
            <section data-transition="none none">
                <aside class="notes">
                    Let's say I want to <b>make a join</b> between an RDD containing the people in <b>Canada</b>
                    with one that contains all the canadian <b>provinces/territories</b>.<br/><br/>

                    *** Well, this would result in a <b>very uneven sharding</b> as some provinces like <b>Quebec</b>
                    are much more populated than territories like<b>Nunavut</b>.<br/><br/>

                    Moreover, we would get a <b>very limited level of parallelism</b> as we would only have 13
                    <b>partitions</b>, and adding more <b>machines</b> to get the job done would not change <b>anything</b>.
                </aside>
                <table>
                    <tr>
                        <td><img
                                style="margin-top:30px; margin-right:60px;box-shadow: 0; background-color: #FFFFFF; width:100%"
                                src="images/spark_scala_meetup/shuffle_3.png"/></td>
                        <td style="vertical-align: middle">
                            canadians.join(provinces)
                            <ul class="fragment" style="width:350px; white-space: nowrap;">
                                <li>Uneven sharding</li>
                                <li>Limited parallelism</li>
                            </ul>
                        </td>
                    </tr>
                </table>
            </section>
            <section data-transition="none none">
                <aside class="notes">
                    Actually, a better way to do this is to use a <b>broadcast variable</b> cointaining all the canadian
                    <b>provinces and territories</b>. This variable could be then sent to all <b>the worker nodes</b> in order,
                    which would avoid a <b>shuffle</b>. Now of course, this is only possible if the <b>dataset</b> being sent can
                    <b>fit in the memory</b> of a single node.
                </aside>
                <table>
                    <tr>
                        <td><img
                                style="margin-top:30px; margin-right:60px;box-shadow: 0; background-color: #FFFFFF; width:100%"
                                src="images/spark_scala_meetup/shuffle_4.png"/></td>
                        <td style="width:350px; vertical-align: middle">
                            <ul style=" white-space: nowrap;">
                                <li>Even Sharding</li>
                                <li>No Shuffle Required</li>
                            </ul>
                        </td>
                    </tr>
                </table>
            </section>
            <section data-transition="none none">
                <aside class="notes">
                    Now what if you cannot use a <b>broadcast variable</b> ? what if the data in <b>both RDDs</b> is too
                    big ? Let's say for example that we need to <b>join</b> an <b>RDD</b> containing all the <b>people in Canada</b>
                    with one containing all the <b>people in the world</b>. Joining those <b>RDDs</b> would result in a <b>lot of
                    data shuffled</b> across the <b>network</b> and potentially in a <b>space problem</b> on the destination nodes.
                </aside>
                <table>
                    <tr>
                        <td><img
                                style="margin-top:30px; width:100%; margin-left:10px;box-shadow: 0; background-color: #FFFFFF"
                                src="images/spark_scala_meetup/shuffle_7.png"/></td>
                        <td style="vertical-align: middle">
                            <ul style="width:350px; white-space: nowrap;">
                                <li>Even Sharding</li>
                                <li>Good Parallelism</li>
                                <li>Space may be a problem</li>
                            </ul>
                        </td>
                    </tr>
                </table>
            </section>
            <section data-transition="none none">
                <aside class="notes">
                    So instead of doing this, we could just create a <b>partial RDD</b> using a <b>filter</b>, and <b>reduce</b>
                    the amount of data to be <b>transferred</b>. The whole idea here is to make sure that you <b>transfer</b> only
                    the data that is required by <b>further</b> operations.
                </aside>
                <table>
                    <tr>
                        <td><img
                                style="margin-top:30px; width:100%; margin-left:10px;box-shadow: 0; background-color: #FFFFFF;"
                                src="images/spark_scala_meetup/shuffle_8.png"/></td>
                        <td style="vertical-align: middle;">
                            <ul style="width:350px; white-space: nowrap;">
                                <li>Even Sharding</li>
                                <li>Good Parallelism</li>
                                <li>Less Space required</li>
                                <li>Less Data shuffled</li>
                            </ul>
                        </td>
                    </tr>
                </table>
            </section>
            <section data-transition="none none">
                <aside class="notes">
                    Finally, what about distribution which would en up in having most of the data inside 10% of the
                    partitions available. One solution is to use salting techniques like Salting, Isolation Salting
                    and Isolation Map joins, however we won't get into details here, especially that there is already
                    a very good presentation about this called Top 5 Mistakes When Writing Spark Applications.


                    This may happen with a Pair RDD which contains a majority of items with an
                    identical key.



                    Well one solution is to map the RDD with an additional key which would be guaranteed
                    to be unique, and to process the RDD just like before
                    Key spaces / skwewed data
                </aside>
                <pre><code class="scala"  style="font-size: 25px;">
case class Order(id: UUID, userId: UUID)
case class User(id: UUID)

val orders: RDD[Order] = ???
val users: RDD[User] = ???

val orderByUserId = orders.map(o => (o.userId, o))
val userById = users.map(u => (u.id, u))

orderByUserId.join(userById)

List()
List()
List((7c436268-8ad1-4e1e-be73-4f0ae7ca5c11,(98756234-4219-49fe-a51d-c0ed90f2f2cb,User(7c436268-8ad1-4e1e-be73-4f0ae7ca5c11))),
     (7c436268-8ad1-4e1e-be73-4f0ae7ca5c11,(963d8bd8-ee68-4554-82d1-e656823da320,User(7c436268-8ad1-4e1e-be73-4f0ae7ca5c11))),
     (7c436268-8ad1-4e1e-be73-4f0ae7ca5c11,(d0e11808-3fd3-4402-8619-566ab3b9648e,User(7c436268-8ad1-4e1e-be73-4f0ae7ca5c11))))
List((968c9384-45e1-4496-9c93-d541b32344a3,(a8ac119a-77e1-4985-b4e6-29b5a965f300,User(968c9384-45e1-4496-9c93-d541b32344a3))))

rdd.map { case (key, value) => (scala.util.Random.nextInt(4), (key, value)) }




import java.util.UUID
import java.util.UUID.{randomUUID => randomUUID}

val user = User(java.util.UUID.randomUUID())
val user2 = User(java.util.UUID.randomUUID())

val orders = sc.parallelize(Seq(Order(randomUUID, user.id), Order(randomUUID, user.id), Order(randomUUID, user.id), Order(randomUUID, user2.id)))
val users = sc.parallelize(Seq(user, user2))


List(
  (8ba22a2c-428b-4477-bb08-ea8f4ec05d4f,(Order(0046088a-73e2-47ec-91fc-df95982f1900,8ba22a2c-428b-4477-bb08-ea8f4ec05d4f),User(8ba22a2c-428b-4477-bb08-ea8f4ec05d4f))),
  (8ba22a2c-428b-4477-bb08-ea8f4ec05d4f,(Order(962c2660-0946-44f6-8b1d-abc69f9b4c7d,8ba22a2c-428b-4477-bb08-ea8f4ec05d4f),User(8ba22a2c-428b-4477-bb08-ea8f4ec05d4f))),
  (8ba22a2c-428b-4477-bb08-ea8f4ec05d4f,(Order(dd235d08-ba29-4cd1-b4eb-8485202459c3,8ba22a2c-428b-4477-bb08-ea8f4ec05d4f),User(8ba22a2c-428b-4477-bb08-ea8f4ec05d4f))),
  (dc8e8463-64a3-4032-943f-06bdbcba1e17,(Order(4a1e69cc-a998-4c5a-98e8-1052a15c20a8,dc8e8463-64a3-4032-943f-06bdbcba1e17),User(dc8e8463-64a3-4032-943f-06bdbcba1e17))))


                </code></pre>

            </section>
            <section data-transition="none none">
                <aside class="notes">
                    Colocation / CoPartitioning
                </aside>
            </section>
        </section>


            <section data-transition="none none">
                <aside class="notes">
                    Fault Tolerance with Shuffling
                </aside>
                <div style="display:inline-block;vertical-align: top">
                    <div class="vertical-box-set" style="display: inline-block;vertical-align: top">
                        <div class="box red fragment background green" data-fragment-index="5"></div>
                        <div class="box red fragment background green" data-fragment-index="5"></div>
                        <div class="box red fragment background green" data-fragment-index="5"></div>
                    </div>
                    <div style="margin:25px 25px 0 25px;display:inline-block;vertical-align: top">
                        <div class="right"><div class="arrow"></div></div><br/>
                        <div class="right"><div class="arrow"></div></div><br/>
                        <div class="right"><div class="arrow"></div></div>
                    </div>
                    <div class="vertical-box-set" style="display: inline-block;vertical-align: top">
                        <div class="box red fragment background green" data-fragment-index="5"></div>
                        <div class="box red fragment background green" data-fragment-index="5"></div>
                        <div class="box red fragment background green" data-fragment-index="5"></div>
                    </div>
                    <div style="margin:25px 25px 0 25px;display: inline-block;vertical-align: top">
                        <div class="up-r"><div class="arrow"></div></div><br/>
                        <div class="right"><div class="arrow"></div></div><br/>
                        <div class="down-r"><div class="arrow"></div></div>
                    </div>
                    <div style="margin-top:50px;"></div>
                    <div class="vertical-box-set" style="display: inline-block;vertical-align: top">
                        <div class="fragment flip" data-fragment-index="3">
                            <div class="box blue fragment background green" data-fragment-index="2"></div>
                        </div>
                        <div class="fragment unflip" data-fragment-index="3">
                            <div class="box blue fragment background green" data-fragment-index="5"></div>
                        </div>
                        <div class="box blue fragment background green" data-fragment-index="5"></div>
                        <div class="box blue fragment background green" data-fragment-index="5"></div>
                    </div>
                    <div style="margin:25px 25px 0 25px;display:inline-block;vertical-align: top">
                        <div class="right"><div class="arrow"></div></div><br/>
                        <div class="right"><div class="arrow"></div></div><br/>
                        <div class="right"><div class="arrow"></div></div>
                    </div>
                    <div class="vertical-box-set" style="display:inline-block;vertical-align: top">
                        <div class="fragment flip" data-fragment-index="3">
                            <div class="box blue fragment background green" data-fragment-index="1"></div>
                        </div>
                        <div class="fragment unflip" data-fragment-index="3">
                            <div class="box blue fragment background green" data-fragment-index="5"></div>
                        </div>
                        <div class="box blue fragment background green" data-fragment-index="5"></div>
                        <div class="box blue fragment background green" data-fragment-index="5"></div>
                    </div>
                    <div style="margin:25px 25px 0 25px;display:inline-block;vertical-align: top">
                        <div class="up-r"><div class="arrow"></div></div><br/>
                        <div class="right"><div class="arrow"></div></div><br/>
                        <div class="down-r"><div class="arrow"></div></div>
                    </div>
                </div>
                <div style="display:inline-block;vertical-align: top;margin-top:42px;">
                    <div class="vertical-box-set" style="display: inline-block;vertical-align: middle">
                        <div class="box purple"></div>
                        <div class="box purple"></div>
                        <div class="box purple fragment background green" data-fragment-index="4"></div>
                        <div class="box purple"></div>
                        <div class="box purple"></div>
                        <div class="box purple"></div>
                    </div>
                </div>
            </section>



        <section data-transition="none none">
            <h2>Conclusion</h2>
        </section>



<!doctype html>
<html lang="en">

<head>
    <meta charset="utf-8">

    <title>Introduction to Spark - by Francis Toth</title>

    <meta name="description" content="A framework for easily creating beautiful presentations using HTML">
    <meta name="author" content="Hakim El Hattab">

    <meta name="apple-mobile-web-app-capable" content="yes" />
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />

    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">

    <link rel="stylesheet" href="css/reveal.css">
    <link rel="stylesheet" href="css/custom.css">
    <link rel="stylesheet" href="css/theme/francis.css" id="theme">

    <!-- Code syntax highlighting -->
    <link rel="stylesheet" href="lib/css/zenburn.css">

    <!-- Printing and PDF exports -->
    <script>
        var link = document.createElement( 'link' );
        link.rel = 'stylesheet';
        link.type = 'text/css';
        link.href = window.location.search.match( /print-pdf/gi ) ? 'css/print/pdf.css' : 'css/print/paper.css';
        document.getElementsByTagName( 'head' )[0].appendChild( link );
    </script>

    <!--[if lt IE 9]>
    <script src="lib/js/html5shiv.js"></script>
    <![endif]-->
</head>

<body>


<aside style="display: block; position: fixed; bottom: 10px; right: 10px; z-index: 30;">
    <a href="http://www.yoppworks.com"><img src="images/spark_scala_meetup/spark_scala_meetup/yoppworks-logo-landscape-large-text-dark-bg.png"
                                            height="30"></a>
</aside>

<div class="reveal">
    <div class="slides">
        <section>
            <aside class="notes">
                Hi everyone, my name is Francis, and today I'd like to give you a brief introduction to Spark.<br/><br/>

                Big Data is overall about two main challenges. First it's about storing data that cannot fit on a
                single node, and secondly it's about figuring out how to compute it.<br/><br/>

                In order to overcome those challenges, Internet giants, such as Google or Amazon, decided to create
                new tools specially designed to deal with large amount of data. And this how technologies such as
                MapReduce and Hadoop were created.<br/><br/>

                However, most of those tools were designed for a specific problem. And because you cannot use the same
                tool for all kind of jobs, some projects were started in order to provide a better answer to the
                Big Data problem.

                This is where Sparks come from.
            </aside>
            <h2 style="display: inline-block;"><img
                    style="vertical-align: text-bottom; margin: 18px 0; border: 0; background-color: #243044"
                    src="images/spark_scala_meetup/spark_logo.transparent.png"/></h2>
        </section>
        <section>
            <h2>Quick overview</h2>
            <aside class="notes">
                So Spark was <b>founded in 2009</b> at the AmpLab in California.<br/><br/>

                ***It is a <b>cluster computing platform</b> which extends the traditional <b>MapReduce model</b>,
                and which is <b>designed to support</b> various types of computations on large <b>dataset</b>.
                Originally, Spark intended to address <b>common problems</b> people were facing while doing Machine
                learning. Therefore, it is highly <b>optimized for iterative and cyclic</b> operations on the same
                set of data.<br/><br/>

                A data pipeline is nothing more than a <b>sequence of steps</b>, each being responsible for <b>processing</b>
                the data, and for <b>passing</b> the result over to the next step. With <b>Hadoop</b>, this is usually
                done by <b>chaining</b> different frameworks with each others and by storing the data resulting from
                each step on the disk.<br/><br/>

                This approach has actually <b>two main issues</b>, first, in terms of <b>performances</b>, as each
                step requires a certain amount of IO to store/read the intermediary data, and secondly in terms of
                <b>maintenance</b>, as it requires to juggle with sometime different languages, paradigms or tools.

                <b>Spark</b> addresses this, by providing a unified API. The main idea is that every Spark's
                library is based on the same core concepts which makes the switch between one library to another
                almost without any problem.<br/><br/>

                ***Compared to <b>traditional MapReduce engines</b>, benchmarks have shown a <b>significant increase</b>
                in performances which have to be qualified however, as a benchmark may show different results
                depending on the context. Overall, you can expect Spark to be twice to three times faster than Hadoop.
                These performances can be actually explained by the fact that Spark makes a heavy use of caching in the
                contrary of Hadoop which is mostly disk dependent.<br/><br/>

                Finally, Spark is today <b>one of the most popular</b> and <b>active</b> project regarding<b>large-scale
                data processing</b>. It is used by <b>Amazon</b>, <b>Ebay</b>, and <b>Netflix</b> among others, and
                has seen its number of <b>contributors</b> growing tremendously since it's hosted by <b>Apache</b>.
            </aside>
            <ul>
                <li>Created in 2009</li>
                <li class="fragment">Large-scale data processing engine</li>
                <li class="fragment">Unified framework</li>
                <li class="fragment">Extremely effective</li>
                <li class="fragment">Popular</li>
            </ul>
        </section>

        <section>
            <h2>Agenda</h2>
            <aside class="notes">
                So today, we'll mainly focus on <b>Spark's core aspects</b>, present its <b>execution model</b> and the
                ways it deals with <b>large-scale data computation</b>. If we have some time, we'll then talk about its
                higher level API's.

                <!--<b>features</b>-->
                <!--it supports. Then we'll briefly cover some of its higher level API's with Spark-SQL, a lib responsible for-->
                <!--doing interactive queries and Spark-Streaming which deals with continuous and close-to-real-time-->
                <!--processing. Unfortunately, we won't have time to cover MLib which is a lib dedicated to machine-->
                <!--learning, and GraphX, which is Spark's graph processing lib.-->
            </aside>
            <div style="display: block; text-align: center">
                <img style="border: 0; background-color: #FFFFFF" src="images/spark_scala_meetup/spark-stack.png"/>
            </div>
        </section>

        <section>
            <h2>Spark Architecture</h2>
            <aside class="notes">
                In terms of architecture, a Spark application relies on three components. First, the driver. The driver is
                the central coordinator of a Spark program. It is responsible for defining distributed dataset along with
                the computations which have to run on top of it<br/><br/>

                In order to perform those <b>tasks</b>, the driver relies on a certain amount of <b>executors</b>. An executor is
                responsible for running the tasks requested by the <b>driver</b>. It provides in-memory storage used by Spark
                for caching purpose along with <b>cores</b> used to run the computation requested by the driver.<br/><br/>

                <!--The Driver and its executors have all their own Java processes allowing a Spark application to be completely-->
                <!--isolated from one another. On the other hand, this also means that data cannot be shared between each-->
                <!--application except by writing it to an external storage.<br/><br/>-->

                What about <b>physical resources</b> now? Those have to come from somewhere right? Well, this is the
                the cluster manager's job. Basically, the cluster manager is responsible for providing the physical
                resources required by a <b>Spark application</b> in order to run its computations.<br/><br/>

                <!--You can think about the cluster manager as the one defining how a job is executed while the driver takes-->
                <!--only care about the what. -->

                So depending on the cluster manager, you may end up with running the Driver
                and the executors on the same laptop or on a distributed cluster composed of several worker nodes like
                it is usually the case.<br/><br/>

                In Spark, the cluster manager is actually a completely <b>independent plugin</b> which does not rely on a
                specific implementation. This allows Spark to run transparently on top of Yarn, Mesos, or its built-in
                standalone cluster manager.<br/><br/>
            </aside>
            <div class="fragment" data-fragment-index="1" style="border: 1px solid white; width: 20%; height: 100px;margin-left: 40%; margin-right: 40%;line-height: 100px;">
                Driver
            </div>
            <div class="fragment" data-fragment-index="5">
                <div class="down" style="margin: 20px 0 10px 0;"><div class="arrow"></div></div>
                <div style="border: 1px solid white; width: 20%; height: 100px;margin-left: 40%; margin-right: 40%">Cluster<br/>Manager</div>
                <div style="margin: 20px 0 10px 0;">
                    <div class="down-l"><div class="arrow"></div></div>
                    <div class="down" style="margin: 0 30px 0 30px;"><div class="arrow"></div></div>
                    <div class="down-r"><div class="arrow"></div></div>
                </div>
            </div>
            <div class="fragment" data-fragment-index="2" style="display: inline-block;">
                <div>
                    <div><span style="background-color: #243044">&nbsp;Executor&nbsp;</span></div>
                    <div style="border: 3px solid #5C97D6;padding: 10px 10px 0 10px;position: relative; z-index: -1;top:-20px;">
                        <div class="fragment" data-fragment-index="4" style="display:inline-block;margin-top:10px;">
                            <div class="partition-block blue" style="width:30px; height:30px;">&nbsp;</div>
                            <div class="partition-block blue" style="width:30px; height:30px;">&nbsp;</div><br/>
                            <div class="partition-block blue" style="width:30px; height:30px;">&nbsp;</div>
                            <div class="partition-block blue" style="width:30px; height:30px;">&nbsp;</div>
                        </div>
                        <div class="fragment" data-fragment-index="3" style="display:inline-block;margin-top:10px;vertical-align: top;border: 1px solid white;padding: 5px;line-height: 65px;">Cache</div>
                    </div>
                </div>
                <div class="fragment" data-fragment-index="6">
                    <div class="line horizontal"></div>
                    <div><span style="background-color: #243044">&nbsp;Node 1&nbsp;</span></div>
                </div>
            </div>
            <div class="fragment" data-fragment-index="2" style="display: inline-block;">
                <div style="display: inline-block;">
                    <div><span style="background-color: #243044">&nbsp;Executor&nbsp;</span></div>
                    <div style="border: 3px solid #5C97D6;padding: 10px 10px 0 10px;position: relative; z-index: -1;top:-20px;">
                        <div class="fragment" data-fragment-index="4" style="display:inline-block;margin-top:10px;">
                            <div class="partition-block blue" style="width:30px; height:30px;">&nbsp;</div>
                            <div class="partition-block blue" style="width:30px; height:30px;">&nbsp;</div><br/>
                            <div class="partition-block blue" style="width:30px; height:30px;">&nbsp;</div>
                            <div class="partition-block blue" style="width:30px; height:30px;">&nbsp;</div>
                        </div>
                        <div class="fragment" data-fragment-index="3" style="display:inline-block;margin-top:10px;vertical-align: top;border: 1px solid white;padding: 5px;line-height: 65px;">Cache</div>
                    </div>
                </div>
                <div style="display: inline-block;">
                    <div><span style="background-color: #243044">&nbsp;Executor&nbsp;</span></div>
                    <div style="border: 3px solid #5C97D6;padding: 10px 10px 0 10px;position: relative; z-index: -1;top:-20px;">
                        <div class="fragment" data-fragment-index="4" style="display:inline-block;margin-top:10px;">
                            <div class="partition-block blue" style="width:30px; height:30px;">&nbsp;</div>
                            <div class="partition-block blue" style="width:30px; height:30px;">&nbsp;</div><br/>
                            <div class="partition-block blue" style="width:30px; height:30px;">&nbsp;</div>
                            <div class="partition-block blue" style="width:30px; height:30px;">&nbsp;</div>
                        </div>
                        <div class="fragment" data-fragment-index="3" style="display:inline-block;margin-top:10px;vertical-align: top;border: 1px solid white;padding: 5px;line-height: 65px;">Cache</div>
                    </div>
                </div>
                <div class="fragment" data-fragment-index="6">
                    <div class="line horizontal"></div>
                    <div><span style="background-color: #243044">&nbsp;Node 2&nbsp;</span></div>
                </div>
            </div>
        </section>

        <section>
            <h2>Resilient Distributed Dataset</h2>
            <section data-transition="none none">
                <aside class="notes">
                    The core fundamental of Spark is the <b>RDD</b>. RDD stands for Resilient Distributed Dataset and is the
                    <b>main entry point</b> of a Spark job. The name may be a bit <b>misleading</b>, as an RDD is not a dataset on its
                    own. It's actually an <b>interface</b> to your data allowing you to define <b>computations</b> on top of it in a
                    transparent way.<br/><br/>

                    Concretely, an RDD represents the data as a <b>collection of partitions</b> distributed among the Spark cluster. Every time a
                    computation is requested, it is sent by the <b>Driver</b> to each executor, in order to process each
                    partition with a certain level of parallelism.<br/><br/>

                    Now why <b>resilient</b>? Well, Spark provides some mechanism allowing to replay a <b>computation</b> if a
                    partition has been lost. But we'll cover this a bit later.
                </aside>

                <div style="display:table;margin: auto">
                    <div style="display:table-cell;vertical-align: middle;width:100px;">
                         <div class="rectangle blue-border">
                            <div class="child blue-border"><div class="child blue-border" style="line-height:70px;"><span style="font-size: 30px;">Data</span></div></div>
                        </div>
                    </div>
                    <div style="display:table-cell;vertical-align: middle;width:150px;">
                        <div style="margin-top:50px;" class="up-r"><div class="arrow"></div></div><br/>
                        <div class="right"><div class="arrow"></div></div><br/>
                        <div class="down-r"><div class="arrow"></div></div>
                    </div>
                    <div style="display:table-cell;vertical-align: middle;">
                        <div class="executor">
                            <div class="title"><span>&nbsp;Executor&nbsp;</span></div>
                            <div class="body">
                                <div class="partition-list">
                                    <div class="block blue">&nbsp;</div>
                                    <div class="block blue" style="opacity: 0.7;">&nbsp;</div>
                                    <div class="block blue" style="opacity: 0.2;">&nbsp;</div>
                                    <div class="block blue" style="background-color: #243044">&nbsp;</div>
                                </div>
                            </div>
                        </div>
                        <div class="executor">
                            <div class="title"><span>&nbsp;Executor&nbsp;</span></div>
                            <div class="body">
                                <div class="partition-list">
                                    <div class="block blue">&nbsp;</div>
                                    <div class="block blue" style="opacity: 0.7;">&nbsp;</div>
                                    <div class="block blue" style="opacity: 0.2;">&nbsp;</div>
                                    <div class="block hidden">&nbsp;</div>
                                </div>
                                <div class="partition-list">
                                    <div class="block blue">&nbsp;</div>
                                    <div class="block blue" style="opacity: 0.7;">&nbsp;</div>
                                    <div class="block hidden">&nbsp;</div>
                                    <div class="block hidden">&nbsp;</div>
                                </div>
                            </div>
                        </div>
                    </div>
                    <div style="display:table-cell;vertical-align: middle;">
                        <div class="line vertical" style="height: 300px;margin-left: 50px;"></div>
                    </div>
                    <div style="display:table-cell;vertical-align: middle;">
                        <span style="margin-left: 30px">RDD</span>
                    </div>
                </div>
            </section>
            <section data-transition="none none">
                <aside class="notes">
                    An RDD can be created using several methods. Overall, Spark can <b>interface</b> with a
                    wide variety of <b>distributed storage</b> including <b>HDFS</b>, <b>OpenStack Swift</b>, <b>Cassandra</b>,
                    <b>S3</b>, or even <b>custom solutions</b>. You can also load simple scala collections which is
                    great for testing purpose.<br/><br/>

                    As you can notice, each data loading method takes an <b>additional parameter</b> defining in how many
                    <b>partitions</b> the data should be split into <b>once loaded</b>. If it is not provided, Spark will
                    fall back on some <b>default values</b> either defined in its configuration or by the <b>datasource</b> itself. For example,
                    in the case of HDFS, Spark will create a single partition for each input split. A simple Scala
                    collection on the other hand would be split by default into as many partitions as there are cores
                    available in the cluster.
                </aside>
                    <pre><code class="scala" data-trim data-noescape style="font-size: 1.3em; line-height: 1.2em">
/* sc stands for Spark's context */

// numSlices is optional
val ints: RDD[String] =
  sc.parallelize(Seq(1, 2, 3,4), numSlices)

// minPartitions is optional
val seqFiles: RDD[(Text, Text)] =
  sc.sequenceFile[Text, text](input, minPartitions)

// minPartitions is optional
val rdd3: RDD[String] =
  sc.textFile("hdfs://...", minPartitions)
                    </code></pre>
            </section>
        </section>
        <section>
            <h2>Transformation & Actions</h2>
            <section data-transition="none none">
                <aside class="notes">
                    Overall, an <b>RDD</b> supports <b>two types</b> of operation. <b>Transformations</b>, which create a new RDD
                    from an <b>existing</b> one, and <b>actions</b> which perform a computation on an RDD's dataset.
                    Most common <b>transformations</b> are <b>map</b>, <b>filter</b> or <b>flatMap</b>, while <b>actions</b>
                    are more about operations consisting in evaluating, <b>collecting</b>, <b>counting</b>, or <b>sampling</b> the data.
                    Actions will be typically responsible for <b>persisting</b> the result of a computation, or sending it back
                    to the <b>driver</b>.
                </aside>
                <div style="margin-top: 80px">
                    Transformations
                    <pre><code class="scala" style="font-size: 25px;">val wc: RDD[(String, Int)] = rdd
    .flatMap(_.split(" "))
    .map((_, 1))
    .reduceByKey(_ + _)</code></pre>

                    Actions
                    <pre><code class="scala" style="font-size: 25px;">
// Retrieves the data of each partition
val result: Seq[(String, Int)] = wc.collect()

// Writes the data in text files
val result: Unit = wc.saveAsTextFile("...")

// Counts the number of element in an RDD
val result: Long = wc.count()

// Retrieves the first four elements of an RDD
val result: Seq[(String, Int)] = wc.take(4)
                    </code></pre>
                </div>
            </section>
            <section data-transition="none none">
                <aside class="notes">
                    One very interesting aspect of <b>data loading</b> and <b>transformations</b>, is that they are <b>lazily
                    evaluated</b>. Actually, as long as <b>no action has been performed</b>, Spark may not have brought
                    data or computed <b>anything</b> at all. Each transformation actually creates a new RDD which
                    <b>maintains a pointer to its ancestors</b> along with the metadata regarding its relationship with
                    them. In <b>Spark</b> terminology, this is what we call the <b>lineage</b> of an RDD. Therefore
                    defining an RDD goes back to define a <b>specification</b> of what an executor should do in order to
                    compute a result.<br/><br/>

                    Now, concretely, an RDD's <b>lineage</b> is stored in an <b>directed acyclic graph</b> which has two
                    main purposes. First, it allows the <b>recomputing</b> of an RDD which data is lost or not available,
                    and <b>Secondly</b>, it is used by Spark to figure out an <b>optimal execution plan</b> we'll talk
                    about in a bit.
                </aside>
                <div style="display: block; text-align: center; margin-top:80px">
                    Lineage<br/>
                    <div style="display: inline-block; width:290px;padding:10px;margin: 10px;font-size: 35px;">
                        <div style="margin:10px;padding:5px;border: 1px solid white">textFile(...)</div>
                        <div class="up"><div class="arrow"></div></div>
                        <div style="margin:10px;padding:5px;border: 1px solid white">.filter(...)</div>
                        <div class="up"><div class="arrow"></div></div>
                        <div style="margin:10px;padding:5px;border: 1px solid white">.map(...)</div>
                        <div class="up"><div class="arrow"></div></div>
                        <div style="margin:10px;padding:5px;border: 1px solid white">.reduceByKey(...)</div>
                    </div>
                    <div style="display: inline-block; width:290px;font-size: 35px;">
                        <div style="margin:10px;padding:5px;border: 1px solid white">HadoopRDD</div>
                        <div class="up"><div class="arrow"></div></div>
                        <div style="margin:10px;padding:5px;border: 1px solid white">FilteredRDD</div>
                        <div class="up"><div class="arrow"></div></div>
                        <div style="margin:10px;padding:5px;border: 1px solid white">MappedRDD</div>
                        <div class="up"><div class="arrow"></div></div>
                        <div style="margin:10px;padding:5px;border: 1px solid white">ShuffledRDD</div>
                    </div>
                </div>
            </section>
            <section data-transition="none none">
                <aside class="notes">
                    Alright, let's quickly look at how a <b>transformation is applied</b>. So as I told you, whenever a computation
                    is needed, the <b>driver</b> sends it to each executor which starts computing <b>as many partition</b> as they have
                    cores available.<br/><br/>

                    Take the <b>map function</b> for example. It takes a partition, process each of its elements, and then
                    store the result in a new partition. Another example is <b>union</b>. Union takes two partition, and merges
                    them into one single partition. The <b>operations</b> involved by these transformations, are all
                    performed locally on <b>each executor</b> without involving any data movement.<br/><br/>

                    Now some transformations may have to <b>access to partitions stored on different machines</b> For
                    example, <b>groupByKey</b> takes an RDD of key value pairs and <b>regroup</b> all the pairs having the same key in
                    the same partition. As those pairs may be stored on <b>different executors</b>, the data has to be
                    re-distributed all over the cluster, in order to get <b>all the pairs</b> with the same key at the same
                    location.<br/><br/>

                    Long story short, some transformations imply <b>data redistribution</b> and some others not. In Spark terminology,
                    the operation consisting in <b>redistributing</b> the data is referred to as a <b>shuffle</b>. Transformations which
                    do not require a shuffle are called <b>narrow transformations</b> while those requiring one are referred
                    to as <b>wide transformations</b>.
                </aside>
                <div style="display:table;margin: 100px 0 auto 100px;">
                    <div style="display:table-cell;vertical-align: middle; ">
                        <div class="executor fragment" data-fragment-index="1" style="margin-left:90px;margin-right: 50px;">
                            <div class="body" style="border: 0; display: inline-block; vertical-align: middle">
                                <div class="partition-list">
                                    <div class="block blue">&nbsp;</div>
                                    <div class="block blue" style="opacity: 0.7;">&nbsp;</div>
                                    <div class="block blue" style="opacity: 0.2;">&nbsp;</div>
                                    <div class="block blue">&nbsp;</div>
                                </div>
                            </div>
                            <div style="display: inline-block; margin: 0 5px 22px 5px;vertical-align: middle">
                                <div class="right" ><div class="arrow"></div></div><br/>
                            </div>
                            <div class="body" style="border: 0; display: inline-block;vertical-align: middle">
                                <div class="partition-list" style="vertical-align: middle">
                                    <div class="block red">&nbsp;</div>
                                    <div class="block red" style="opacity: 0.7;">&nbsp;</div>
                                    <div class="block red" style="opacity: 0.2;">&nbsp;</div>
                                    <div class="block red">&nbsp;</div>
                                </div>
                            </div>
                        </div>
                        <div class="executor fragment" data-fragment-index="2" style="margin-left:90px;margin-right: 50px;">
                            <div class="body" style="border: 0; display: inline-block; vertical-align: middle">
                                <div class="partition-list">
                                    <div class="block blue">&nbsp;</div>
                                    <div class="block blue" style="opacity: 0.7;">&nbsp;</div>
                                    <div class="block blue" style="opacity: 0.2;">&nbsp;</div>
                                    <div class="block blue">&nbsp;</div>
                                </div>
                                <div class="partition-list">
                                    <div class="block red">&nbsp;</div>
                                    <div class="block red" style="opacity: 0.7;">&nbsp;</div>
                                    <div class="block red" style="opacity: 0.2;">&nbsp;</div>
                                    <div class="block red">&nbsp;</div>
                                </div>
                            </div>
                            <div style="display: inline-block; margin: 0px 5px 22px 5px;vertical-align: middle">
                                <div class="down-r" ><div class="arrow"></div></div><br/>
                                <div class="up-r" ><div class="arrow"></div></div>
                            </div>
                            <div class="body" style="border: 0; display: inline-block;vertical-align: middle">
                                <div class="partition-list" style="vertical-align: middle">
                                    <div class="block blue">&nbsp;</div>
                                    <div class="block red" style="opacity: 0.7;">&nbsp;</div>
                                    <div class="block blue" style="opacity: 0.7;">&nbsp;</div>
                                    <div class="block">...</div>
                                </div>
                            </div>
                        </div>
                    </div>
                    <div class="fragment" data-fragment-index="1" style="display:table-cell;vertical-align: middle;padding: 20px;border-left: 3px solid white">
                        <ul style="margin-left:50px;">
                            <li>map</li>
                            <li>flatMap</li>
                            <li class="fragment" data-fragment-index="2">union</li>
                            <li class="fragment" data-fragment-index="2">...</li>
                        </ul>
                    </div>
                </div>
                <div style="height:30px;"></div>
                <div style="display:table;margin: auto 0 auto 100px;">
                    <div style="display:table-cell;vertical-align: middle">
                        <div class="executor fragment" data-fragment-index="3" style="margin-left:90px;margin-right:50px;">
                            <div class="body" style="border: 0; display: inline-block; vertical-align: middle">
                                <div class="partition-list">
                                    <div class="block blue">&nbsp;</div>
                                    <div class="block blue" style="opacity: 0.7;">&nbsp;</div>
                                    <div class="block blue" style="opacity: 0.2;">&nbsp;</div>
                                    <div class="block blue">&nbsp;</div>
                                </div>
                                <div class="partition-list">
                                    <div class="block red">&nbsp;</div>
                                    <div class="block red" style="opacity: 0.7;">&nbsp;</div>
                                    <div class="block red" style="opacity: 0.2;">&nbsp;</div>
                                    <div class="block red">&nbsp;</div>
                                </div>
                            </div>
                            <div style="display: inline-block; margin: 0 5px 30px 5px;vertical-align: middle">
                                <div class="up-r" ><div class="arrow"></div></div><br/>
                                <div class="right" ><div class="arrow"></div></div><br/>
                                <div class="down-r" ><div class="arrow"></div></div><br/>
                            </div>
                            <div class="body" style="border: 0; display: inline-block;vertical-align: middle">
                                <div class="partition-list" style="vertical-align: middle">
                                    <div class="block blue">&nbsp;</div>
                                    <div class="block blue" style="opacity: 0.7;">&nbsp;</div>
                                    <div class="block red" style="opacity: 0.2;">&nbsp;</div>
                                    <div class="block red">&nbsp;</div>
                                </div>
                                <div class="partition-list" style="vertical-align: middle">
                                    <div class="block blue" style="opacity: 0.7;">&nbsp;</div>
                                    <div class="block red" style="opacity: 0.2;">&nbsp;</div>
                                    <div class="block red">&nbsp;</div>
                                    <div class="block blue" style="opacity: 0.7;">&nbsp;</div>
                                </div>
                                <div class="partition-list" style="vertical-align: middle">
                                    <div class="block" style="width:120px;">...</div>
                                </div>
                            </div>
                        </div>
                    </div>
                    <div class="fragment" data-fragment-index="3" style="display:table-cell;vertical-align: middle;padding: 20px;border-left: 3px solid white">
                        <ul style="margin-left:50px;">
                            <li>reduceByKey</li>
                            <li>groupByKey</li>
                            <li>join*</li>
                            <li>...</li>
                        </ul>
                    </div>
                </div>
            </section>

            <section data-transition="none none">
                <aside class="notes">
                    What does that imply <b>concretely</b>? Well a <b>narrow transformation</b> can be applied on each partition
                    without involving any <b>data movement</b>. However, when you get into the second case, nothing is that
                    sure anymore. <b>Data needs to be moved</b>, and therefore has to be <b>serialized first</b>, sent over the network,
                    and <b>deserialized</b> once received.<br/><br/>

                    As you can guess, this may be a pretty expensive operation especially if the data to shuffle reaches
                    a certain size. Actually we'll see that this is just the tip of the iceberg.
                </aside>
                <div style="display:table;margin: auto">
                    <div style="display:table-cell;vertical-align: middle;width:100px;">
                        <div class="rectangle blue-border">
                            <div class="child blue-border"><div class="child blue-border" style="line-height:70px;"><span style="font-size: 30px;">Data</span></div></div>
                        </div>
                    </div>
                    <div style="display:table-cell;vertical-align: middle;width:150px;">
                        <div style="margin-top:50px;" class="up-r"><div class="arrow"></div></div><br/>
                        <div class="right"><div class="arrow"></div></div><br/>
                        <div class="down-r"><div class="arrow"></div></div>
                    </div>
                    <div style="display:table-cell;vertical-align: middle;">
                        <div class="executor" style="margin-top:30px;">
                            <div class="title"><span>&nbsp;Executor&nbsp;</span></div>
                            <div class="body">
                                <div class="partition-list" style="display: inline-block">
                                    <div class="block blue">&nbsp;</div>
                                    <div class="block blue" style="opacity: 0.7;">&nbsp;</div>
                                    <div class="block blue" style="opacity: 0.2;">&nbsp;</div>
                                </div>
                                <div class="right" style="display: inline-block"><div class="arrow"></div></div>
                                <div class="partition-list" style="display: inline-block">
                                    <div class="block red">&nbsp;</div>
                                    <div class="block red" style="opacity: 0.7;">&nbsp;</div>
                                    <div class="block red" style="opacity: 0.2;">&nbsp;</div>
                                </div>
                            </div>
                        </div>
                        <div class="executor">
                            <div class="title"><span>&nbsp;Executor&nbsp;</span></div>
                            <div class="body">
                                <div class="partition-list" style="display: inline-block">
                                    <div class="block blue">&nbsp;</div>
                                    <div class="block blue" style="opacity: 0.7;">&nbsp;</div>
                                    <div class="block blue" style="opacity: 0.2;">&nbsp;</div>
                                </div>
                                <div class="right" style="display: inline-block"><div class="arrow"></div></div>
                                <div class="partition-list" style="display: inline-block">
                                    <div class="block green">&nbsp;</div>
                                    <div class="block green" style="opacity: 0.7;">&nbsp;</div>
                                    <div class="block green" style="opacity: 0.2;">&nbsp;</div>
                                </div>
                            </div>
                        </div>
                    </div>
                    <div class="fragment" data-fragment-index="2" style="display:table-cell;width:70px;vertical-align: middle;">
                        <div class="up-r" style="margin-top:50px;"  ><div class="arrow"></div></div><br/>
                        <div class="right" ><div class="arrow"></div></div><br/>
                        <div class="down-r"><div class="arrow"></div></div>
                    </div>
                    <div class="fragment" data-fragment-index="2" style="display:table-cell;vertical-align: middle;">
                        <div class="executor" style="margin-top:30px;">
                            <div class="title"><span>&nbsp;Executor&nbsp;</span></div>
                            <div class="body">
                                <div class="partition-list" style="display: inline-block">
                                    <div class="block green">&nbsp;</div>
                                    <div class="block red" style="opacity: 0.7;">&nbsp;</div>
                                </div>
                            </div>
                        </div>
                        <div class="executor">
                            <div class="title"><span>&nbsp;Executor&nbsp;</span></div>
                            <div class="body">
                                <div class="partition-list" style="display: inline-block">
                                    <div class="block green">&nbsp;</div>
                                    <div class="block green" style="opacity: 0.2;">&nbsp;</div>
                                </div>
                                <div class="partition-list" style="display: inline-block">
                                    <div class="block red" style="opacity: 0.7;">&nbsp;</div>
                                    <div class="block red" style="opacity: 0.2;">&nbsp;</div>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </section>
            <section data-transition="none none">
                <aside class="notes">
                    Let's look at some code now and see what part of Spark program runs on the Driver and on its
                    executors.<br/><br/>

                    Remember that every transformation is executed on the executors. Therefore, when you read a Spark
                    program, keep in mind that some part of the code is run on the driver, while some others are
                    performed on the executors. And some others may actually involve both.<br/><br/>

                    As you can see, Spark's API abstracts how the code is concretely run. This is actually great but
                    sometime a bit confusing.<br/><br/>
                </aside>
                <div style="text-align: left;margin-top:50px;display:inline-block;">
                    <ul style="margin-left: 0;margin-right:80px; list-style-type:none;white-space: nowrap;vertical-align: top;">
                        <li class="fragment highlight-light-blue"  data-fragment-index="1">val output = sparkContext</li>
                        <li class="fragment highlight-red"   data-fragment-index="2" style="padding-left: 100px">.textFile("hdfs://...")</li>
                        <li class="fragment highlight-red"   data-fragment-index="2" style="padding-left: 100px">.flatMap(_.split(" "))</li>
                        <li class="fragment highlight-red"   data-fragment-index="2" style="padding-left: 100px">.map((_, 1))</li>
                        <li class="fragment highlight-red"   data-fragment-index="2" style="padding-left: 100px">.reduceByKey(_ + _)</li>
                        <li class="fragment highlight-green" data-fragment-index="3" style="padding-left: 100px">.collect()</li>
                        <li class="fragment highlight-light-blue"  data-fragment-index="1">print output</li>
                    </ul>
                </div>
                <div style="text-align: left; margin-top:60px;display:inline-block; font-size: 30px;width:460px;">
                    <ul style="list-style: none;vertical-align: top;">
                        <li class="fragment" data-fragment-index="1"><div class="box blue"  style="display:inline-block;vertical-align: middle"></div>&nbsp;Executed on the driver</li>
                        <li class="fragment" data-fragment-index="2"><div class="box red"   style="display:inline-block;vertical-align: middle"></div>&nbsp;Executed on the executors</li>
                        <li class="fragment" data-fragment-index="3"><div class="box light-green" style="display:inline-block;vertical-align: middle"></div>&nbsp;Both may be involved</li>
                    </ul>
                </div>
            </section>
            <section data-transition="none none">
                <aside class="notes">
                    Having this in mind, is there anyone who could tell me what is the value printed out once this program
                    is run?<br/><br/>

                    In this example, the <b>closure</b> inside the foreach function refers to the counter variable. In
                    order to <b>distribute</b> this operation, Spark has to <b>serialize it</b> along with any variable
                    it refers to (so, in our case, the <b>counter</b> variable).<br/><br/>

                    During this <b>serialization process</b>, the counter variable will be <b>copied</b> and sent along
                    with the <b>closure</b>. So at some point, the <b>counter</b> variable used by the driver and its
                    executors <b>are no longer the same</b>.<br/><br/>

                    Let's get further.
                </aside>
                <ul style="list-style-type:none;padding-right: 30px">
                    <li class="fragment highlight-green" data-fragment-index="1">var counter = 0</li>
                    <li class="fragment highlight-green" data-fragment-index="1">val rdd = sc.parallelize(Seq(1, 2, 3, ...))</li>
                    <li>&nbsp;</li>
                    <li>rdd.map { rddItem ⇒
                    <li class="fragment highlight-red" data-fragment-index="2">&nbsp;&nbsp;&nbsp;counter += 1</li>
                    <li class="fragment highlight-red" data-fragment-index="2">&nbsp;&nbsp;&nbsp;rddItem</li>
                    <li>}.collect()<br/>&nbsp;</li>
                    <li class="fragment highlight-green" data-fragment-index="1">print("Counter value: " + counter)</li>
                </ul>
            </section>
            <section data-transition="none none">
                <aside class="notes">
                    What about this code now? Anybody can tell me what is going on here?<br/><br/>

                    Well this program <b>cannot run</b>. Remember, a transformation has to be <b>serialized</b> in order to be
                    executed, and a JDBC connection cannot be serialized.<br/><br/>

                    Alright, we can fix the problem like this. Any concern regarding the solution?<br/><br/>

                    Well whenever we run this program, a <b>JDBC connection</b> will be requested for each record of the dataset.
                    Which is pretty bad. Actually, do you have an idea about what would happen if initializing this
                    connection would require a <b>lot of memory</b>? Well it would imply many <b>garbage collections</b>
                    cycles. Whenever you do this, look at the number of records first, just in case of.<br/><br/>

                    This brings us to ask <b>two questions</b>. How can we define variables which are shared by the driver and
                    its executors? And secondly what can we do about variables which cannot be <b>serialized</b>b>?<br/><br/>

                    Alright, let's start with the second question as it's easier to answer.
                </aside>
                <pre><code class="scala" style="font-size: 25px;">val rdd = sc.parallelize(Seq(1, 2, 3, 4))

val con = ... // get JDBC connection
rdd.map { record ⇒
    findOrderById(con, record).length
}</code></pre>

                <pre class="fragment"><code class="scala" style="font-size: 25px;">val rdd = sc.parallelize(Seq(1, 2, 3, 4))

rdd.map { record ⇒
    val con = ... // get JDBC connection
    findOrderById(con, record).length
}</code></pre>
            </section>
            <section data-transition="none none">
                <aside class="notes">
                    In order to solve this problem, we'll use a <b>special transformation</b> called mapPartition. Basically
                    mapPartition allows you to execute a function for <b>each partition</b> of the dataset. This function
                    takes an <b>iterator</b> which iterates over the records contained in a <b>given partition</b>, and returns
                    another iterator. This code will now request a <b>JDBC connection</b> for each partition instead of
                    requesting one for each record.<br/><br/>

                    We won't get into details here, but almost all transformations provided by Spark are actually
                    a <b>specialization</b> of the mapPartition function. One thing to watch for when you use <b>mapPartition</b> is
                    to resist the temptation of <b>evaluating</b> the iterator by getting its size or its content. Doing so
                    would force the loading of all the records of the partition being <b>processed</b>b, which may throw an
                    OME.<br/><br/>

                    Now what about the other question, regarding how a variable can be shared between the driver and
                    its executors. Well, in order to answer that one, let's move on to the next topic.
                </aside>
                <pre><code class="scala" style="font-size: 25px;">val rdd = sc.parallelize(Seq(1, 2, 3, 4))

rdd.map { record ⇒
    val con = ... // get JDBC connection
    findOrderById(con, record).length
}</code></pre>

                <pre><code class="scala" style="font-size: 25px;">rdd.mapPartitions { recordIterator ⇒
    val con = ... // get JDBC connection
    recordIterator.map { record ⇒
        findOrderById(con, record).length
    }
}</code></pre>
            </section>
        </section>

        <section>
            <h2>Shared variables</h2>
            <section data-transition="none none">
                <aside class="notes">
                    Now let's go back to the <b>previous example</b>. The whole point of this code is to <b>share</b> a
                    variable between the <b>Driver</b> and its <b>executor</b>. However, and as we've seen it, this
                    cannot work for the reasons we've mentioned earlier. But actually, it is even <b>worse</b> than that. <br/>

                    Actually, the <b>serialization process</b> we've talked about earlier is done <b>as many time as</b>
                    there are partitions. So if your <b>cluster</b> contains <b>10 nodes</b> with a <b>hundred partitions</b>
                    each, this closure will be <b>serialized</b> and sent through the network a <b>thousand</b> times.<br/><br/>

                    Now if the <b>counter</b> variable is actually a <b>huge object</b>, this may get very <b>inefficient</b>
                    at some point. So the whole problem here is not only about <b>sharing a variable</b>, but also to do
                    it while <b>minimizing IOs</b> and in a <b>safe way</b> in terms of <b>concurrency</b>.

                    In order to resolve this problem, Sparks supports <b>2 types of shared variables</b> : <b>Accumulators</b>
                    and <b>Broadcast variables</b>. The first is designed to send a value from the <b>executors</b> to their
                    <b>driver</b>, while the <b>second one</b> is for the <b>other way around</b>.
                </aside>
                <div style="text-align: left">
                    <pre style="width:100%;"><code class="scala" style="font-size: 25px;">var counter = 0
val rdd = sc.parallelize(data)

rdd.map { rddItem ⇒ counter += 1 }.collect()
print("Counter value: " + counter)</code></pre>

                    <pre class="fragment" style="width:100%;"><code class="scala" style="font-size: 25px;">val hugeArray = ...
val bigRddWithIndex = ...

bigRddWithIndex.map { index ⇒ hugeArray[rddItem.key] }
...</code></pre>
                </div>
                <p class="fragment">Accumulators and Broadcast variables !</p>
            </section>

            <section data-transition="none none">
                <aside class="notes">
                    Accumulators are overall designed to <b>aggregate</b> values coming from the executors to the
                    driver. In order to make this safely, an accumulator's value is <b>only visible by the driver</b>,
                    and can be <b>updated only by its executors</b>.<br/><br/>

                    Now as you remember, an RDD may be <b>recomputed</b> for different reasons, which means that, if
                    updated inside a <b>transformation</b>, an <b>accumulator</b> may be modified more than required.
                    Actually, Spark <b>guarantees</b> that for a <b>given accumulator</b>, updates are applied <b>only
                    once</b> if they are done inside an <b>action</b>, but this is <b>not true</b> if they are applied inside
                    a <b>transformation</b>. Actually, this feature has been a lot <b>criticized</b>, as it has been found out that
                    in some edge cases, <b>accumulators</b> may be updated more than required even if those <b>updates</b> are done
                    inside an action. Therefore, I strongly suggest you to use them only for <b>debugging purpose</b>, or if
                    they are idempotent.
                </aside>
                <table>
                    <tr>
                        <td style="vertical-align: middle">
                            Accumulators
                        <pre style="width:100%;"><code class="scala" style="font-size: 25px;">val counter = sc.accumulator(0)
var rdd = sc.parallelize(data)

rdd.foreach {
    rddItem ⇒ counter += 1
}
print(counter.value)</code></pre>
                        </td>
                        <td><img style="margin-top:60px; box-shadow: 0; background-color: #FFFFFF; width:100%"
                                 src="images/spark_scala_meetup/accumulators.png"/></td>
                    </tr>
                    <tr>
                        <td colspan="2">Accumulators aggregate values coming from the executors to the driver</td>
                    </tr>
                </table>
            </section>

            <section data-transition="none none">
                <aside class="notes">
                    In a previous example, we were wondering about how it would be possible to share a <b>heavy
                    object</b> without <b>impacting network throughput</b>, or dealing with <b>concurrency issues</b>.
                    Well <b>broadcast variables</b> are exactly designed for this purpose. Basically, a broadcast variable
                    is able to wrap a value and is guaranteed to be <b>sent only once</b> per worker node. This is
                    especially useful if you need to share a <b>lookup table</b> across a cluster's nodes. <br/><br/>

                    In this example, we <b>wrap a big array</b> inside a <b>broadcast variable</b>, and are able to access it by
                    calling the <b>value property</b>. <!--Now no matter if this value is <b>mutable or not</b>, any of its
                    <b>updates</b> won't ever be <b>propagated</b> to the other nodes and will remain <b>local only</b>.<br/><br/>-->

                    Now <b>broadcast variables</b> may be a <b>bottleneck</b> at some point, as serializing and deserializing
                    a data structure can be <b>sometime expensive</b>. So it is important to choose a proper <b>data structure</b>
                    along with a good <b>serialization library</b>. Spark comes by default with <b>Kryo</b> but you can
                    also use other libraries or your own <b>serialization routines</b>.
                </aside>
                <table>
                    <tr>
                        <td style="vertical-align: middle">
                            Broadcast variables
                        <pre style="width:100%;"><code class="scala" style="font-size: 25px;">val bigArray = sc.broadcast(...)
var bigRddWithIndex = ...

bigRddWithIndex.map { e ⇒
  broadcastedArray.value[e.key]
}</code></pre>
                        </td>
                        <td><img style="margin-top:60px; box-shadow: 0; background-color: #FFFFFF; width:100%"
                                 src="images/spark_scala_meetup/broadcast_var.png"/></td>
                    </tr>
                    <tr>
                        <td colspan="2">Broadcast variables propagate a read-only value to all executors</td>
                    </tr>
                </table>
            </section>
        </section>

        <section data-transition="none none">
            <h2>RDD : Persistence and Caching</h2>
            <section data-transition="none none">
                <aside class="notes">
                    As we've mentioned it, Spark will <b>recompute</b> an RDD <b>any time</b> an action is being
                    performed on it. As you may guess, recomputing an RDD can quickly become a burden depending on the
                    size of its dataset.<br/><br/>

                    So in order to prevent that, Spark provides caching features allowing an RDDs data to be stored
                    either in <b>memory, on disk, or both of them</b>. This is especially useful, if you need to iterate
                    over an RDD, like when you're training a machine learning model for example.<br/><br/>

                    Persisting an RDD allows <b>future actions</b> to be much faster, actually often by more than 10
                    times. In order to do it, all you need is to marked the RDD as persisted, using the persist method
                    and to specify a storage level.<br/><br/>

                    ***You can also use the <b>cache method</b>, which basically is like persist but using the default
                    storage that is in memory.<br/><br/>

                    ***Finally, RDDs come with a method <b>unpersist()</b> that lets you manually remove them from the
                    cache.
                </aside>
                <pre><code class="scala" style="font-size: 25px;">val rdd = input.map(x ⇒ x * x)
rdd.count()      // RDD is computed first here
result.collect() // RDD is recomputed here</code></pre>

                <pre class="fragment" style="font-size: 25px;"><code class="scala">val rdd = input.map(x ⇒ x * x)
rdd.persist(StorageLevel.DISK_ONLY)

rdd.count()      // RDD is computed first here
result.collect() // RDD won' be recomputed here</code></pre>

                <pre class="fragment" ><code class="scala"  style="font-size: 25px;">result.cache()     // Use memory storage level
result.unpersist() // Suggest a manual eviction
</code></pre>
            </section>

            <section data-transition="none none">
                <aside class="notes">
                    Now no matter if you decide to cache an RDD's data in the memory or in the disk, keep in mind that
                    Spark will always favor memory over disk in order to allows operations to run as fast as possible.
                    So, data will be <b>spilled</b> on the disk only if you attempt to cache <b>too much data to fit in
                    memory</b>. Secondly, Spark will evict old cached partitions using an LRU cache policy.<br/><br/>

                    Here are the <b>standard ways</b> of caching an RDD's data. As you can see each has its <b>tradeoffs</b>
                    and <b>advantages</b>. You can also <b>replicate</b> an RDD's data over <b>several nodes</b> or, if
                    the RDD contains <b>too much</b> data to fit on a <b>single node</b>, caching it on multiple nodes.

                    Overall, this means that you should not worry too much about your <b>job breaking</b> if you ask
                    Spark to cache <b>too much data</b>. <b>However</b>, keep in mind that caching <b>unnecessary</b>
                    data can lead to more <b>data evictions</b> and <b>re-computation time</b>.
                </aside>
                <table>
                    <tr>
                        <td>Level</td>
                        <td>Space</td>
                        <td>CPU</td>
                        <td>RAM</td>
                        <td>Disk</td>
                    </tr>
                    <tr>
                        <td>MEMORY_ONLY</td>
                        <td>High</td>
                        <td>Low</td>
                        <td>Y</td>
                        <td>N</td>
                    </tr>
                    <tr>
                        <td>DISK_ONLY</td>
                        <td>Low</td>
                        <td>High</td>
                        <td>N</td>
                        <td>Y</td>
                    </tr>
                    <tr>
                        <td>MEMORY_AND_DISK</td>
                        <td>High</td>
                        <td>Medium</td>
                        <td>Some</td>
                        <td>Some</td>
                    </tr>
                    <tr>
                        <td>MEMORY_ONLY_SER*</td>
                        <td>Low</td>
                        <td>High</td>
                        <td>Y</td>
                        <td>N</td>
                    </tr>
                    <tr>
                        <td>MEMORY_AND_DISK_SER*</td>
                        <td>Low</td>
                        <td>High</td>
                        <td>Some</td>
                        <td>Some</td>
                    </tr>
                    <tr>
                        <td>&nbsp;</td>
                    </tr>
                    <tr>
                        <td colspan="5">(*) : Data is serialized before being persisted (takes less space)</td>
                    </tr>
                </table>
            </section>
            <section data-transition="none none">
                <aside class="notes">
                    Now another caching feature we did not talk about is <b>checkpointing</b>. In contrast with persisting,which
                    stores an RDD's partitions in the <b>caching layer</b> of each executor, checkpointing writes them to an
                    <b>external storage</b> such as HDFS.<br/><br/>

                    Persisting is performed in the <b>executor's JVM</b>, and therefore it may take space that could be used for
                    other <b>computations</b> or increase the risk of memory failures. As <b>checkpointing</b> allow an RDD to be stored
                    outside the executor, it allows to leave space for further <b>computations</b>. Also Checkpointing allows an
                    RDD's data to survive beyond duration of a Spark application<br/><br/>

                    However, it also has some <b>tradeoffs</b>. First of all, checkpointing may be slower than persisting, and
                    just like <b>persisting</b>, it forces the <b>materialization</b> of the RDD. No magic here.<br/><br/>

                    One last thing, checkpoint is an <b>action</b>. Therefore, if you perform any other computations with the
                    checkpointed RDD, think about caching it before checkpointing it.<br/><br/>
                </aside>
                Checkpointing: Write an RDD on an external storage
                <pre style="font-size: 25px;"><code class="scala">sc.setCheckpointDir("...")
someRDD.checkpoint // Materialize the RDD</code></pre>
                <div style="text-align: left">
                    <ul>
                        <li>Release pressure on executor's memory/disk</li>
                        <li>RDD's data survives beyond a Spark application</li>
                        <li>Checkpointing is an action!</li>
                    </ul>
                </div>
            </section>
            <section data-transition="none none">
                <aside class="notes">
                    Now you may wonder, when should I persist, cache or checkpoint an RDD. Well it's very hard to answer
                    and is a case by case problem. Actually there is a very good book which addresses this question, and
                    I will provide you with its reference at the end of this talk.<br/><br/>

                    Keep in mind, that persisting or checkpointing an RDD has some cost and is unlikely to improve
                    performances for operations that are performed only once. Secondly, sometime, the cost of persisting
                    or checkpointing can be so high that recomputing is a better option.<br/><br/>

                    So here are some best practices. First of all, you should rely on persisting whenever multiple
                    actions have to be performed on the same RDD.<br/><br/>

                    Secondly, whenever the computation generated by a long chain of transformation gets too expansive or
                    prone to fail. Breaking the whole chain may allow you to release some pressure on the memory and the
                    CPU as well.<br/>

                    In general, it is worth reusing an RDD rather than recomputing it if the computation is large, relative
                    to your cluster and the rest of your job.
                </aside>
                <div style="text-align: left">
                    Best Practices:<br/>
                    <ul>
                        <li>Iterative computations / Multiple actions</li>
                        <li>Breaking Long chain of (expansive) transformations</li>
                        <li>If the cost to compute each partition is too high</li>
                    </ul>
                </div>
            </section>
        </section>

        <section data-transition="none none">
            <h2>Spark Execution Plan</h2>
            <section data-transition="none none">
                <aside class="notes">
                    Now that you have a <b>basic understanding</b> of how an RDD works, we can get a little bit more
                    into details and see how it is <b>computed</b> by Spark<br/><br/>

                    Whenever an <b>action</b> is called on an RDD, Spark <b>translates</b> the lineage of the RDD to a <b>execution plan</b>.
                    First it traverses the <b>whole graph</b> starting by the <b>last transformation</b> requested and goes <b>backward</b>
                    until the creation of the RDD.<br/><br/>

                    Once this done, Spark <b>divides</b> the work to be done into one to <b>multiples stages</b> depending on the
                    nature of the transformations involved.<br/><br/>

                    A stage is basically a set of <b>one to many</b> transformations which do not require any <b>data
                    movement</b> or <b>shuffling</b>, and which can be therefore <b>pipelined</b>. It's some kind of a
                    <b>super-operation</b> in which <b>transformations</b> have been <b>fused</b> together in order to
                    not going over the data <b>multiple time</b> and to avoid the <b>overhead</b> of each operation if
                    they were <b>executed</b> one after the other.<br/><br/>

                    Now, each stage is composed of as <b>many task</b> as there are partitions available <b>when its
                    executed</b>. So if you have <b>10 partitions</b> for a given stage, the scheduler will create
                    <b>10 tasks</b> for it. Now as you may guess, <b>the more task</b> your whole Spark process needs
                    to execute, <b>the longer</b> it will take to compute the final result. So one way to optimize
                    this, is to <b>prevent shuffling</b> to happen, or at least limit its <b>impact on performances</b>,
                    which is exactly what we're about to discuss.
                </aside>
                <div style="display:table;margin: 120px auto auto auto">
                    <div style="display: table-caption; text-align: center;margin-bottom: 80px;">
                        <div class="fragment border" data-fragment-index="1" style="display: inline-block">
                            <div style="display: inline-block">&nbsp;textFile&nbsp;</div>
                            <div class="left" style="vertical-align: middle"><div class="arrow"></div></div>
                            <div style="display: inline-block">&nbsp;flatMap&nbsp;</div>
                            <div class="left" style="vertical-align: middle"><div class="arrow"></div></div>
                            <div style="display: inline-block">&nbsp;map&nbsp;</div>
                        </div>
                        <div class="left" style="vertical-align: middle"><div class="arrow"></div></div>
                        <div class="fragment border" data-fragment-index="2" style="display: inline-block">
                            <div style="display: inline-block">&nbsp;reduceByKey&nbsp;</div>
                        </div>
                    </div>
                    <div style="display:table-row;">
                        <div style="display:table-cell;vertical-align: middle;width:100px;">
                            <div class="rectangle blue-border">
                                <div class="child blue-border"><div class="child blue-border" style="line-height:70px;"><span style="font-size: 30px;">Data</span></div></div>
                            </div>
                        </div>
                        <div style="display:table-cell;vertical-align: middle;width:150px;">
                            <div style="margin-top:50px;" class="up-r"><div class="arrow"></div></div><br/>
                            <div class="right"><div class="arrow"></div></div><br/>
                            <div class="down-r"><div class="arrow"></div></div>
                        </div>
                        <div class="fragment border" data-fragment-index="1" style="display:table-cell;vertical-align: middle;margin-top:25px;padding:0 5px 0 5px;">
                            <div style="display:table-cell;vertical-align: middle;">
                                <div class="executor">
                                    <div class="title"><span>&nbsp;Executor&nbsp;</span></div>
                                    <div class="body">
                                        <div class="partition-list" style="display: inline-block">
                                            <div class="block blue">&nbsp;</div>
                                            <div class="block blue" style="opacity: 0.7;">&nbsp;</div>
                                            <div class="block blue" style="opacity: 0.2;">&nbsp;</div>
                                        </div>
                                        <div class="right" style="display: inline-block"><div class="arrow"></div></div>
                                        <div class="partition-list" style="display: inline-block">
                                            <div class="block red">&nbsp;</div>
                                            <div class="block red" style="opacity: 0.7;">&nbsp;</div>
                                            <div class="block red" style="opacity: 0.2;">&nbsp;</div>
                                        </div>
                                    </div>
                                </div>
                                <div class="executor">
                                    <div class="title"><span>&nbsp;Executor&nbsp;</span></div>
                                    <div class="body">
                                        <div class="partition-list" style="display: inline-block">
                                            <div class="block blue">&nbsp;</div>
                                            <div class="block blue" style="opacity: 0.7;">&nbsp;</div>
                                            <div class="block blue" style="opacity: 0.2;">&nbsp;</div>
                                        </div>
                                        <div class="right" style="display: inline-block"><div class="arrow"></div></div>
                                        <div class="partition-list" style="display: inline-block">
                                            <div class="block green">&nbsp;</div>
                                            <div class="block green" style="opacity: 0.7;">&nbsp;</div>
                                            <div class="block green" style="opacity: 0.2;">&nbsp;</div>
                                        </div>
                                    </div>
                                </div>
                            </div>
                        </div>
                        <div style="display:table-cell;width:70px;vertical-align: middle;">
                            <div class="up-r"  ><div class="arrow"></div></div><br/>
                            <div class="right" ><div class="arrow"></div></div><br/>
                            <div class="down-r"><div class="arrow"></div></div>
                        </div>
                        <div class="fragment border" data-fragment-index="2" style="display:table-cell;margin-top:25px;padding:0 5px 0 5px;vertical-align: middle;">
                            <div class="executor">
                                <div class="title"><span>&nbsp;Executor&nbsp;</span></div>
                                <div class="body">
                                    <div class="partition-list" style="display: inline-block">
                                        <div class="block green">&nbsp;</div>
                                        <div class="block red" style="opacity: 0.7;">&nbsp;</div>
                                    </div>
                                </div>
                            </div>
                            <div class="executor">
                                <div class="title"><span>&nbsp;Executor&nbsp;</span></div>
                                <div class="body">
                                    <div class="partition-list" style="display: inline-block">
                                        <div class="block green">&nbsp;</div>
                                        <div class="block green" style="opacity: 0.2;">&nbsp;</div>
                                    </div>
                                    <div class="partition-list" style="display: inline-block">
                                        <div class="block red" style="opacity: 0.7;">&nbsp;</div>
                                        <div class="block red" style="opacity: 0.2;">&nbsp;</div>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </div>
                    <div style="display:table-row;vertical-align: middle;width:100px;">
                        <div style="display:table-cell;"></div>
                        <div style="display:table-cell;"></div>
                        <div class="fragment" data-fragment-index="1" style="display:table-cell;text-align: center">
                            <span class="fragment flip" data-fragment-index="3">Stage 1</span>
                            <div class="fragment" data-fragment-index="3">
                                <div class="down"><div class="arrow orange"></div></div>
                                <div class="rectangle small orange-border" style="margin: 0 auto 0 auto">
                                    <div class="child small orange-border"><div class="child small orange-border">&nbsp;</div></div>
                                </div>
                            </div>
                        </div>
                        <div style="display:table-cell;"></div>
                        <div class="fragment" data-fragment-index="2" style="display:table-cell;text-align: center">
                            <span class="fragment flip" data-fragment-index="3">Stage 2</span>
                            <div class="fragment" data-fragment-index="3">
                                <div class="down"><div class="arrow orange"></div></div>
                                <div class="rectangle small orange-border" style="margin: 0 auto 0 auto">
                                    <div class="child small orange-border"><div class="child small orange-border">&nbsp;</div></div>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </section>
        </section>
        <section data-transition="none none">
            <h2>Efficient Shuffles</h2>
            <section data-transition="none none">
                <aside class="notes">
                    Alright, what's the problem with shuffles.<br/><br/>

                    We saw that first it implies <b>data movement</b> which may be expensive.<br/><br/>

                    Also, and as we've just seen it, the more shuffles you do, the more <b>stages</b> are created.
                    And Stages are <b>sequential</b>, so Spark cannot start s stage before making sure that the <b>dataset</b> has
                    been <b>completely</b> processed by the previous stage.<br/><br/>


                    And finally, a shuffle has an impact on <b>fault tolerance</b>.
                </aside>
                <div style="display:inline-block;text-align: left; margin: auto auto auto auto">
                    What's wrong with shuffles?<br/>
                    <ul style="margin-left: 50px;">
                        <li>Data movement is expansive</li>
                        <li>Shuffles generate Stages</li>
                        <li>Fault tolerance</li>
                    </ul>
                </div>
            </section>
            <section data-transition="none none">
                <aside class="notes">
                    Fault Tolerance with Shuffling

                    So for all of these reasons, shuffles should be limited. How can we do this, well, let's take
                    a couple of use cases.
                </aside>
                <div style="display:inline-block;vertical-align: top">
                    <div class="vertical-box-set" style="display: inline-block;vertical-align: top">
                        <div class="box red fragment background green" data-fragment-index="5"></div>
                        <div class="box red"></div>
                        <div class="box red fragment background green" data-fragment-index="5"></div>
                    </div>
                    <div style="margin:25px 25px 0 25px;display:inline-block;vertical-align: top">
                        <div class="right"><div class="arrow"></div></div><br/>
                        <div class="right"><div class="arrow"></div></div><br/>
                        <div class="right"><div class="arrow"></div></div>
                    </div>
                    <div class="vertical-box-set" style="display: inline-block;vertical-align: top">
                        <div class="box red fragment background green" data-fragment-index="5"></div>
                        <div class="box red"></div>
                        <div class="box red fragment background green" data-fragment-index="5"></div>
                    </div>
                    <div style="margin:25px 25px 0 25px;display: inline-block;vertical-align: top">
                        <div class="up-r"><div class="arrow"></div></div><br/>
                        <div class="right"><div class="arrow"></div></div><br/>
                        <div class="down-r"><div class="arrow"></div></div>
                    </div>
                    <div style="margin-top:50px;"></div>
                    <div class="vertical-box-set" style="display: inline-block;vertical-align: top">
                        <div class="fragment flip" data-fragment-index="3">
                            <div class="box blue fragment background green" data-fragment-index="2"></div>
                        </div>
                        <div class="fragment unflip" data-fragment-index="3">
                            <div class="box blue"></div>
                        </div>
                        <div class="box blue fragment background green" data-fragment-index="5"></div>
                        <div class="box blue"></div>
                    </div>
                    <div style="margin:25px 25px 0 25px;display:inline-block;vertical-align: top">
                        <div class="right"><div class="arrow"></div></div><br/>
                        <div class="right"><div class="arrow"></div></div><br/>
                        <div class="right"><div class="arrow"></div></div>
                    </div>
                    <div class="vertical-box-set" style="display:inline-block;vertical-align: top">
                        <div class="fragment flip" data-fragment-index="3">
                            <div class="box blue fragment background green" data-fragment-index="1"></div>
                        </div>
                        <div class="fragment unflip" data-fragment-index="3">
                            <div class="box blue"></div>
                        </div>
                        <div class="box blue fragment background green" data-fragment-index="5"></div>
                        <div class="box blue"></div>
                    </div>
                    <div style="margin:25px 25px 0 25px;display:inline-block;vertical-align: top">
                        <div class="up-r"><div class="arrow"></div></div><br/>
                        <div class="right"><div class="arrow"></div></div><br/>
                        <div class="down-r"><div class="arrow"></div></div>
                    </div>
                </div>
                <div style="display:inline-block;vertical-align: top;margin-top:42px;">
                    <div class="vertical-box-set" style="display: inline-block;vertical-align: middle">
                        <div class="box purple"></div>
                        <div class="box purple"></div>
                        <div class="box purple fragment background green" data-fragment-index="4"></div>
                        <div class="box purple"></div>
                        <div class="box purple"></div>
                        <div class="box purple"></div>
                    </div>
                </div>
            </section>

            <section data-transition="none none">
                <aside class="notes">
                    Let's say I want to <b>make a join</b> between an RDD containing the people in <b>Canada</b>
                    with one that contains all the canadian <b>provinces/territories</b>.<br/><br/>

                    *** Well, this would result in a <b>very uneven sharding</b> as some provinces like <b>Quebec</b>
                    are much more populated than territories like<b>Nunavut</b>.<br/><br/>

                    Moreover, we would get a <b>very limited level of parallelism</b> as we would only have 13
                    <b>partitions</b>, and adding more <b>machines</b> to get the job done would not change <b>anything</b>.
                </aside>
                <table>
                    <tr>
                        <td><img
                                style="margin-top:30px; margin-right:60px;box-shadow: 0; background-color: #FFFFFF; width:100%"
                                src="images/spark_scala_meetup/shuffle_3.png"/></td>
                        <td style="vertical-align: middle">
                            canadians.join(provinces)
                            <ul class="fragment" style="width:350px; white-space: nowrap;">
                                <li>Uneven sharding</li>
                                <li>Limited parallelism</li>
                            </ul>
                        </td>
                    </tr>
                </table>
            </section>
            <section data-transition="none none">
                <aside class="notes">
                    Actually, a better way to do this is to use a <b>broadcast variable</b> cointaining all the canadian
                    <b>provinces and territories</b>. This variable could be then sent to all <b>the worker nodes</b> in order,
                    which would avoid a <b>shuffle</b>. Now of course, this is only possible if the <b>dataset</b> being sent can
                    <b>fit in the memory</b> of a single node.
                </aside>
                <table>
                    <tr>
                        <td><img
                                style="margin-top:30px; margin-right:60px;box-shadow: 0; background-color: #FFFFFF; width:100%"
                                src="images/spark_scala_meetup/shuffle_4.png"/></td>
                        <td style="width:350px; vertical-align: middle">
                            <ul style=" white-space: nowrap;">
                                <li>Even Sharding</li>
                                <li>No Shuffle Required</li>
                            </ul>
                        </td>
                    </tr>
                </table>
            </section>
            <section data-transition="none none">
                <aside class="notes">
                    Now what if you cannot use a <b>broadcast variable</b> ? what if the data in <b>both RDDs</b> is too
                    big ? Let's say for example that we need to <b>join</b> an <b>RDD</b> containing all the <b>people in Canada</b>
                    with one containing all the <b>people in the world</b>. Joining those <b>RDDs</b> would result in a <b>lot of
                    data shuffled</b> across the <b>network</b> and potentially in a <b>space problem</b> on the destination nodes.
                </aside>
                <table>
                    <tr>
                        <td><img
                                style="margin-top:30px; width:100%; margin-left:10px;box-shadow: 0; background-color: #FFFFFF"
                                src="images/spark_scala_meetup/shuffle_7.png"/></td>
                        <td style="vertical-align: middle">
                            <ul style="width:350px; white-space: nowrap;">
                                <li>Even Sharding</li>
                                <li>Good Parallelism</li>
                                <li>Space may be a problem</li>
                            </ul>
                        </td>
                    </tr>
                </table>
            </section>
            <section data-transition="none none">
                <aside class="notes">
                    So instead of doing this, we could just create a <b>partial RDD</b> using a <b>filter</b>, and <b>reduce</b>
                    the amount of data to be <b>transferred</b>. The whole idea here is to make sure that you <b>transfer</b> only
                    the data that is required by <b>further</b> operations.<br/><br/>

                    Overall, this leads to one fundamental question. The main reason why Shuffles is required is because
                    data is not properly distributed. So how can we optimize that? If data is distributed properly, we
                    could prevent shuffling from being so expensive. So let's think about this.
                </aside>
                <table>
                    <tr>
                        <td><img
                                style="margin-top:30px; width:100%; margin-left:10px;box-shadow: 0; background-color: #FFFFFF;"
                                src="images/spark_scala_meetup/shuffle_8.png"/></td>
                        <td style="vertical-align: middle;">
                            <ul style="width:350px; white-space: nowrap;">
                                <li>Even Sharding</li>
                                <li>Good Parallelism</li>
                                <li>Less Space required</li>
                                <li>Less Data shuffled</li>
                            </ul>
                        </td>
                    </tr>
                </table>
            </section>
        </section>
        <section data-transition="none none">
            <h2>Efficient Data distribution</h2>
            <section data-transition="none none">
                <aside class="notes">
                    Alright so what do we know about shuffling so far....


                    <!--The second thing you should always keep in mind when working with Spark is how your data is-->
                    <!--distributed over the Spark cluster. A bad distribution may lead to heavy shuffles, unbalanced-->
                    <!--partitions, bad performances, or even worse, failed jobs.-->

                    We saw that in Spark, the data is split into multiple partitions which are spread among the executors.
                    We've also seen that the initial number of partition can be provided when loading the data, and that
                    by default Spark uses either its configuration or the one specific to each datasource.<br/><br/>

                    Partitions cannot span over several executors, therefore, you may end up having partitions of different
                    size, and their number may change over time depending on the transformations performed.<br/><br/>

                    As data distribution is such a big deal, it should be possible to manage it somehow. Well, this is
                    exactly what Spark allows you to do.<br/><br/>
                    <!---->
                    <!--Why do we care? Well the number of partitions processed by Spark at a given moment can have a-->
                    <!--significant impact on performances and can some time make the difference between a job which-->
                    <!--completes from one which does not. For these reasons, Spark provides different ways to redistribute-->
                    <!--the data in order to make it more manageable.-->

                    <!--Partitioning:-->
                    <!--* Problems: Skewed data, not enough partitions, too many partitions-->
                </aside>
               <ul>
                    <li>Data is split into multiple partitions</li>
                    <li>The initial number of partition can be configured</li>
                    <li>Partitions do not span</li>
                    <li>The number of partition can change</li>
                </ul><br/><br/>
                A good data distribution is the key for efficient Spark jobs.
            </section>

            <section data-transition="none none">
                <aside class="notes">
                    Let's first look at how partitioning can be changed across transformations. Just like data loading
                    functions, some transformations provide an additional parameter defining the number of partitions
                    in which the data will be split into once transformed.<br/><br/>

                    So as you can see here, we've created an RDD of integers, which is grouped by ranges.<br/><br/>

                    In order to display the content of each partition, we use the function foreachPartition which
                    works pretty much like mapPartition. So it takes an iterator which iterates on the partition list.<br/><br/>

                    groupBy can also take a second parameter which defines the number of partitions we want to output.
                </aside>
                <pre><code class="scala"  style="font-size: 25px;">val ints = sc.parallelize(Seq(1,2,11,12,21,22))

// use the default number of partitions
ints.groupBy((i: Int) ⇒ i / 10)
    .foreachPartition(p => println(p.toList))</code></pre>
<pre><code class="scala"  style="font-size: 25px;">List()
List((0,Seq(1, 2)))
List((1,Seq(11, 12)))
List((2,Seq(21, 22)))</code></pre>

<pre class="fragment" data-fragment-index="1"><code class="scala"  style="font-size: 25px;">// distributes the resulting data in 2 partitions
ints.groupBy((i: Int) ⇒ i / 10, 2)
    .foreachPartition(p => println(p.toList))</code></pre>
<pre class="fragment" data-fragment-index="1"><code class="scala"  style="font-size: 25px;">List((1,Seq(11, 12)))
List((0,Seq(1, 2)), (2,Seq(21, 22)))</code></pre>
            </section>
            <section data-transition="none none">
                <aside class="notes">
                    Most of transformations allowing you to set the <b>number of output partitions</b> also allow you to
                    provide a specific <b>partitioner</b>. By default, Spark uses the <b>HashPartitioner</b>, but it also provides
                    a RangePartitioner, and you can also implement your own partitioner.<br/><br/>

                    I won't get into details here, the documentation is pretty straightforward for that, but overall,
                    depending on the partitioning logic, you may manage to increase your performances significantly.<br/><br/>

                    ------------------------------<br/><br/>

                    The <b>Hashpartitioner</b> simply hashes each value in order to resolve the <b>partition</b> to which the value
                    belongs.<br/><br/>

                    The <b>RangePartitioner</b> works a bit differently. First it requires a <b>pair rdd</b> which contains
                    the <b>ranges</b> in which each data item falls in. This pair rdd is then <b>sampled</b> in order to figure an
                    <b>estimation of the ranges</b> which are mostly used, and a <b>RangePartitioner</b> is created. Then we pass this
                    partitioner to the groupBy function and proceed as before.<br/><br/>

                    Using a <b>RangePartitioner</b>, you can sometime get a more <b>even distribution</b> of the data. Aside the
                    HashPartitioner and the RangePartitioner, you can also implement your very own <b>custom partitioner</b>
                    by extending the <b>Partitioner interface</b>.
                </aside>
                <pre><code class="scala"  style="font-size: 25px;">val ints = sc.parallelize(Seq(1,2,11,12,21,22))
// p(k) = k % num_partitions
val partitioner = new HashPartitioner(4)
ints.groupBy((i:Int) => i / 10, partitioner)</code></pre>
<pre><code class="scala"  style="font-size: 25px;">List((0,Seq(1, 2)))
List((1,Seq(11, 12)))
List((2,Seq(21, 22)))
List()</code></pre>
<!--<pre class="fragment" data-fragment-index="1"><code class="scala"  style="font-size: 25px;">val ints = sc.parallelize(Seq(8,96,240,400,401,800))-->
<!--val ranges = ints.map(i => (i / 10, i))-->
<!--val partitioner = new RangePartitioner(4, ranges))-->

<!--ints.groupBy((i:Int) => i / 10, ranges)</code></pre>-->
<!--<pre class="fragment" data-fragment-index="1"><code class="scala"  style="font-size: 25px;">List((40,Seq(400, 401)))-->
<!--List((24,Seq(240)))-->
<!--List((80,Seq(800)))-->
<!--List((0,Seq(8)), (9,Seq(96)))</code></pre>-->
            </section>
            <section data-transition="none none">
                <aside class="notes">
                    You can also pass a <b>partitioner</b> without making any transformation using the <b>partitionBy</b> function.
                    However, whenever you do this, make sure to persist the <b>resulting RDD</b>, as partitionBy forces a
                    <b>shuffle</b>. And in order to prevent doing it every time the rdd is <b>computed</b>, the resulting rdd has to be
                    cached.<br/><br/>

                    A couple methods allowing to get some information regarding the data partitioning.<br/><br/>

                    Alright, last thing regarding <b>partitioning</b>. You have to be aware that in some cases, an RDD's partitioning
                    information may be <b>lost</b>. This happens, when you use a <b>transformation</b> which may change how the data
                    is <b>distributed</b>. For example, mapping a <b>PairRDD</b> may lead to change its <b>keys</b>. As the keys are used
                    by Spark to partition the data, the resulting RDD has no longer any information about how it is
                    supposed to be <b>partitioned</b>.
                </aside>
                <pre><code class="scala"  style="font-size: 25px;">val partitioneddRdd = ints.partitionBy(partitioner)
partitioneddRdd.cache()</code></pre>

                <pre class="fragment"><code class="scala"  style="font-size: 25px;">rdd.partitioner      // returns an Option[Partitioner]
rdd.getNumPartitions // returns an Int</code></pre>

                <pre class="fragment"><code class="scala"  style="font-size: 25px;">val pairs = sc.parallelize(
   Seq("a" -> 1, "b" -> 2, "c" -> 3)
)
val partitioner = new RangePartitioner(3, pairs)

val rdd1 = pairs.partitionBy(partitioner)
val rdd2 = rdd1.map { case (k,v) => (v,v) }

rdd1.partitioner // returns Some(RangePartitioner@...)
rdd2.partitioner // returns None</code></pre>
            </section>

            <section data-transition="none none">
                <aside class="notes">
                    Finally, you can also <b>explicitly</b> ask Spark to <b>redistribute</b> the data using <b>coalesce</b> and <b>repartition</b>.
                    Overall these two functions consists in <b>changing the number</b> of partitions used to store the data.<br/><br/>

                    <b>Coalesce</b> will allow you to reduce this number while <b>repartition</b> can increase or reduce it. The main
                    difference is that <b>coalesce</b> will just regroup the <b>partitions</b> in a smaller amount of partitions,
                    without <b>worrying</b> about how well the <b>data</b> is spread, while repartition <b>garantees</b> the data to be spread
                    as evenly as possible.<br/><br/>

                    Now considering what we said so far, could you tell me what kind of transformation are coalesce and
                    repartition?
                </aside>
                <pre><code class="scala"  style="font-size: 25px;">val ints = sc.parallelize(Seq(1,2,3,4,5,6,7,8,9,0), 4)</code></pre>
<pre><code class="scala"  style="font-size: 25px;">List(3, 4, 5)
List(6, 7)
List(1, 2)
List(8, 9, 0)</code></pre>

<pre class="fragment" data-fragment-index="1"><code class="scala"  style="font-size: 25px;">ints.coalesce(3)</code></pre>
<pre class="fragment" data-fragment-index="1"><code class="scala"  style="font-size: 25px;">List(3, 4, 5)
List(6, 7, 8, 9, 0)
List(1, 2)
</code></pre>

<pre class="fragment" data-fragment-index="2"><code class="scala"  style="font-size: 25px;">ints.repartition(3)</code></pre>
<pre class="fragment" data-fragment-index="2"><code class="scala"  style="font-size: 25px;">List(5, 7, 8)
List(1, 3, 9)
List(2, 4, 6, 0)
</code></pre>
            </section>

            <section data-transition="none none">
                <aside class="notes">
                    Alright, overall, as we kept mentioning during this talk, data distribution should be one of your
                    primary concern when writing a Spark job. Spark provides different ways to manage this in an optimal
                    way. Now one question you may ask, how many partition should I use to distribute the data.<br/><br/>


                    Well a good metric is to take the number of cores available and to multiply it by 2 or 3. The goal is to
                    make the job as parallelizable as possible.<br/><br/>

                    You basically don't won't to have too few partitions otherwise some resources of your cluster would be idle.<br/><br/>

                    Having too many partitions on the other hand may also be a problem, as this would imply more IO.<br/><br/>

                    Finally, sometime partitions may be too heavy to load into an executor's memory, in which case, you
                    would have to trade some IO.<br/><br/>

                    So overall, you have to figure out the good balance between parallelism and the size of each
                    partition, along with how data is distributed.
                </aside>
                <ul>
                    <li>Optimal number of partition: number of cores * 2 or 3</li>
                    <li>Too few partition would not take fully advantage of your cluster</li>
                    <li>Too many partition would imply too much IO</li>
                    <li>Keep the balance between memory, parallelism and data distribution</li>
                </ul>
            </section>
        </section>

        <!--<section data-transition="none none">-->
            <!--<h2>Recap</h2>-->
            <!--<aside class="notes">-->
                <!--Alright, so this was pretty much Spark in a nutshell. We covered how Spark represents the data-->
                <!--and how it performs computations on it. Overall, an RDD is a sequence of steps that each executor-->
                <!--has to perform on each partition in order to obtain the final result. We saw that depending on the-->
                <!--operations involved, data may have to be redistributed among the cluster or not, which may have a-->
                <!--significant impact on a Spark job.-->

                <!--We've also covered how this looks like under the hood and saw that narrow transformations are actually-->
                <!--fused together for optimization reasons, and that the operation resulting from this fusion is called-->
                <!--a stage. Each stage is then run on each partition of the dataset by the executors.-->

                <!--Having all this in mind, there are three things you should look at when a Spark job does not meet-->
                <!--with your expectations in terms of performances:-->

                <!-- - The transformations implied by the job.-->
                <!--&lt;!&ndash;Are they expansive, easy to compute, where does the input&ndash;&gt;-->
                <!--&lt;!&ndash;data come from, do they create extra objects whenever a record is processed, etc...&ndash;&gt;-->

                <!-- - The data distribution:-->
                <!--&lt;!&ndash;This is probably the one you should care about the most. A bad&ndash;&gt;-->
                <!--&lt;!&ndash;distribution may lead to heavy shuffles, skewed data, that is data which partitioning led to a set&ndash;&gt;-->
                <!--&lt;!&ndash;of unbalanced partitions, and so forth...&ndash;&gt;-->

                <!-- - Finally shuffles:-->
                <!--&lt;!&ndash;To be honest it is hard to avoid them however, there's some tricks you can use to&ndash;&gt;-->
                <!--&lt;!&ndash;shuffle efficiently.&ndash;&gt;-->

                <!--In the next part of this talk, we'll focus on these three items and will cover some solutions you-->
                <!--could think about whenever you face one of those issue.-->
            <!--</aside>-->
            <!--<div style="text-align: left">-->
                <!--Spark in a nutshell:<br/>-->
                <!--<ul style="margin-left: 70px;">-->
                    <!--<li>RDD is an in interface to your dataset</li>-->
                    <!--<li>Wide / Narrow Transformations</li>-->
                    <!--<li>Data redistribution may be (very) expensive</li>-->
                <!--</ul>-->
            <!--</div>-->
            <!--<br/>-->
            <!--<div style="text-align: left">-->
                <!--Considerations:<br/>-->
                <!--<ul style="margin-left: 70px;">-->
                    <!--<li>Transformations</li>-->
                    <!--<li>Data distribution</li>-->
                    <!--<li>Shuffles</li>-->
                <!--</ul>-->
            <!--</div>-->
        <!--</section>-->

        <!-- ################################################################################################### -->
        <!-- ################################################################################################### -->
        <!-- ################################################################################################### -->
        <!-- ################################################################################################### -->
        <!-- ################################################################################################### -->

        <!-- ################################################################################################### -->
        <!-- ################################################################################################### -->
        <!-- ################################################################################################### -->
        <!-- ################################################################################################### -->
        <!-- ################################################################################################### -->

        <!-- ################################################################################################### -->
        <!-- ################################################################################################### -->
        <!-- ################################################################################################### -->
        <!-- ################################################################################################### -->
        <!-- ################################################################################################### -->

        <!-- ################################################################################################### -->
        <!-- ################################################################################################### -->
        <!-- ################################################################################################### -->
        <!-- ################################################################################################### -->
        <!-- ################################################################################################### -->



        <!-- ################################################################################################### -->
        <!-- ################################################################################################### -->
        <!-- ################################################################################################### -->
        <!-- ################################################################################################### -->
        <!-- ################################################################################################### -->

        <section data-transition="none none">
            <h2>Efficient Shuffles</h2>

            <section data-transition="none none">
                <aside class="notes">
                    Finally, what about distribution which would en up in having most of the data inside 10% of the
                    partitions available. One solution is to use salting techniques like Salting, Isolation Salting
                    and Isolation Map joins, however we won't get into details here, especially that there is already
                    a very good presentation about this called Top 5 Mistakes When Writing Spark Applications.


                    This may happen with a Pair RDD which contains a majority of items with an
                    identical key.



                    Well one solution is to map the RDD with an additional key which would be guaranteed
                    to be unique, and to process the RDD just like before
                    Key spaces / skwewed data
                </aside>
                <pre><code class="scala"  style="font-size: 25px;">
case class Order(id: UUID, userId: UUID)
case class User(id: UUID)

val orders: RDD[Order] = ???
val users: RDD[User] = ???

val orderByUserId = orders.map(o => (o.userId, o))
val userById = users.map(u => (u.id, u))

orderByUserId.join(userById)

List()
List()
List((7c436268-8ad1-4e1e-be73-4f0ae7ca5c11,(98756234-4219-49fe-a51d-c0ed90f2f2cb,User(7c436268-8ad1-4e1e-be73-4f0ae7ca5c11))),
     (7c436268-8ad1-4e1e-be73-4f0ae7ca5c11,(963d8bd8-ee68-4554-82d1-e656823da320,User(7c436268-8ad1-4e1e-be73-4f0ae7ca5c11))),
     (7c436268-8ad1-4e1e-be73-4f0ae7ca5c11,(d0e11808-3fd3-4402-8619-566ab3b9648e,User(7c436268-8ad1-4e1e-be73-4f0ae7ca5c11))))
List((968c9384-45e1-4496-9c93-d541b32344a3,(a8ac119a-77e1-4985-b4e6-29b5a965f300,User(968c9384-45e1-4496-9c93-d541b32344a3))))

rdd.map { case (key, value) => (scala.util.Random.nextInt(4), (key, value)) }




import java.util.UUID
import java.util.UUID.{randomUUID => randomUUID}

val user = User(java.util.UUID.randomUUID())
val user2 = User(java.util.UUID.randomUUID())

val orders = sc.parallelize(Seq(Order(randomUUID, user.id), Order(randomUUID, user.id), Order(randomUUID, user.id), Order(randomUUID, user2.id)))
val users = sc.parallelize(Seq(user, user2))


List(
  (8ba22a2c-428b-4477-bb08-ea8f4ec05d4f,(Order(0046088a-73e2-47ec-91fc-df95982f1900,8ba22a2c-428b-4477-bb08-ea8f4ec05d4f),User(8ba22a2c-428b-4477-bb08-ea8f4ec05d4f))),
  (8ba22a2c-428b-4477-bb08-ea8f4ec05d4f,(Order(962c2660-0946-44f6-8b1d-abc69f9b4c7d,8ba22a2c-428b-4477-bb08-ea8f4ec05d4f),User(8ba22a2c-428b-4477-bb08-ea8f4ec05d4f))),
  (8ba22a2c-428b-4477-bb08-ea8f4ec05d4f,(Order(dd235d08-ba29-4cd1-b4eb-8485202459c3,8ba22a2c-428b-4477-bb08-ea8f4ec05d4f),User(8ba22a2c-428b-4477-bb08-ea8f4ec05d4f))),
  (dc8e8463-64a3-4032-943f-06bdbcba1e17,(Order(4a1e69cc-a998-4c5a-98e8-1052a15c20a8,dc8e8463-64a3-4032-943f-06bdbcba1e17),User(dc8e8463-64a3-4032-943f-06bdbcba1e17))))


                </code></pre>

            </section>
            <section data-transition="none none">
                <aside class="notes">
                    Colocation / CoPartitioning
                </aside>
            </section>
        </section>

        <section data-transition="none none">
            <h2>Conclusion</h2>
        </section>
    </div>
</div>

<script src="lib/js/head.min.js"></script>
<script src="js/reveal.js"></script>

<script>

    // Full list of configuration options available at:
    // https://github.com/hakimel/reveal.js#configuration
    Reveal.initialize({
        controls: true,
        progress: true,
        history: true,
        center: true,

        transition: 'slide', // none/fade/slide/convex/concave/zoom

        // Optional reveal.js plugins
        dependencies: [
            { src: 'lib/js/classList.js', condition: function() { return !document.body.classList; } },
            { src: 'plugin/markdown/marked.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
            { src: 'plugin/markdown/markdown.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
            { src: 'plugin/highlight/highlight.js', async: true, condition: function() { return !!document.querySelector( 'pre code' ); }, callback: function() { hljs.initHighlightingOnLoad(); } },
            { src: 'plugin/zoom-js/zoom.js', async: true },
            { src: 'plugin/notes/notes.js', async: true }
        ]
    });

</script>
</body>
</html>

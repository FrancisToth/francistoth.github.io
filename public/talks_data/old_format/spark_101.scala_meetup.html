<!doctype html>
<html lang="en">

<head>
    <meta charset="utf-8">

    <title>Introduction to Spark - by Francis Toth</title>

    <meta name="description" content="A framework for easily creating beautiful presentations using HTML">
    <meta name="author" content="Hakim El Hattab">

    <meta name="apple-mobile-web-app-capable" content="yes" />
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />

    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">

    <link rel="stylesheet" href="css/reveal.css">
    <link rel="stylesheet" href="css/custom.css">
    <link rel="stylesheet" href="css/theme/francis.css" id="theme">

    <!-- Code syntax highlighting -->
    <link rel="stylesheet" href="lib/css/zenburn.css">

    <!-- Printing and PDF exports -->
    <script>
        var link = document.createElement( 'link' );
        link.rel = 'stylesheet';
        link.type = 'text/css';
        link.href = window.location.search.match( /print-pdf/gi ) ? 'css/print/pdf.css' : 'css/print/paper.css';
        document.getElementsByTagName( 'head' )[0].appendChild( link );
    </script>

    <!--[if lt IE 9]>
    <script src="lib/js/html5shiv.js"></script>
    <![endif]-->
</head>

<body>


<aside style="display: block; position: fixed; bottom: 10px; right: 10px; z-index: 30;">
    <a href="http://www.yoppworks.com"><img src="images/spark_scala_meetup/spark_scala_meetup/yoppworks-logo-landscape-large-text-dark-bg.png"
                                            height="30"></a>
</aside>

<div class="reveal">
    <div class="slides">
        <section>
            <aside class="notes">Hello everyone, my name is Francis, and today I'll give you a brief introduction to Spark
            </aside>
            <h2 style="display: inline-block;"><img
                    style="vertical-align: text-bottom; margin: 18px 0; border: 0; background-color: #243044"
                    src="images/spark_scala_meetup/spark_logo.transparent.png"/></h2>
        </section>

        <section>
            <h2>Disclaimer</h2>
            <aside class="notes">But before doing that, I'd like to make a small <b>disclaimer</b>.<br/><br/>

                ***First of all, I'm not a <b>Spark guru</b>. I've actually starting using it about 1 year and a half
                ago and am mostly familiar with its core aspects.<br/><br/>

                ***Secondly, we won't have the time to cover everything about Spark, and we will <b>skip some details</b>.
                This presentation intends to give you only an overview of how Spark works along with the challenges it
                involves.<br/><br/>

                <!--***Spark has been written in <b>Scala</b>, and therefore the examples you'll see in this talk will also-->
                <!--be in Scala. However, Spark also provides a <b>Python</b> and a <b>Java</b> api, which are very well-->
                <!--described-->
                <!--in the official documentation. If you are not familiar with Scala, don't worry, it shouldn't really-->
                <!--matter.<br/><br/>-->

                ***Finally, whenever you feel lost, <b>please interrupt me</b>. I've prepared this introduction in order
                to leave plenty of time for questions. So don't hesitate. Now let's move on and see what Spark is all
                about.
            </aside>
            <ul>
                <li class="fragment">I'm no Spark guru</li>
                <li class="fragment">This is a quick crash course</li>
                <!--<li class="fragment">Examples are in Scala</li>-->
                <li class="fragment">Stop me whenever you feel lost</li>
            </ul>
        </section>

        <section>
            <h2>Quick overview</h2>
            <section data-transition="none none">
                <aside class="notes">
                    So Spark was <b>founded in 2009</b> at the AmpLab in California.<br/><br/>

                    ***It is a <b>cluster computing platform</b> which extends the traditional <b>MapReduce model</b>,
                    and which is <b>designed to support</b> various types of computations on large <b>dataset</b>.
                    Originally, Spark intended to address <b>common problems</b> people were facing while doing Machine
                    learning. Therefore, it is highly <b>optimized for iterative and cyclic</b> operations on the same
                    set of data.<br/><br/>
                </aside>
                <ul>
                    <li class="fragment">Created in 2009</li>
                    <li class="fragment">Large-scale data processing engine</li>
                    <li style="visibility: hidden">Unified framework</li>
                    <li style="visibility: hidden">Extremely effective</li>
                    <li style="visibility: hidden">Popular</li>
                </ul>
            </section>
            <section data-transition="none none">
                <aside class="notes">
                    A data pipeline is nothing more than a <b>sequence of steps</b>, each being responsible for <b>processing</b>
                    the data, and for <b>passing</b> the result of its computation to the next step in the sequence. With <b>Hadoop</b>,
                    this is usually done by <b>chaining</b> different frameworks with each others and by storing the data
                    resulting from each step in <b>HDFS</b>.<br/><br/>

                    This approach has actually <b>two main issues</b>, first, in terms of <b>performances</b>, as each
                    step required a certain amount of IO to store/read the intermediary data, and secondly in terms of
                    <b>maintenance</b>, as it requires to juggle with sometime different languages, paradigms or tools.
                    <b>Spark</b> addresses this by providing a unified framework. The main idea is that every Spark's
                    library is based on the same core concepts which makes the switch between one library to the other
                    very easy.<br/><br/>

                    <!--<b>Spark</b> makes <b>easy</b> and <b>inexpensive</b> the combination of different <b>processing-->
                    <!--types</b> using a <b>unified-->
                    <!--framework</b>, which is extremely useful when maintaining <b>data analysis pipelines</b>, and which-->
                    <!--contributes-->
                    <!--to <b>reduce the management</b> of separate tools which is a very <b>common thing</b> in-->
                    <!--Hadoop.<br/><br/>-->

                    ***Compared to <b>traditional MapReduce engines</b>, benchmarks have shown a <b>significant increase</b>
                    in performances which, compared to <b>Hadoop</b>, is about a <b>hundred faster in memory</b>, and
                    <b>ten time</b> faster on disk. Actually, Spark makes a <b>heavy use of caching</b> and
                    <b>in-memory processing</b> in the contrary of Hadoop which is <b>mostly disk dependent</b>.
                </aside>
                <ul>
                    <li>Created in 2009</li>
                    <li>Large-scale data processing engine</li>
                    <li>Unified framework</li>
                    <li class="fragment">Extremely effective</li>
                    <li style="visibility: hidden">Popular</li>
                </ul>
            </section>
            <section data-transition="none none">
                <aside class="notes">
                    Finally, Spark is today <b>one of the most popular</b> and <b>active</b> project regarding<b>large-scale
                    data processing</b>. It is used by <b>Amazon</b>, <b>Ebay</b>, and <b>Netflix</b> among others, and
                    has seen its number of <b>contributors</b> growing tremendously since it's hosted by <b>Apache</b>.
                </aside>
                <ul>
                    <li>Created in 2009</li>
                    <li>Large-scale data processing engine</li>
                    <li>Unified framework</li>
                    <li>Extremely effective</li>
                    <li>Popular</li>
                </ul>
            </section>
        </section>

        <section>
            <h2>Agenda</h2>
            <aside class="notes">
                So today, we'll mainly focus on <b>Spark's core aspects</b>, present its <b>execution model</b> and the
                ways it deals with <b>large-scale data computation</b>. If we have some time, we'll then talk about its
                higher level API's.

                <!--<b>features</b>-->
                <!--it supports. Then we'll briefly cover some of its higher level API's with Spark-SQL, a lib responsible for-->
                <!--doing interactive queries and Spark-Streaming which deals with continuous and close-to-real-time-->
                <!--processing. Unfortunately, we won't have time to cover MLib which is a lib dedicated to machine-->
                <!--learning, and GraphX, which is Spark's graph processing lib.-->
            </aside>
            <div style="display: block; text-align: center"><img style="border: 0; background-color: #FFFFFF"
                                                                 src="images/spark_scala_meetup/spark-stack.png"/></div>
        </section>

        <section>
            <h2>Spark Architecture</h2>
            <section data-transition="none none">
                <aside class="notes">
                    In terms of architecture, a Spark application relies on <b>three components</b>. First, the driver. The driver is
                    the <b>central coordinator</b> of a Spark program. It is responsible for <b>defining distributed
                    dataset</b> on the cluster and for <b>launching parallel operations</b> on them.
                </aside>
                <div style="display: block; text-align: center; margin-top: 60px"><img width="60%"
                                                                                       style="border: 0; background-color: #FFFFFF"
                                                                                       src="images/spark_scala_meetup/architecture_1.png"/>
                </div>
            </section>
            <section data-transition="none none">
                <aside class="notes">
                    In order to perform those tasks, the <b>driver</b> relies on a certain amount of <b>executors</b>.
                    An executor executors has <b>two roles</b>. First, it is responsible for <b>running the tasks</b> required
                    by the driver, and secondly, it provides <b>in-memory storage</b> which is used by Spark for <b>caching purpose</b>.
                </aside>
                <div style="display: block; text-align: center; margin-top: 60px"><img width="60%"
                                                                                       style="border: 0; background-color: #FFFFFF"
                                                                                       src="images/spark_scala_meetup/architecture_2.png"/>
                </div>
            </section>
            <section data-transition="none none">
                <aside class="notes">
                    Now in order to make the <b>glue</b> between those two, a <b>Spark application</b> relies on a <b>cluster
                    manager</b> responsible for <b>providing the worker nodes to the driver</b>. With Spark, the cluster manager
                    is an <b>independent plugin</b> which doesn't rely on a <b>specific implementation</b>. This allows
                    Spark to <b>run transparently</b> on top of <b>Yarn</b>, <b>Mesos</b> or its <b>built-in standalone
                    cluster manager</b>.<br/><br/>

                    The driver and its executors have all their own Java process and form together a Spark application.
                </aside>
                <div style="display: block; text-align: center; margin-top: 60px"><img width="60%"
                                                                                       style="border: 0; background-color: #FFFFFF"
                                                                                       src="images/spark_scala_meetup/architecture_3.png"/>
                </div>
            </section>
        </section>

        <section>
            <h2>Resilient Distributed Dataset</h2>
            <section data-transition="none none">
                <aside class="notes">
                    The <b>core fundamental</b> of Spark is the <b>RDD</b>. RDD stands for <b>Resilient Distributed
                    Dataset</b>.
                    It's basically an <b>interface</b> to your data, or more precisely, a <b>representation</b> of the
                    data
                    that's coming <b>into your system</b> in an object format and on top of which, you can do <b>computations</b>.<br/><br/>

                    Internally, an RDD is simply an <b>immutable distributed collection</b> of objects. This collection
                    is
                    split into <b>multiple partitions</b>, each of them referencing a <b>subset</b> of your data, and
                    which are
                    <b>distributed</b> over your <b>cluster</b> nodes.
                </aside>
                <div style="display: block; text-align: center;"><img
                        style="border: 0; background-color: #FFFFFF; width:60%" src="images/spark_scala_meetup/rdd_2.png"/></div>
            </section>
            <section data-transition="none none">
                <aside class="notes">
                    Concretely, you can create an RDD *** <b>by parallelizing</b> a collection such as a simple list of
                    String,
                    *** <b>or by loading</b> an external dataset. Overall, Spark can <b>interface</b> with a wide
                    variety of
                    <b>distributed storage</b> including <b>HDFS</b>, <b>OpenStack Swift</b>, <b>Cassandra</b>,
                    <b>S3</b>, or even <b>custom solutions</b>,
                    and can also interface with your <b>local file system</b>, which is great for testing purpose.
                </aside>
                <div style="text-align: left;margin-top:50px;display:inline-block">
                    <ul style="margin-left: 0;margin-right:80px; list-style-type:none;white-space: nowrap;">
                        <li>val conf = new SparkConf()</li>
                        <li class="fragment" style="color:#A3BCFF;">val parCollection = sc.parallelize(Array(1, 2, 3,
                            4))
                        </li>
                        <li class="fragment" style="color:#01B623;">val distFile = sc.textFile("hdfs://...")</li>
                    </ul>
                </div>
            </section>
            <section data-transition="none none">
                <aside class="notes">
                    An <b>RDD</b> supports <b>two types</b> of operation. <b>Transformations</b>, which create a new RDD
                    from an <b>existing</b> one, and <b>actions</b> which perform a computation on an RDD's dataset.
                    Most common <b>transformations</b> are <b>map</b>, <b>filter</b> and <b>flatMap</b>, while <b>actions</b>
                    are more about operations consisting in <b>collecting</b>, <b>counting</b>, or <b>sampling</b> the data.
                    Actions will be typically responsible for <b>persisting</b> the result of a computation, or sending it back
                    to the <b>driver</b>.
                </aside>
                <div>
                    Transformations
            <pre><code class="scala" style="font-size: 25px;">val wordOccurrences = distFile
    .flatMap(_.split(" "))
    .map((_, 1))
    .reduceByKey(_ + _)</code></pre>

                    Actions
            <pre><code class="scala" style="font-size: 25px;">wordOccurrences.collect() // returns Array[(String, Int)]
wordOccurrences.saveAsTextFile("...") //
wordOccurrences.count()
wordOccurrences.take(4)</code></pre>
                </div>
            </section>
        </section>

        <section>
            <h2>RDD : Lineage</h2>
            <section data-transition="none none">
                <aside class="notes">
                    One interesting aspect of <b>creations</b> and <b>transformations</b>, is that they are <b>lazily
                    evaluated</b>.
                    Actually, as long as <b>no action is performed</b>, Spark may not have brought data or computed <b>anything</b>
                    at all. Each transformation actually creates a new RDD which <b>maintains a pointer to its
                    ancestors</b>
                    along with the metadata regarding its relationship with them. In <b>Spark</b> terminology, this is
                    what we
                    call the <b>lineage</b> of an RDD, which is basically a <b>specification</b> of what an executor
                    should do in
                    order to compute a result.<br/><br/>

                    Now, concretely, an RDD's <b>lineage</b> is stored in an <b>directed acyclic graph</b> which has two
                    main purposes.
                    First, it allows the <b>recomputing</b> of an RDD which data is lost or not available.
                    <b>Secondly</b>, it is used
                    by Spark to figure out an <b>optimal execution plan</b> in order to prevent any <b>overhead</b> and
                    avoid <b>multiple
                    reading</b> of the same data. But we'll come back to this later.
                </aside>
                <div style="display: block; text-align: center"><img
                        style="border: 0; background-color: #FFFFFF; width:50%" src="images/spark_scala_meetup/lineage2.png"/></div>
            </section>
            <section data-transition="none none">
                <aside class="notes">
                    Now, if you want to have an idea about how an RDD is computed, you can give a glance at the
                    <b>toDebugString method</b>, which provides a description of the RDD along with its dependencies.
                </aside>
                <pre><code class="console" style="font-size: 23px;">scala> wordOccurrences.toDebugString

(4) MapPartitionsRDD[16] at map at &lt;console&gt;:26 []
|  ShuffledRDD[15] at reduceByKey at &lt;console&gt;:26 []
+-(4) MapPartitionsRDD[14] at map at &lt;console&gt;:26 []
|  MapPartitionsRDD[13] at flatMap at &lt;console&gt;:26 []
|  ParallelCollectionRDD[0] at parallelize at &lt;console&gt;:23 []</code></pre>
            </section>
        </section>

        <section>
            <h2>RDD : Persistence and Caching</h2>

            <section data-transition="none none">
                <aside class="notes">
                    As we've just seen it, Spark will <b>recompute</b> an RDD <b>any time</b> when a action is being
                    performed on it.
                    This is alwyas true except if the RDD in question has been <b>cached</b>. As you may guess,
                    recomputing an RDD can quickly become a burden depending on the size of its dataset.<br/><br/>

                    ***So in order to prevent that, Spark provides caching features allowing an RDDs data to be stored
                    either
                    in <b>memory, on disk, or both of them</b>. This is especially useful, if you need to iterate over
                    an
                    RDD, like when you're training a machine learning model for example.<br/><br/>
                </aside>
                <pre><code class="scala">val result = input.map(x => x*x)
println(result.count())
// RDD will be recomputed here
println(result.collect().mkString(","))</code></pre>

                <pre class="fragment"><code class="scala">import org.apache.spark.storage.StorageLevel

val result = input.map(x => x * x)
result.persist(StorageLevel.DISK_ONLY)

println(result.count())
// RDD won' be recomputed here
println(result.collect().mkString(","))</code></pre>

                <pre style="visibility: hidden;"><code class="scala">// Use default storage level (MEMORY_ONLY)
result.cache()</code></pre>

                <pre style="visibility: hidden;"><code class="scala">result.unpersist()</code></pre>
            </section>

            <section data-transition="none none">
                <aside class="notes">
                    Persisting an RDD allows <b>future actions</b> to be much faster, actually often by more than 10
                    times.
                    In order to do it, all you need is to marked the RDD as persisted, using the persist method and to
                    specify a storage level.<br/><br/>

                    ***You can also use the <b>cache method</b>, which basically is like persist but using the default
                    storage
                    that is in memory.<br/><br/>

                    ***Finally, RDDs come with a method <b>unpersist()</b> that lets you manually remove them from the
                    cache.
                </aside>
                <pre><code class="scala">val result = input.map(x => x*x)
println(result.count())
// RDD will be recomputed here
println(result.collect().mkString(","))</code></pre>

                <pre><code class="scala">import org.apache.spark.storage.StorageLevel

val result = input.map(x => x * x)
result.persist(StorageLevel.DISK_ONLY)

println(result.count())
// RDD won' be recomputed here
println(result.collect().mkString(","))</code></pre>

            <pre class="fragment"><code class="scala">// Use default storage level (MEMORY_ONLY)
result.cache()</code></pre>

                <pre class="fragment"><code class="scala">result.unpersist()</code></pre>
            </section>

            <section data-transition="none none">
                <aside class="notes">
                    Now no matter if you decide to cache an RDD's data in the memory or in the disk, keep in mind that
                    Spark will always favor memory over disk in order to allows operations to run as fast as possible.
                    So, data will be <b>spilled</b> on the disk only if you attempt to cache <b>too much data to fit in
                    memory</b>. Secondly, Spark will evict old cached partitions using an LRU cache policy.<br/><br/>

                    Here are the <b>standard ways</b> of caching an RDD's data. As you can see each has its <b>tradeoffs</b>
                    and <b>advantages</b>. You can also <b>replicate</b> an RDD's data over <b>several nodes</b> or, if
                    the RDD contains <b>too much</b> data to fit on a <b>single node</b>, caching it on multiple nodes.

                    Overall, this means that you should not worry too much about your <b>job breaking</b> if you ask
                    Spark to cache <b>too much data</b>. <b>However</b>, keep in mind that caching <b>unnecessary</b>
                    data can lead to more <b>data evictions</b> and <b>re-computation time</b>.
                </aside>
                <table>
                    <tr>
                        <td>Level</td>
                        <td>Space</td>
                        <td>CPU</td>
                        <td>RAM</td>
                        <td>Disk</td>
                    </tr>
                    <tr>
                        <td>MEMORY_ONLY</td>
                        <td>High</td>
                        <td>Low</td>
                        <td>Y</td>
                        <td>N</td>
                    </tr>
                    <tr>
                        <td>DISK_ONLY</td>
                        <td>Low</td>
                        <td>High</td>
                        <td>N</td>
                        <td>Y</td>
                    </tr>
                    <tr>
                        <td>MEMORY_AND_DISK</td>
                        <td>High</td>
                        <td>Medium</td>
                        <td>Some</td>
                        <td>Some</td>
                    </tr>
                    <tr>
                        <td>MEMORY_ONLY_SER*</td>
                        <td>Low</td>
                        <td>High</td>
                        <td>Y</td>
                        <td>N</td>
                    </tr>
                    <tr>
                        <td>MEMORY_AND_DISK_SER*</td>
                        <td>Low</td>
                        <td>High</td>
                        <td>Some</td>
                        <td>Some</td>
                    </tr>
                    <tr>
                        <td>&nbsp;</td>
                    </tr>
                    <tr>
                        <td colspan="5">(*) : Data is serialized before being persisted (takes less space)</td>
                    </tr>
                </table>
            </section>
        </section>

        <section>
            <h2>RDD : Execution</h2>
            <section data-transition="none none">
                <aside class="notes">
                    Now let's move on, and focus a little bit on how a <b>sequence of operations</b> is processed by
                    Spark.
                    One typical <b>mistake</b> Spark beginners tend to do is to forget about the <b>distributed nature
                    of an RDD</b>.
                    Writing a Spark program may sound <b>easy</b>, but it requires to understand how <b>computations</b>
                    are <b>distributed</b>
                    over the <b>cluster</b>. In order to illustrate this, let's go back to our <b>word count</b>. Here,
                    we have an
                    <b>RDD</b> creation, a couple of <b>transformations</b>, and an <b>action</b> in the end.
                </aside>
                <div style="text-align: left;margin-top:50px;display:inline-block">
                    <ul style="margin-left: 0;margin-right:80px; list-style-type:none;white-space: nowrap;">
                        <li>val output = sparkContext</li>
                        <li style="padding-left: 100px">.textFile("hdfs://...")</li>
                        <li style="padding-left: 100px">.flatMap(_.split(" "))</li>
                        <li style="padding-left: 100px">.map((_, 1))</li>
                        <li style="padding-left: 100px">.reduceByKey(_ + _)</li>
                        <li style="padding-left: 100px">.collect()</li>
                        <li>print output</li>
                    </ul>
                </div>

                <div style="text-align: left;vertical-align: top; margin-top:60px;display:inline-block; font-size: 30px;width:460px;">
                    <div style="vertical-align: middle;width:30px;height:30px;margin-bottom:10px;background-color:#A3BCFF;display:none;"></div>
                    <span style="visibility: hidden">&nbsp;Executed on the driver</span><br/>
                    <div style="vertical-align: middle;width:30px;height:30px;margin-bottom:10px;background-color:#01B623;display:none;"></div>
                    <span style="visibility: hidden">&nbsp;Executed on the executors</span><br/>
                    <div style="vertical-align: middle;width:30px;height:30px;margin-bottom:10px;background-color:#FF0905;display:none;"></div>
                    <span style="visibility: hidden">&nbsp;May return a value to the Driver</span>
                </div>
            </section>
            <section data-transition="none none">
                <aside class="notes">
                    The lines in <b>blue</b> will be executed on the <b>driver</b> as they are part of the main program
                </aside>
                <div style="text-align: left;margin-top:50px;display:inline-block">
                    <ul style="margin-left: 0;margin-right:80px; list-style-type:none;white-space: nowrap;">
                        <li style="color:#A3BCFF;">val output = sparkContext</li>
                        <li style="padding-left: 100px">.textFile("hdfs://...")</li>
                        <li style="padding-left: 100px">.flatMap(_.split(" "))</li>
                        <li style="padding-left: 100px">.map((_, 1))</li>
                        <li style="padding-left: 100px">.reduceByKey(_ + _)</li>
                        <li style="padding-left: 100px">.collect()</li>
                        <li style="color:#A3BCFF">print output</li>
                    </ul>
                </div>

                <div style="text-align: left;vertical-align: top; margin-top:60px;display:inline-block; font-size: 30px;width:460px;">
                    <div style="vertical-align: middle;width:30px;height:30px;margin-bottom:10px;background-color:#A3BCFF;display:inline-block;"></div>
                    <span style="visibility: visible">&nbsp;Executed on the driver</span><br/>
                    <div style="vertical-align: middle;width:30px;height:30px;margin-bottom:10px;background-color:#01B623;display:none;"></div>
                    <span style="visibility: hidden">&nbsp;Executed on the executors</span><br/>
                    <div style="vertical-align: middle;width:30px;height:30px;margin-bottom:10px;background-color:#FF0905;display:none;"></div>
                    <span style="visibility: hidden">&nbsp;May return a value to the Driver</span>
                </div>
            </section>
            <section data-transition="none none">
                <aside class="notes">
                    On the other hand, <b>creation and transformations</b>, that is the <b>textFile</b>, <b>flatMap</b>,
                    <b>map</b> and <b>reduceByKey</b> will be
                    executed on the worker nodes by the <b>executors</b>.
                </aside>

                <div style="text-align: left;margin-top:50px;display:inline-block">
                    <ul style="margin-left: 0;margin-right:80px; list-style-type:none;white-space: nowrap;">
                        <li style="color:#A3BCFF;">val output = sparkContext</li>
                        <li style="color:#01B623; padding-left: 100px">.textFile("hdfs://...")</li>
                        <li style="color:#01B623; padding-left: 100px">.flatMap(_.split(" "))</li>
                        <li style="color:#01B623; padding-left: 100px">.map((_, 1))</li>
                        <li style="color:#01B623; padding-left: 100px">.reduceByKey(_ + _)</li>
                        <li style="padding-left: 100px">.collect()</li>
                        <li style="color:#A3BCFF">print output</li>
                    </ul>
                </div>

                <div style="text-align: left;vertical-align: top; margin-top:60px;display:inline-block; font-size: 30px;width:460px;">
                    <div style="vertical-align: middle;width:30px;height:30px;margin-bottom:10px;background-color:#A3BCFF;display:inline-block;"></div>
                    <span style="visibility: visible">&nbsp;Executed on the driver</span><br/>
                    <div style="vertical-align: middle;width:30px;height:30px;margin-bottom:10px;background-color:#01B623;display:inline-block;"></div>
                    <span style="visibility: visible">&nbsp;Executed on the executors</span><br/>
                    <div style="vertical-align: middle;width:30px;height:30px;margin-bottom:10px;background-color:#FF0905;display:none;"></div>
                    <span style="visibility: hidden">&nbsp;May return a value to the Driver</span>
                </div>
            </section>
            <section data-transition="none none">
                <aside class="notes">
                    Finally, the code in <b>red</b>, that is the <b>collect</b>, is an <b>action</b>, and actions may
                    <b>transfer data</b> from the
                    workers to the Driver. Some <b>actions</b> actually don't do this, and can instead <b>persist</b>
                    the result on
                    an <b>external</b> storage. Let's see now, what happens when you perform a <b>collect</b> action.
                </aside>

                <div style="text-align: left;margin-top:50px;display:inline-block">
                    <ul style="margin-left: 0;margin-right:80px; list-style-type:none;white-space: nowrap;">
                        <li style="color:#A3BCFF;">val output = sparkContext</li>
                        <li style="color:#01B623; padding-left: 100px">.textFile("hdfs://...")</li>
                        <li style="color:#01B623; padding-left: 100px">.flatMap(_.split(" "))</li>
                        <li style="color:#01B623; padding-left: 100px">.map((_, 1))</li>
                        <li style="color:#01B623; padding-left: 100px">.reduceByKey(_ + _)</li>
                        <li style="color:#FF0905; padding-left: 100px">.collect()</li>
                        <li style="color:#A3BCFF">print output</li>
                    </ul>
                </div>

                <div style="text-align: left;vertical-align: top; margin-top:60px;display:inline-block; font-size: 30px; width:460px;">
                    <div style="vertical-align: middle;width:30px;height:30px;margin-bottom:10px;background-color:#A3BCFF;display:inline-block;"></div>
                    <span style="visibility: visible">&nbsp;Executed on the driver</span><br/>
                    <div style="vertical-align: middle;width:30px;height:30px;margin-bottom:10px;background-color:#01B623;display:inline-block;"></div>
                    <span style="visibility: visible">&nbsp;Executed on the executors</span><br/>
                    <div style="vertical-align: middle;width:30px;height:30px;margin-bottom:10px;background-color:#FF0905;display:inline-block;"></div>
                    <span style="visibility: visible">&nbsp;May return a value to the Driver</span>
                </div>
            </section>

            <section data-transition="none none">
                <aside class="notes">
                    So here we have our <b>workers</b>, each responsible for a <b>partition</b>.
                </aside>
                <div style="display: block; text-align: center; margin-left:50px"><img
                        style="border: 0; background-color: #FFFFFF; width:80%" src="images/spark_scala_meetup/collect_1.png"/></div>
                &nbsp;
            </section>
            <section data-transition="none none">
                <aside class="notes">
                    When we call collect, we are actually <b>telling Spark to collect and send all </b> the data from each
                    partition, to the driver. With small dataset, this shouldn't be a problem. <b>However</b>, as Spark
                    is designed to work with <b>datasets that are too big</b> to fit in the memory of a <b>single machine</b>,
                    collecting all the data of a cluster and putting it on the driver may trigger an <b>OutOfMemory</b>
                    exception.<br/><br/>

                    ***So a general rule of thumb is to avoid using <b>collect</b>, or similar actions, if your data is not
                    <b>bounded</b>. If this is not possible, consider storing it on an <b>external</b> storage.
                </aside>
                <div style="display: block; text-align: center; margin-left:50px"><img
                        style="border: 0; background-color: #FFFFFF; width:80%" src="images/spark_scala_meetup/collect_2.png"/></div>
                <span class="fragment">Don't use collect() on large RDDs !</span>
            </section>

            <section data-transition="none none">
                <aside class="notes">
                    One other common mistake is to write <b>code</b> such as this one. Anyone has an idea why this could
                    cause
                    a problem ?<br/><br/>Well, as we've seen it, Spark's unified API <b>abstracts</b> the fact that your
                    code may be
                    <b>executed</b> by the <b>driver</b> or not. So depending on how you <b>structure</b> your <b>transformations</b>
                    and <b>actions</b>,
                    some <b>operations</b> may cause an <b>overhead</b> or <b>behave</b> in an unexpected way.
                </aside>
                <ul style="list-style-type:none;padding-right: 30px">
                    <li>var counter = 0</li>
                    <li>var rdd = sc.parallelize(data)</li>
                    <li>&nbsp;</li>
                    <li>rdd.foreach { rddItem => counter += 1 }</li>
                    <li>print("Counter value: " + counter)</li>
                </ul>
            </section>

            <section data-transition="none none">
                <aside class="notes">
                    In this example, the <b>closure</b> inside the foreach function refers to the counter variable. In
                    order to
                    <b>distribute</b> this operation, Spark has to <b>serialize it</b> along with any variable it refers
                    to (so, in our
                    case, the <b>counter</b> variable).<br/><br/>

                    During this <b>serialization process</b>, the counter variable will be <b>copied</b> and sent along
                    with the <b>closure</b>.
                    So at some point, the <b>counter</b> variable used by the driver and its executors <b>are no longer
                    the same</b>.<br/><br/>

                    So, the <b>final value</b> of counter will always be <b>zero</b> since the we are
                    <b>incrementing</b> it in the foreach action.
                    Still, you may wonder how we can <b>achieve</b> this, well Spark provides a feature called <b>accumulators</b>
                    which is exactly <b>designed</b> for this purpose, but we'll come back to it in a few seconds.
                </aside>
                <ul style="list-style-type:none;padding-right: 30px">
                    <li style="background-color:#0029FF;">var counter = 0</li>
                    <li>var rdd = sc.parallelize(data)</li>
                    <li>&nbsp;</li>
                    <li style="background-color:#0029FF;">rdd.foreach { rddItem => counter += 1 }</li>
                    <li>print("Counter value: " + counter)</li>
                </ul>
            </section>
        </section>

        <section>
            <h2>RDD : Recap</h2>
            <section data-transition="none none">
                <aside class="notes">
                    Let's make a quick recap of what we've learnt so far.<br/><br/>

                    *** A Spark application is <b>composed</b> of a driver and its executors.<br/><br/>

                    *** Its core fundamental is the RDD or <b>Resilient Distributed Dataset</b>. An RDD is basically
                    an immutable collection distributed over a cluster nodes.<br/><br/>

                    *** An RDD supports <b>2 types of operation</b> :
                    Transformations and Actions. Transformations are <b>lazily evaluated</b> and compose together a
                    sequence of
                    operation which is played only when an action is performed.
                </aside>
                <ul>
                    <li class="fragment">Spark = driver + executors</li>
                    <li class="fragment">RDD is an immutable distributed collection</li>
                    <li class="fragment">An RDD supports Transformations and Actions</li>
                    <li style="visibility: hidden">An RDD's lineage is a sequence of Transformation</li>
                    <li style="visibility: hidden">An RDD can be persisted for caching purpose</li>
                    <li style="visibility: hidden">Watch out for an RDD's execution (Driver vs. Executors)</li>
                </ul>
            </section>

            <section data-transition="none none">
                <aside class="notes">
                    This sequence is called the <b>lineage</b> of an RDD, and allows its recomputation whenever its data
                    is not available.<br/><br/>

                    **** In order to <b>prevent uneccessary recomputations</b>, we can persist an RDD in <b>memory</b>,
                    in <b>disk</b> or
                    <b>both</b> depending on what we want to achieve and what resources are available.<br/><br/>

                    *** Finally, it is important to understand how a sequence of operations is <b>distributed</b>, as
                    some parts of it may
                    be executed on the driver while others may be performed on the executors.
                </aside>
                <ul>
                    <li>Spark = driver + executors</li>
                    <li>RDD is an immutable distributed collection</li>
                    <li>An RDD supports Transformations and Actions</li>
                    <!--<li>A Sequence of Transformations composes an RDD's lineage</li>-->
                    <li>An RDD's lineage is a sequence of ransformation</li>
                    <li class="fragment">An RDD can be persisted for caching purpose</li>
                    <li class="fragment">Watch out for an RDD's execution (Driver vs. Executors)</li>
                </ul>
            </section>
        </section>

        <section>
            <h2>Shared variables</h2>
            <section data-transition="none none">
                <aside class="notes">
                    Now let's go back to the <b>previous example</b>. The whole point of this code is to <b>share</b> a
                    variable between
                    the <b>Driver</b> and its <b>executor</b>. However, and as we've seen it, this cannot work fo the
                    reasons we've
                    mentioned earlier. But actually, it is even <b>worse</b> than that. <br/><!--*** Actually, the serialization process
                    we've talked about earlier is done <b>as many time as</b> there are partitions. So if your cluster contains
                    10 nodes with a hundred partitions each, this closure will be serialized and sent through the
                    network a thousand times.<br/> Now if the counter variable is actually a <b>huge object</b>, this may get very
                    <b>inefficient</b> at some point. So the whole problem here is not only about <b>sharing a variable</b>, but also
                    to do it while <b>minimizing IOs</b> and in a <b>safe way</b> in terms of concurrency.<br/>
                    ****
                    In order to resolve this problem, Sparks supports <b>2 types of shared variables</b> : Accumulators and
                    Broadcast variables. The first is designed to send a value from the executors to their driver,
                    while the second one is for the other way around. Now of course, there are some <b>subtleties</b>, but
                    this is the main idea. So let's start with accumulators.-->
                </aside>
                <div style="text-align: left">
                    <pre style="width:100%;"><code class="scala" style="font-size: 25px;">var counter = 0
var rdd = sc.parallelize(data)

rdd.foreach { rddItem => counter += 1 }
print("Counter value: " + counter)</code></pre>

                    <pre style="width:100%;visibility: hidden"><code class="scala" style="font-size: 25px;">val hugeArray = ...
var bigRddWithIndex = ...

bigRddWithIndex.map { rddItem => hugeArray[rddItem.key] }
...</code></pre>
                </div>
                <p style="visibility: hidden">Accumulators and Broadcast variables !</p>
            </section>

            <section data-transition="none none">
                <aside class="notes">
                    Actually, the <b>serialization process</b> we've talked about earlier is done <b>as many time as</b>
                    there
                    are partitions. So if your <b>cluster</b> contains <b>10 nodes</b> with a <b>hundred partitions</b>
                    each, this closure
                    will be <b>serialized</b> and sent through the network a <b>thousand</b> times.<br/><br/>

                    Now if the <b>counter</b> variable
                    is actually a <b>huge object</b>, this may get very <b>inefficient</b> at some point. So the whole
                    problem here is not only about <b>sharing a variable</b>, but also to do it while <b>minimizing
                    IOs</b>
                    and in a <b>safe way</b> in terms of <b>concurrency</b>.
                </aside>
                <div style="text-align: left">
                    <pre style="width:100%;"><code class="scala" style="font-size: 25px;">var counter = 0
var rdd = sc.parallelize(data)

rdd.foreach { rddItem => counter += 1 }
print("Counter value: " + counter)</code></pre>

                    <pre style="width:100%;"><code class="scala" style="font-size: 25px;">val hugeArray = ...
var bigRddWithIndex = ...

bigRddWithIndex.map { rddItem => hugeArray[rddItem.key] }
...</code></pre>
                </div>
                <p style="visibility: hidden">Accumulators and Broadcast variables !</p>
            </section>

            <section data-transition="none none">
                <aside class="notes">
                    In order to resolve this problem, Sparks supports <b>2 types of shared variables</b> : <b>Accumulators</b>
                    and
                    <b>Broadcast variables</b>. The first is designed to send a value from the <b>executors</b> to their
                    <b>driver</b>,
                    while the <b>second one</b> is for the <b>other way around</b>. Now of course, there are some <b>subtleties</b>,
                    but
                    this is the main idea. <!--So let's start with <b>accumulators</b>-->.
                </aside>
                <div style="text-align: left">
                    <pre style="width:100%;"><code class="scala" style="font-size: 25px;">var counter = 0
var rdd = sc.parallelize(data)

rdd.foreach { rddItem => counter += 1 }
print("Counter value: " + counter)</code></pre>

                    <pre style="width:100%;"><code class="scala" style="font-size: 25px;">val hugeArray = ...
var bigRddWithIndex = ...

bigRddWithIndex.map { rddItem => hugeArray[rddItem.key] }
...</code></pre>
                </div>
                <p>Accumulators and Broadcast variables !</p>
            </section>

            <section data-transition="none none">
                <aside class="notes">
                    Accumulators are overall designed to <b>aggregate</b> values coming from the executors to the
                    driver. In order to make this safely, an accumulator's value is <b>only visible by the driver</b>,
                    and can be <b>updated only by its executors</b>. <!--So from the executor's point of view, an accumulator
                    is <b>write-only</b>, while from the driver, it's a <b>read-only value</b>.--><br/><br/>

                    Now as you remember, an RDD may be <b>recomputed</b> for different reasons, which means that, if
                    updated inside a <b>transformation</b>, an <b>accumulator</b> may be modified more than required.
                    Actually, Spark <b>guarantees</b> that for a <b>given accumulator</b>, updates are applied <b>only
                    once</b> they are done inside an <b>action</b>, but this is <b>not true</b> if they are applied inside
                    a <b>transformation</b>. Actually, this feature has been a lot <b>criticized</b>, as it has been found out that
                    in some edge cases, <b>accumulators</b> may be updated more than required even if those <b>updates</b> are done
                    inside an action. Therefore, I strongly suggest you to use them only for <b>debugging purpose</b>, or if
                    they are idempotent.

                    <!--for <b>accumulators</b>
                    which are updated inside a <b>transformation</b>. Therefore, be very <b>cautious</b> with accumulators,
                    and don't <b>rely</b> on them if they are used inside transformations. <!-- Regarding how <b>accumulators</b>
                    can be extended, Spark also <b>supports custom accumulators</b> and custom aggregation operations as
                    long as they honor the <b>Accumulator's contract</b>. -->
                </aside>
                <table>
                    <tr>
                        <td style="vertical-align: middle">
                            Accumulators
                        <pre style="width:100%;"><code class="scala" style="font-size: 25px;">val counter = sc.accumulator(0)
var rdd = sc.parallelize(data)

rdd.foreach {
    rddItem => counter += 1
}
print(counter.value)</code></pre>
                        </td>
                        <td><img style="margin-top:60px; box-shadow: 0; background-color: #FFFFFF; width:100%"
                                 src="images/spark_scala_meetup/accumulators.png"/></td>
                    </tr>
                    <tr>
                        <td colspan="2">Accumulators aggregate values coming from the executors to the driver</td>
                    </tr>
                </table>
            </section>

            <section data-transition="none none">
                <aside class="notes">
                    In a previous example, we were wondering about how it would be possible to share a <b>heavy
                    object</b> without <b>impacting network throughput</b>, or dealing with <b>concurrency issues</b>.
                    Well <b>broadcast variables</b> are exactly designed for this purpose. Basically, a broadcast variable
                    is able to wrap a value and is guaranteed to be <b>sent only once</b> per worker node. This is
                    especially useful if you need to share a <b>lookup table</b> across a cluster's nodes. <br/><br/>

                    In this example, we <b>wrap a big array</b> inside a <b>broadcast variable</b>, and are able to access it by
                    calling the <b>value property</b>. <!--Now no matter if this value is <b>mutable or not</b>, any of its
                    <b>updates</b> won't ever be <b>propagated</b> to the other nodes and will remain <b>local only</b>.<br/><br/>-->

                    Now <b>broadcast variables</b> may be a <b>bottleneck</b> at some point, as serializing and deserializing
                    a data structure can be <b>sometime expensive</b>. So it is important to choose a proper <b>data structure</b>
                    along with a good <b>serialization library</b>. Spark comes by default with <b>Kryo</b> but you can
                    also use other libraries or or your own <b>serialization routines</b>.
                </aside>
                <table>
                    <tr>
                        <td style="vertical-align: middle">
                            Broadcast variables
                        <pre style="width:100%;"><code class="scala" style="font-size: 25px;">val bigArray = sc.broadcast(...)
var bigRddWithIndex = ...

bigRddWithIndex.map { e =>
  broadcastedArray.value[e.key]
}</code></pre>
                        </td>
                        <td><img style="margin-top:60px; box-shadow: 0; background-color: #FFFFFF; width:100%"
                                 src="images/spark_scala_meetup/broadcast_var.png"/></td>
                    </tr>
                    <tr>
                        <td colspan="2">Broadcast variables propagate a read-only value to all executors</td>
                    </tr>
                </table>
            </section>

            <section data-transition="none none">
                <aside class="notes">
                    So let's make a quick recap.<br/><br/>

                    Spark provides <b>two kinds</b> of shared variables, <b>accumulators</b> which are designed to
                    aggregate values coming from the <b>executors</b>, and <b>broadcast variables</b> that are great for
                    <b>sharing</b> objects between a cluster's nodes.<br/><br/>

                    <b>Accumulators</b> should be used for <b>debugging purpose only</b>, and only in <b>actions</b> as
                    they may be updated more than required in some cases. On the other hand <b>Broadcast variables</b> are
                    guaranteed to be sent only <b>once</b> per worker node, and require an <b>efficient data structure</b>
                    along with a <b>fast serialization library</b>.

                    <!--In order to <b>share variables</b> between the <b>driver</b> and its <b>executors</b>, we have to-->
                    <!--<b>rely</b> on <b>accumulators</b>-->
                    <!--and <b>broadcast variables</b>. Accumulators are <b>write-only variables</b> which can be only read-->
                    <!--by the <b>driver</b>.-->
                    <!--As their name <b>suggests it</b>, they are great for <b>aggregating data</b>. However, <b>don't-->
                    <!--rely</b> on them if they-->
                    <!--are used inside <b>transformations</b>, as a transformation maybe executed <b>more than once</b>. As-->
                    <!--long as they-->
                    <!--are used in <b>actions only</b>, or if they are <b>idempotent</b>, you should be fine.<br/><br/>-->

                    <!--On the other hand, <b>broadcast variables</b> are great for sharing <b>read-only</b> data across the-->
                    <!--cluster's-->
                    <!--nodes. They are <b>guaranteed</b> to be sent only once per <b>worker node</b>, and are a good-->
                    <!--candidate if you-->
                    <!--need to share a <b>lookup table</b>. However, they may be <b>expansive</b> if badly serialized,-->
                    <!--therefore, you-->
                    <!--have to use an <b>efficient data-structure</b> along with a good <b>serialization process</b>.-->
                </aside>
                <table>
                    <tr>
                        <td style="vertical-align: top">Accumulators :</td>
                        <td>
                            <ul>
                                <li>Can be only read by the driver</li>
                                <li>Great for aggregations</li>
                                <li>Recommended for debugging only (if used in transformations)</li>
                            </ul>
                        </td>
                    </tr>
                    <tr>
                        <td style="vertical-align: top; white-space: nowrap">Broadcast variables :</td>
                        <td>
                            <ul>
                                <li>Read-Only values</li>
                                <li>Great for lookup tables</li>
                                <li>Watch out for serialization process</li>
                            </ul>
                        </td>
                    </tr>
                </table>
            </section>
        </section>

        <section>
            <h2>Spark execution plan</h2>
            <section data-transition="none">
                <aside class="notes">
                    Now that you have a <b>basic understanding</b> of how an RDD works, we can get a little bit more
                    into details and see how it is <b>computed</b> by Spark. <!--Knowing about Spark's <b>internals</b> is
                    actually something
                    which will help you to avoid <b>some silly errors</b> and to debug your application's
                    jobs.--><br/><br/>

                    ****Earlier, we've seen that an RDD is actually <b>lazily evaluated</b>, and that nothing is
                    computed until
                    an <b>action</b> is performed. <!--We've also seen that before being <b>executed</b>,--> Secondly an RDD is
                    represented as a
                    <b>directed acyclic graph</b> which is a <b>logical representation</b> of what needs to be computed
                    in
                    order to get a result. Now, this <b>graph</b> is actually translated by Spark into a <b>physical
                    execution plan</b>.<br/><br/>

                    So when an <b>action</b> is called, Spark's <b>scheduler</b> will visit each <b>RDD</b>
                    starting from the <b>final RDD</b>, that is the one obtained with the <b>last filter</b>, and will
                    then go <b>backward</b>
                    to find what needs to be computed until the <b>first RDD</b> has been reached. During this process,
                    Spark will create <b>computation stages</b>.
                    <!-- highlight the arrows -->
                </aside>
                <img class="fragment"
                     style="margin-top:100px;margin-left:50px; box-shadow: 0; background-color: #FFFFFF; width:100%"
                     src="images/spark_scala_meetup/execution_plan_3.png"/>
            </section>

            <section data-transition="none none">
                <aside class="notes">
                    A stage is basically a set of <b>one to many</b> transformations which do not require any <b>data
                    movement</b> or
                    <b>shuffling</b>, and which can be therefore <b>pipelined</b>. It's some kind of a <b>super-operation</b>
                    in which
                    <b>transformations</b> have been <b>fused</b> together in order to not going over the data <b>multiple
                    time</b> and to
                    avoid the <b>overhead</b> of each operation if they were <b>executed</b> one after the other.
                </aside>
                <img style="margin-top:100px;margin-left:50px; box-shadow: 0; background-color: #FFFFFF; width:100%"
                     src="images/spark_scala_meetup/execution_plan_2.png"/>
            </section>

            <section data-transition="none none">
                <aside class="notes">
                    Now, each stage is composed of as <b>many task</b> as there are partitions available <b>when its
                    executed</b>. So if
                    you have <b>10 partitions</b> for a given stage, the scheduler will create <b>10 tasks</b> for it.
                    Now as
                    you may guess, <b>the more task</b> your whole Spark process needs to execute, <b>the longer</b> it
                    will take to
                    compute the final result. So one way to optimize this, is to <b>prevent shuffling</b> to happen, or
                    at least
                    limit its <b>impact on performances</b>, which is exactly what we're about to discuss.
                </aside>
                <img style="margin-top:100px;margin-left:50px; box-shadow: 0; background-color: #FFFFFF; width:100%"
                     src="images/spark_scala_meetup/execution_plan_1.png"/>
            </section>

            <section data-transition="none none">
                <aside class="notes">
                    So very briefly, here is <b>how Spark executes an RDD</b>. The user starts by defining a <b>directed
                    acyclic
                    graph</b> of RDDs. An action is then called, and triggers the <b>translation of the graph</b> to an
                    execution
                    plan. Spark will then perform some optimizations by <b>collapsing RDDs</b> that can be pipelined,
                    and
                    finally, <b>the tasks</b> resulting from each stages are <b>scheduled and executed</b> on the
                    cluster.
                </aside>
                <!--une image serait peut tre plus parlante-->
                <ul>
                    <li>Definition of a DAG</li>
                    <li>An action is called</li>
                    <li>The DAG is translated to an execution plan</li>
                    <li>Optimizations (pipeline)</li>
                    <li>Tasks are created, scheduled and executed</li>
                </ul>
            </section>

            <!-- -> Partitioning (repartitions + partitions)-->
            <!-- -> Controlling partitioning + Shuffling + Strategies-->
            <!--https://www.youtube.com/watch?v=w0Tisli7zn4-->
        </section>

        <section>
            <h2>Shuffling</h2>
            <section data-transition="none none">
                <aside class="notes">
                    As we've just said, Spark can <b>combine transitions</b> together as long as no data needs to be <b>shuffled</b>,
                    that is moved from one node to the other. Now <b>shuffling</b> may be <b>expansive</b>, still it is
                    required sometime,
                    and therefore needs to be <b>limited</b> in terms of impact on <b>performances</b>.<br/><br/>

                    In the next few slides, we'll discuss about <b>what makes a shuffle happening</b>, explain <b>why it
                    is expansive</b>, and <b>how to limit its impact</b>.<br/><br/>

                    ****
                    So first, let's see what <b>triggers</b> a shuffle. As we've seen it, these first lines here, the
                    <b>textFile</b>,
                    the <b>flatMap</b>, and the <b>map</b> transitions, are <b>regrouped</b> in a single <b>stage</b>
                    and all happening on the same
                    worker node.
                </aside>
                <table class="fragment">
                    <tr>
                        <td style="vertical-align: middle">
                            <ul style="margin-left: 0;margin-right:30px; list-style-type:none;white-space: nowrap;">
                                <li>sparkContext</li>
                                <li style="color:#A3BCFF; padding-left: 25px">.textFile("hdfs://...")</li>
                                <li style="color:#A3BCFF; padding-left: 25px">.flatMap(_.split(" "))</li>
                                <li style="color:#A3BCFF; padding-left: 25px">.map((_, 1))</li>
                                <li style="padding-left: 25px">.reduceByKey(_ + _)</li>
                            </ul>
                        </td>
                        <td><img
                                style="margin-top:30px; margin-left:100px;box-shadow: 0; background-color: #FFFFFF; width:100%"
                                src="images/spark_scala_meetup/shuffle_1.png"/></td>
                    </tr>
                </table>
            </section>
            <section data-transition="none none">
                <aside class="notes">
                    But once <b>reduceByKey</b> has been called, Spark will take <b>all the pairs</b> having the <b>same
                    word</b>, and put them on the <b>same machine</b> together. And this is typically the kind of situation which <b>triggers
                    a shuffle</b>. More generally, consider than any <b>all-to-all operations</b> such as a <b>groupByKey</b> or
                    a <b>join</b> will trigger that.<br/><br/>

                    Now why is this <b>expansive</b> ? Well this may sound <b>obvious</b> but <b>serializing data</b>,
                    <b>sending</b> it through the
                    <b>network</b>, and <b>deserializing</b> it on the <b>destination</b> node can quickly become very
                    <b>costly</b>, especially
                    if the dataset is <b>big</b>. So overall, in order to limit <b>shuffling</b>, there are <b>two</b>
                    main
                    aspects you should focus on <b>the quantity of data to shuffle</b>, and <b>the level of
                    parallelism</b>.
                </aside>
                <table>
                    <tr>
                        <td style="vertical-align: middle">
                            <ul style="margin-left: 0;margin-right:30px; list-style-type:none;white-space: nowrap;">
                                <li>sparkContext</li>
                                <li style="color:#A3BCFF; padding-left: 25px">.textFile("hdfs://...")</li>
                                <li style="color:#A3BCFF; padding-left: 25px">.flatMap(_.split(" "))</li>
                                <li style="color:#A3BCFF; padding-left: 25px">.map((_, 1))</li>
                                <li style="color:#9EE13C; padding-left: 25px">.reduceByKey(_ + _)</li>
                            </ul>
                        </td>
                        <td><img
                                style="margin-top:30px; margin-left:100px;box-shadow: 0; background-color: #FFFFFF; width:100%"
                                src="images/spark_scala_meetup/shuffle_2.png"/></td>
                    </tr>
                </table>
            </section>

            <section data-transition="none none">
                <aside class="notes">
                    Let's say I want to <b>make a join</b> between an RDD containing the people in <b>Canada</b>
                    with one that contains all the canadian <b>provinces/territories</b>.<br/><br/>

                    *** Well, this would result in a <b>very uneven sharding</b> as some
                    provinces like <b>Quebec</b> being much more populated than territories like
                    <b>Nunavut</b>.<br/><br/>

                    Moreover, we would get a <b>very limited level of parallelism</b> as we would only have 13 <b>partitions</b>,
                    and
                    adding more <b>machines</b> to get the job done would not change <b>anything</b>.
                </aside>
                <table>
                    <tr>
                        <td><img
                                style="margin-top:30px; margin-right:60px;box-shadow: 0; background-color: #FFFFFF; width:100%"
                                src="images/spark_scala_meetup/shuffle_3.png"/></td>
                        <td class="fragment" style="vertical-align: middle">
                            <ul style="width:350px; white-space: nowrap;">
                                <li>Uneven sharding</li>
                                <li>Limited parallelism</li>
                            </ul>
                        </td>
                    </tr>
                </table>
            </section>
            <section data-transition="none none">
                <aside class="notes">
                    Actually, a better way to do this is to use a <b>broadcast variable</b> cointaining all the canadian
                    <b>provinces and territories</b>. This variable could be then sent to all <b>the worker nodes</b>,
                    which would
                    avoid a <b>shuffle</b>. Now of course, this is only possible if the <b>dataset</b> being sent can
                    <b>fit in the memory</b>
                    of a single node.
                </aside>
                <table>
                    <tr>
                        <td><img
                                style="margin-top:30px; margin-right:60px;box-shadow: 0; background-color: #FFFFFF; width:100%"
                                src="images/spark_scala_meetup/shuffle_4.png"/></td>
                        <td style="width:350px; vertical-align: middle">
                            <ul style=" white-space: nowrap;">
                                <li>Even Sharding</li>
                                <li>No Shuffle Required</li>
                            </ul>
                        </td>
                    </tr>
                </table>
            </section>
            <section data-transition="none none">
                <aside class="notes">
                    Now what if you cannot use a <b>broadcast variable</b> ? what if the data in <b>both RDDs</b> is too
                    big ? Let's
                    say for example that we need to <b>join</b> an <b>RDD</b> containing all the <b>people in Canada</b>
                    with one
                    containing all the <b>people in the world</b>. Joining those <b>RDDs</b> would result in a <b>lot of
                    data shuffled</b>
                    across the <b>network</b> and potentially a <b>space problem</b> in the destination nodes.
                </aside>
                <table>
                    <tr>
                        <td><img
                                style="margin-top:30px; width:100%; margin-left:10px;box-shadow: 0; background-color: #FFFFFF"
                                src="images/spark_scala_meetup/shuffle_7.png"/></td>
                        <td style="vertical-align: middle">
                            <ul style="width:350px; white-space: nowrap;">
                                <li>Even Sharding</li>
                                <li>Good Parallelism</li>
                                <li>Space may be a problem</li>
                            </ul>
                        </td>
                    </tr>
                </table>
            </section>
            <section data-transition="none none">
                <aside class="notes">
                    So instead of doing this, we could just create a <b>partial RDD</b> using a <b>filter</b>, and <b>reduce</b>
                    the amount
                    of data to be <b>transferred</b>. The whole idea here is to make sure that you <b>transfer</b> only
                    the data that
                    is required by <b>further</b> operations.
                </aside>
                <table>
                    <tr>
                        <td><img
                                style="margin-top:30px; width:100%; margin-left:10px;box-shadow: 0; background-color: #FFFFFF;"
                                src="images/spark_scala_meetup/shuffle_8.png"/></td>
                        <td style="vertical-align: middle;">
                            <ul style="width:350px; white-space: nowrap;">
                                <li>Even Sharding</li>
                                <li>Good Parallelism</li>
                                <li>Less Space required</li>
                                <li>Less Data shuffled</li>
                            </ul>
                        </td>
                    </tr>
                </table>
            </section>
            <section data-transition="none none">
                <aside class="notes">
                    So these are a <b>couple of solutions</b>, but overall, there is no <b>strict rule</b> or <b>silver
                    bullet</b> to avoid <b>shuffling</b>. Use <b>efficient data-structures</b> along with a <b>fast serialization
                    library</b> and
                    <b>know your data</b>, so you can only <b>transfer the minimum required</b>. In any case, if you
                    have a doubt
                    about what's <b>happening</b>, Spark provides a <b>UI</b> containing <b>tons of graphics and
                    metrics</b>. It's very
                    easy to use and very well documented, and I highly suggest you to have a glance at it.
                </aside>
                <table>
                    <tr>
                        <td style="vertical-align: top;white-space: nowrap">Recommendations :</td>
                        <td style="white-space: nowrap">
                            <ul>
                                <li>Fast serialization library (Kryo)</li>
                                <li>Lightweight data-structures</li>
                                <li>Shuffle only the minimum required</li>
                                <li>Use Spark's UI</li>
                            </ul>
                        </td>
                    </tr>
                </table>
            </section>
        </section>

        <!-- ####################################################################################################### -->

        <section>
            <h2>Conclusion</h2>
            <aside class="notes">
                So this is the end of this <b>presentation</b>, and I hope that it met your <b>expectations</b>. Spark
                is probably one of the <b>biggest thing</b> happening nowadays in the <b>big data industry</b> as it <b>tackles</b> the
                Big Data problem from a completely <b>different angle</b>. A modern <b>stream processing pipeline</b> is actually not only
                about <b>processing</b> the data but also about the associated <b>pre-processing</b> and <b>post-processing</b> aspects. In
                other words, the <b>same stream of data</b> may be used in <b>batch</b> jobs, <b>machine learning</b>, <b>graphs</b> as
                much as to provide <b>live updates</b>. Some time ago, this implied that you had to <b>deal with many different frameworks</b> and <b>disparate
                programming models</b> in order to achieve this. Spark makes this actually <b>very simple</b>, thanks to a <b>unified
                API</b>, much <b>better performances</b> than traditional Map-Reduce engines, and an <b>ever growing community</b>.
            </aside>
            <ul>
                <li><span style="font-size: 30px;color: #FF6600;">Learning Spark : </span><span style="font-size: 25px">by Andy Konwinski, Holden Karau, and Patrick Wendell</span>
                </li>
                <li><span style="font-size: 30px;color: #00CC33;">Advanced Analytics with Spark : </span><span
                        style="font-size: 25px; ">by Josh Wills, Sandy Ryza, Sean Owen, and Uri Laserson</span></li>
                <li><span style="font-size: 30px;color: #FF6600;">Introduction to Apache Spark : </span> <span
                        style="font-size: 25px">by Paco Nathan</span></li>
            </ul>
        </section>

        <section>
            <aside class="notes">
                So I haven't introduced my self, my name is Francis Toth, I'm the Scala tam Lead at Yoppworks. Very quickly,
                Yoppworks has been created on the remains of Bold Radius, and provides consulting and training services
                in partnership with Lightbend and Hortonworks. We are quite new but have more projects than people,
                therefore we are hiring. We are mostly looking for people having experienced with the Lightbend's stack along with
                Big Data projects. So if you are interested, just come see me, I'll be happy to talk about a potential collaboration
                or anything else actually. Here are some business cards, feel free to take one of those. Thank you.
            </aside>
            <img style="background-color: transparent"
                 src="images/spark_scala_meetup/spark_scala_meetup/yoppworks-logo-landscape-large-text-dark-bg.png"/>
            Thank you ! Questions ?
        </section>

        <!-- ####################################################################################################### -->

        <section>
            <h2>Spark's high-level APIs</h2>
            <aside class="notes">
                So this is all about Spark's core. I know it's a lot of material but at least this gives you an overview
                of how Spark works. Now let's move on, an look at some higher level API's. As we've said in the
                begining of this talk, we won't have the time to cover them all and will only focus on the broad lines.
            </aside>
        </section>

        <section>
            <h2>Spark SQL</h2>

            <section data-transition="none none">
                <aside class="notes">
                    So, Spark SQL. In a couple of words, Spark SQL is an API*** released in 2015***, which allows you to
                    interact with a dataset through SQL, as long as it can be represented in a tabular format. So the
                    data must be structured and have a schema. This allows you to read and write data in a variety****
                    of structured format like Json, Hive or Parquet in a very unified and transparent way.****<br/><br/>

                    In some way Spark SQL is very close to Hive as they both deal with interactive querying. Actually,
                    they are not exclusive.**** Spark SQL supports indeed most of Hive features, and allows you to access
                    Hive tables, User-Defined functions, SerDes for serialization and deserialization formats, and the Hive Query
                    Language.<br/><br/>

                    Now it's important to notice that using Hive features with Spark SQL does not require a Hive
                    installation. A library called spark-hive provides the glue between these two frameworks which let you take
                    advantage of Hive without even installing it.

                    <!--The reason why you can choose to use Hive or not, is that some-->
                    <!--users may have dependency conflict between the version of Hive used by their cluster and the one-->
                    <!--they wish to use with Spark.-->
                </aside>
                <ul>
                    <li class="fragment">Released in February 2015</li>
                    <li class="fragment">Structured dataset querying through SQL</li>
                    <li class="fragment">Integrates with Hive, Parquet, JSon...</li>
                    <li class="fragment">Supports most of Hive's feature</li>
                </ul>
            </section>
            <section data-transition="none none">
                <aside class="notes">
                    Under the hood, Spark SQL is based on an extension of the RDD model called a Dataframe. Dataframes
                    are overall similar to tables in a relational database and contain Row objects, which are just
                    wrappers around arrays of basic types. Each row represents a record, and as they are structured, they can be
                    stored in a much more efficient manner than native RDDs. Actually, some benchmarks show that a Dataframe is
                    twice faster than an RDD if the Scala API is used, and ten time faster if using the one written in
                    Python.
                </aside>
                <img style="margin-top:30px; width:100%; margin-left:50px;box-shadow: 0; background-color: #FFFFFF;"
                     src="images/spark_scala_meetup/dataframe_0.png"/>
            </section>

            <section data-transition="none none">
                <aside class="notes">
                    Now enough talking now, let's see some code.

                    First thing you need to do in order to play with Spark-SQL, is to import a context.**** Depending
                    on whether you use Hive or not, Spark SQL has two main entry points : The SQLContext and the
                    HiveContext. According the official documentation, it is actually recommended to use Hive's features as a lot of
                    support material is already available for them. ****A dataframe can be created by converting an existing RDD
                    or ****from external sources like a json or an hdfs file. For some sources, Sparq SQL will be able
                    to infer the schema of your data, and this is exactly what happens when I create a dataframe from a
                    json file. **** This schema can by the way be printed using the method show() provided by the dataframe
                    object.
                </aside>

                <div class="fragment">
                    <pre><code class="scala">import org.apache.spark.sql.hive.HiveContext
val hiveCtx = new HiveContext(new SparkContext(...))</code></pre>

                    <pre><code class="scala">import org.apache.spark.sql.SQLContext
val sqlCtx = new SQLContext(new SparkContext(...))</code></pre>
                </div>

                <div class="fragment">
                    <pre><code class="scala">val dataframe = sqlCtx.createDataFrame(rdd)</code></pre>
                    <pre><code class="scala">val dataframe = hiveCtx.jsonFile(inputFile)</code></pre>
                </div>

                <pre class="fragment"><code class="console">scala> dataframe.show()
root
|-- foo: struct (nullable = true)
|    |-- waldos: array (nullable = true)
|    |    |-- element: string (containsNull = false)
|-- bar: boolean (nullable = true)
|-- qux: string (nullable = true)</code></pre>
            </section>

            <section data-transition="none none">
                <aside class="notes">
                    Now, in order to query my data, I just have to register a table name and use the sql method. This
                    brings me back a dataframe containing all the data I've requested, so I can make more computation on top of
                    it. Actually, depending on the kind of source the dataframe has been created from, Spark-SQL will be
                    able to select only a subset of the fields and smartly scan only the data for those fields, instead of
                    scanning everything.****

                    Now if I decide to apply some transformations on that dataframe, I have two options. I can either
                    use one of dataframe functions like select, filter or groupBy, or I can convert the resulting
                    dataframe into an rdd, and apply the functions provided by it.
                </aside>
                <pre><code class="scala">input.registerTempTable("foobar")
val foobarz = hiveCtx.sql("SELECT * FROM foobar ORDER BY qux LIMIT 10")</code></pre>

                <pre class="fragment"><code class="scala">foobarz.select(...).filter(...).groupBy(...)
foobarz.rdd.filter(...).map(...)</code></pre>
            </section>

            <section data-transition="none none">
                <aside class="notes">
                    Regarding how dataframes integrate with the rest of the world, Spark SQL provides a datasource API,
                    which allows you to integrate Spark SQL with Avro, HBase, ElasticSearch, and Cassandra to name only a few.
                </aside>
                <img style="margin-top:30px; width:100%; margin-left:10px;box-shadow: 0; background-color: #FFFFFF;"
                     src="images/spark_scala_meetup/dataframe_5.png"/>
            </section>

        </section>

        <section>
            <h2>Spark Streaming</h2>
            <section data-transition="none none">
                <aside class="notes">
                    Not let's move on to another API which is Spark Streaming. So, Spark Streaming was released in 2013,
                    *** that is, in early versions of Spark and has been always pretty stable since. *** It's real-time
                    analytics library which is used in cases such as monitoring the flow of users on a website or
                    detecting fraud transaction in real time. *** Spark-Streaming caught a lot of attention recently
                    , as real time is today extremely in demand. According to Databricks, the company created by the
                    founders of Spark, 56% more Spark's users globally ran Spark Streaming applications in 2015 compared
                    to 2014., and among those users, you'll find names like Uber, Netflix and Pinterest to name only a
                    few.
                    <!--http://www.datanami.com/2015/11/30/spark-streaming-what-is-it-and-whos-using-it/-->
                    <!--https://opensource.com/business/15/4/guide-to-apache-spark-streaming-->

                </aside>
                <ul>
                    <li class="fragment">Released in 2013</li>
                    <li class="fragment">Very stable and robust</li>
                    <li class="fragment">Real-Time analytics API</li>
                    <li class="fragment">Trendy</li>
                </ul>
            </section>
            <section data-transition="none none">
                <aside class="notes">
                    Spark-Streaming relies on micro-batch architecture. So the streaming computation is divided in small
                    continuous series of batches created at regular time intervals. At the beginning of each interval,
                    a new batch is created, and any data that arrives during this interval gets added to that batch.
                    Once the batch is done growing, it is processed as an RDD just like we've seen earlier.
                </aside>
                <img style="margin-top:50px; width:100%; margin-left:40px;box-shadow: 0; background-color: #FFFFFF;"
                     src="images/spark_scala_meetup/streaming_archi.png"/>
            </section>

            <section data-transition="none none">
                <aside class="notes">
                    The fundamental abstraction of Spark-Streaming is a DStream, or discretized stream. A DStream is
                    a sequence of RDD where each RDD has one time slice of the data in the stream. Just like RDDs,
                    DStreams can be created from external sources or by applying transformations to other DStreams.
                    Once created and properly transformed, we can apply ouput actions such as print() to the DStream.
                    Output actions are actually similar to RDD actions in that they write data to external systems.
                    However, these actions run periodically on each time step, producing output in batches.

                    Now these DStreams won't be started as long as I don't call the start method here. It's pretty
                    much like the concept of actions with RDDs. This means that all the DStreams which compose my
                    program must be specified before calling the start method. Once it has been called, any modification
                    of these streams won't be taken in account. The streaming will be effective until the
                    streams are finished, either manually or du to an error, and this is what the awaitTermination is
                    for.

                    As you can see, the syntax used to create an RDD and a DStream is pretty similar, and actually
                    this is what really makes Spark shine. No matter if you are doing batch or streaming processing,
                    the programming model is always the same.
                </aside>

                <pre><code class="scala">// Boilerplate
val conf = new SparkConf()
    .setMaster("local[2]").setAppName("NetworkWordCount")
val ssc = new StreamingContext(conf, Seconds(1))

// Create a DStream listening on port 9999
val lines: DStream[String] = ssc.socketTextStream("localhost", 9999)

lines.flatMap(_.split(" "))
    .map(word => (word, 1))
    .reduceByKey(_ + _)
    .print()

ssc.start() // Start the computation
ssc.awaitTermination() // Wait for the computation to terminate</code></pre>
            </section>

            <section data-transition="none none">
                <aside class="notes">
                    Regarding fault-tolerance, DStreams provide the same properties than RDDs. As long as a copy of the
                    data is
                    available, a DStream can recompute any state derived from it, using the lineage of its RDDs.
                    Therefore,
                    received data is replicated across two nodes by default, allowing a DStream to tolerate single
                    worker
                    failures. Secondly, as it can be expansive to recompute a long stream of data, Spark-Streaming
                    provides a mechanism called checkpointing that saves state periodically to a reliable filesystem. So
                    in case of failure, Spark will only have to go back to the last checkpoint. Finally, Spark-Streaming
                    can also be configured in order to run 24/7. Properly configured, it is able to ensure that no
                    matter
                    if the driver, a worker or a receiver fails, it will always be able to get back on its feet. Of
                    course,
                    this comes with some challenges and configurations but this is the idea.
                </aside>
                <table>
                    <tr>
                        <td style="vertical-align: top;white-space: nowrap">Fault-tolerance :</td>
                        <td style="white-space: nowrap">
                            <ul>
                                <li>Data replication</li>
                                <li>Checkpointing</li>
                                <li>Can run 24/7</li>
                            </ul>
                        </td>
                    </tr>
                </table>
            </section>

            <section data-transition="none none">
                <aside class="notes">
                    Regarding how you can integrate Spark-Streaming with other frameworks, well there is nothing
                    surprising.
                    Spark-Streaming is able to integrate with Akka, Kafka, Flume, and Kinesis to name only a few.
                </aside>
                <img style="margin-top:50px; width:100%; margin-left:40px;box-shadow: 0; background-color: #FFFFFF;"
                     src="images/spark_scala_meetup/streaming_integration.png"/>
            </section>
        </section>

        <section>
            <h2>Spark APIs : What's left ?</h2>
            <aside class="notes">
                So, we're reaching the end of this presentation, and unfortunately we won't have time to cover Spark ML
                and GraphX. But let's have a very quick word about them. ***

                Spark ML is Spark's Machine Learning Library and is designed to run in parallel on clusters. It provides
                mostly algorithms which are designed to run in parallel such as distributed random forests, K-means or
                alternating least squares, and are best suited for running on very large dataset, more than on small
                ones.
                ***

                On the other hand, GraphX is Spark's graph processing framework and intends to unify Data-parallel and
                graph parallel systems. It is designed for parallel iterative graph computation, provides a wide range
                of
                graph algorithm, and it's also very fast, actually about 10 times faster than traditional Hadoop graph
                algorithms. Secondly, if you've already worked with Graphs, you probably know that the graph processing
                is only a small part of the problem. Actually as data get bigger and bigger, the graph creation and
                post-processing tend to become a bottleneck. And this why GraphX is interesting. It provides support for
                graph construction and post-processing through a unified API.
            </aside>
            <table>
                <tr class="fragment">
                    <td style="vertical-align: top; white-space: nowrap">Spark ML :</td>
                    <td>
                        <ul>
                            <li>Native Machine Learning Framework</li>
                            <li>Supports Classification, Regression, Clustering</li>
                            <li>Designed to run on large distributed data in parallel</li>
                        </ul>
                    </td>
                </tr>
                <tr class="fragment">
                    <td style="vertical-align: top; white-space: nowrap">GraphX :</td>
                    <td>
                        <ul>
                            <li>Native Graph Processing framework</li>
                            <li>Similar to Pregel, Giraph, and Graphlab</li>
                            <li>Designed for network-oriented analytics</li>
                            <li>Very fast !</li>
                        </ul>
                    </td>
                </tr>
            </table>
        </section>

        <!--<section>-->
        <!--<h2>Conclusion</h2>-->
        <!--<aside class="notes">-->
        <!--So this is the end of this <b>presentation</b>, and I hope that it met your <b>expectations</b>. Spark-->
        <!--is probably one of the <b>biggest thing</b> happening nowadays in the <b>big data industry</b> as it <b>tackles</b> the-->
        <!--Big Data problem from a completely <b>different angle</b>. A modern <b>stream processing pipeline</b> is actually not only-->
        <!--about <b>processing</b> the data but also about the associated <b>pre-processing</b> and <b>post-processing</b> aspects. In-->
        <!--other words, the <b>same stream of data</b> may be used in <b>batch</b> jobs, <b>machine learning</b>, <b>graphs</b> as-->
        <!--much as to provide <b>live updates</b>. Some time ago, this implied that you had to <b>deal with many different frameworks</b> and <b>disparate-->
        <!--programming models</b> in order to achieve this. Spark makes this actually <b>very simple</b>, thanks to a <b>unified-->
        <!--API</b>, much <b>better performances</b> than traditional Map-Reduce engines, and an <b>ever growing community</b>.-->
        <!--</aside>-->
        <!--<ul>-->
        <!--<li><span style="font-size: 30px;color: #FF6600;">Learning Spark : </span><span style="font-size: 25px">by Andy Konwinski, Holden Karau, and Patrick Wendell</span>-->
        <!--</li>-->
        <!--<li><span style="font-size: 30px;color: #00CC33;">Advanced Analytics with Spark : </span><span-->
        <!--style="font-size: 25px; ">by Josh Wills, Sandy Ryza, Sean Owen, and Uri Laserson</span></li>-->
        <!--<li><span style="font-size: 30px;color: #FF6600;">Introduction to Apache Spark : </span> <span-->
        <!--style="font-size: 25px">by Paco Nathan</span></li>-->
        <!--</ul>-->
        <!--</section>-->

        <!--<section>-->
        <!--<img style="background-color: transparent"-->
        <!--src="images/spark_scala_meetup/spark_scala_meetup/yoppworks-logo-landscape-large-text-dark-bg.png"/>-->
        <!--Thank you ! Questions ?-->
        <!--</section>-->
    </div>
</div>

<script src="lib/js/head.min.js"></script>
<script src="js/reveal.js"></script>

<script>

    // Full list of configuration options available at:
    // https://github.com/hakimel/reveal.js#configuration
    Reveal.initialize({
        controls: true,
        progress: true,
        history: true,
        center: true,

        transition: 'slide', // none/fade/slide/convex/concave/zoom

        // Optional reveal.js plugins
        dependencies: [
            { src: 'lib/js/classList.js', condition: function() { return !document.body.classList; } },
            { src: 'plugin/markdown/marked.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
            { src: 'plugin/markdown/markdown.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
            { src: 'plugin/highlight/highlight.js', async: true, condition: function() { return !!document.querySelector( 'pre code' ); }, callback: function() { hljs.initHighlightingOnLoad(); } },
            { src: 'plugin/zoom-js/zoom.js', async: true },
            { src: 'plugin/notes/notes.js', async: true }
        ]
    });

</script>
</body>
</html>